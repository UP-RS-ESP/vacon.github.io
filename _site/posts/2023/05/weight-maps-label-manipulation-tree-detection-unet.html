<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.18.1 by Michael Rose
  Copyright 2013-2019 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Understanding weight maps and label manipulation in tree detection from high-resolution orthophotos with U-Net - VACON</title>
<meta name="description" content="Fine individiual tree crown delineation can be achieved from RGB + NIR orthophotos using CNN-based semantic segmentation with weighted losses, but how do different weight maps perform, and how does manipulation of training label sets affect this?  ">


  <meta name="author" content="Daniel Lusk">


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="VACON">
<meta property="og:title" content="Understanding weight maps and label manipulation in tree detection from high-resolution orthophotos with U-Net">
<meta property="og:url" content="http://localhost:4000/posts/2023/05/weight-maps-label-manipulation-tree-detection-unet">


  <meta property="og:description" content="Fine individiual tree crown delineation can be achieved from RGB + NIR orthophotos using CNN-based semantic segmentation with weighted losses, but how do different weight maps perform, and how does manipulation of training label sets affect this?  ">



  <meta property="og:image" content="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/banner.png">





  <meta property="article:published_time" content="2023-05-01T00:00:00+02:00">





  

  


<link rel="canonical" href="http://localhost:4000/posts/2023/05/weight-maps-label-manipulation-tree-detection-unet">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Remote Sensing and Earth Surface Processes",
      "url": "http://localhost:4000/"
    
  }
</script>






<!-- end _includes/seo.html -->


<link href="/feed.xml" type="application/atom+xml" rel="alternate" title="VACON Feed">

<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->



<!-- Mathjax Support -->
<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single wide">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          VACON
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/team_members/">Team</a>
            </li><li class="masthead__menu-item">
              <a href="/publications/">Publications</a>
            </li><li class="masthead__menu-item">
              <a href="/theses/">BSc and MSc Theses</a>
            </li><li class="masthead__menu-item">
              <a href="/msc-rsiv/">MSc RSIV</a>
            </li><li class="masthead__menu-item">
              <a href="/year-archive/">Blog Posts</a>
            </li><li class="masthead__menu-item">
              <a href="/codes/">Codes</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: linear-gradient(rgba(0, 0, 0, 0.3), rgba(0, 0, 0, 0.3)), url('https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/banner.png');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Understanding weight maps and label manipulation in tree detection from high-resolution orthophotos with U-Net

        
      </h1>
      
        <p class="page__lead">Fine individiual tree crown delineation can be achieved from RGB + NIR orthophotos using CNN-based semantic segmentation with weighted losses, but how do different weight maps perform, and how does manipulation of training label sets affect this?

</p>
      
      
      
      
    </div>
  
  
    <span class="page__hero-caption">U-Net based tree detection using eroded labels and border weight maps
</span>
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person">

  
    <div class="author__avatar">
      
        <img src="/images/DanielLusk.jpeg" alt="Daniel Lusk" itemprop="image">
      
    </div>
  

  <div class="author__content">
    
      <h3 class="author__name" itemprop="name">Daniel Lusk</h3>
    
    
      <div class="author__bio" itemprop="description">
        <p>MSc Student</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name">University of Potsdam</span>
        </li>
      

      
        
          
            <li><a href="mailto:lusk@uni-potsdam.de" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i> Email</a></li>
          
        
          
            <li><a href="https://github.com/dluks" rel="nofollow noopener noreferrer"><i class="fas fa-fw fa-link" aria-hidden="true"></i> Website</a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Understanding weight maps and label manipulation in tree detection from high-resolution orthophotos with U-Net">
    <meta itemprop="description" content="Fine individiual tree crown delineation can be achieved from RGB + NIR orthophotos using CNN-based semantic segmentation with weighted losses, but how do different weight maps perform, and how does manipulation of training label sets affect this?">
    <meta itemprop="datePublished" content="2023-05-01T00:00:00+02:00">
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-file-alt"></i> Understanding weight maps and label manipulation in tree detection from high-resolution orthophotos with U-Net</h4></header>
              <ul class="toc__menu">
  <li><a href="#introduction">Introduction</a></li>
  <li><a href="#methods">Methods</a>
    <ul>
      <li><a href="#study-site-and-data-acquisition">Study site and data acquisition</a></li>
      <li><a href="#data-preprocessing">Data preprocessing</a></li>
      <li><a href="#label-generation">Label generation</a></li>
      <li><a href="#weight-maps">Weight maps</a></li>
      <li><a href="#deep-learning-model-training">Deep learning model training</a></li>
      <li><a href="#tree-segmentation-from-deep-learning-model-predictions">Tree segmentation from deep learning model predictions</a></li>
    </ul>
  </li>
  <li><a href="#results">Results</a>
    <ul>
      <li><a href="#overall-performance">Overall performance</a></li>
      <li><a href="#tree-area-histograms">Tree area histograms</a></li>
      <li><a href="#visual-inspection">Visual inspection</a></li>
    </ul>
  </li>
  <li><a href="#discussion-and-conclusions">Discussion and conclusions</a></li>
  <li><a href="#references">References</a></li>
</ul>

            </nav>
          </aside>
        
        <p>Fine individiual tree crown delineation can be achieved from RGB + NIR orthophotos using CNN-based semantic segmentation with weighted losses, but how do different weight maps perform, and how does manipulation of training label sets affect this?</p>

<h1 id="introduction">Introduction</h1>

<p>Convolutional neural networks (CNN) have been used in vegetation remote sensing for years, and are an especially popular choice for image classification tasks <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>\(^,\)<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Fully convolutional neural networks (FCNN) such as U-Net, in particular, are currently considered best-in-class when it comes to image segmentation as their output is of the same resolution as their inputs, providing pixel-by-pixel classification, and have seen growing popularity in remote sensing image segmentation <sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>\(^,\)<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>\(^,\)<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>\(^,\)<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. Not only are FCNNs like U-Net effective at semantic segmentation—the process of classifying an image pixel-by-pixel, but not identifying distinct objects—others, such as Mask R-CNN, are capable of performing instance segmentation—the classification of not only pixels but also of objects (Figure 1) <sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>\(^,\)<sup id="fnref:8"><a href="#fn:8" class="footnote" rel="footnote" role="doc-noteref">8</a></sup>\(^,\)<sup id="fnref:9"><a href="#fn:9" class="footnote" rel="footnote" role="doc-noteref">9</a></sup>. However, while instance segmentation architectures can be powerful for tasks like tree detection, their architectures are often deep and multi-faceted and can require the optimization of a multitude of different hyperparameters. U-Net, on the other hand, is lightweight and simple to implement, with few hyperparameters, and is therefore an appealing option for researchers wishing to perform image segmentation while minimizing model tuning.</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/semantic-vs-instance.png" />
  <figcaption><b>Figure 1.</b> A comparison of semantic segmentation (e.g. the output of architectures like U-Net) and instance segmentation.</figcaption>
</figure>

<p>The problem remains, however, that U-Net can only perform semantic segmentation, i.e. it cannot provide object detection, and so additional methods are needed to process the semantically classified output into individual objects. Weight maps, for example, can be used to focus special attention on the boundaries between objects in order to train the model to be more conservative with class predictions in those regions, as has been done to achieve cell segmentation in medical images<sup id="fnref:3:1"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>. The semantic output can then be segmented algorithmically to identify individual objects. This same approach has been applied successfully by Brandt, <em>et al</em>. (2020) and Mugabowindekwe <em>et al</em>. (2022) to remote sensing imagery for tree detection <sup id="fnref:5:1"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>\(^,\)<sup id="fnref:6:1"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. To achieve this, the authors utilized weight maps corresponding to the boundaries of tree crowns during training to reduce the connectivity of neighboring trees in the semantic output and then performed several morphological operations on the resulting semantic output, such as circle fitting and region growing to recover the “missing” pixels. This ensemble process (training with weight maps + morphological post-processing) can prove difficult with imagery of dense forests in which tree crowns border each other on all sides, however, and deeper understanding of the influence of training schemes and post-processing methods may be useful in improving tree detection from optical remote sensing imagery.</p>

<p>In this exploration, aerial orthophotos of the city of Berlin, Germany are used in the training of seven U-Net-based models. We seek to answer three questions: i) which weight map types should be used; ii) what is the effect of training a model on eroded (shrunken) labels compared to unmodified labels; and iii) which post-processing operations are most effective in isolating individual trees from semantic output.</p>

<h1 id="methods">Methods</h1>
<h2 id="study-site-and-data-acquisition">Study site and data acquisition</h2>
<p>True aerial orthophotos of Berlin were acquired by the Berlin Office of Cartography and Geodesy in daylight hours in the summer of 2020 with a spatial resolution of 0.2 m and a positional accuracy of +/- 0.4 m <sup id="fnref:10"><a href="#fn:10" class="footnote" rel="footnote" role="doc-noteref">10</a></sup>\(^,\)<sup id="fnref:11"><a href="#fn:11" class="footnote" rel="footnote" role="doc-noteref">11</a></sup>. The data used for this study consists of red, green, blue (RGB), and near-infrared (NIR) bands, and were sourced from the TrueDOP20RGB and TrueDOP20CIR datasets available for download on Geoportal Berlin (FIS-Broker). Overall, four 1-km<sup>2</sup> tiles and one 0.5-km<sup>2</sup> tile were obtained from Geoportal Berlin for a total extent of 4.5-km<sup>2</sup> (Figure 2).</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/overview.png" />
  <figcaption><b>Figure 2.</b> Overview of study area (Berlin, Germany). Training tiles are outlined in blue, and the test/validation tile is outlined in green.</figcaption>
</figure>

<p>To generate canopy height maps (CHM) for semi-automated label generation prior to the training of the U-Net models, LiDAR point clouds acquired using airborne laser scanning (ALS) were also collected from Geoportal Berlin, with the same respective extents as the four 1-km<sup>2</sup> orthophoto tiles <sup id="fnref:12"><a href="#fn:12" class="footnote" rel="footnote" role="doc-noteref">12</a></sup>. The point clouds were pre-classified with the following classes: soil (class 2), low (class 3), medium (class 4), and high vegetation (class 5), outliers (class low points class 7), and default (class 0).</p>

<h2 id="data-preprocessing">Data preprocessing</h2>

<p><em>Normalization and NDVI</em></p>

<p>All band values for RGB and NIR were scaled to 0-1, and NDVI was calculated for all images using the formula \(\frac{NIR - Red}{NIR + Red}\). NDVI values were then also normalized to 0-1 to ensure consistency of data ranges.</p>

<h2 id="label-generation">Label generation</h2>

<p><em>Training label set</em></p>

<p>Tree labels for model training were generated for the four 1-km<sup>2</sup> tiles using in a “semi-automated” fashion. First, vegetation points were reclassified by filtering out all last-return points, as these points are most likely to be hard surfaces such as ground or buildings. From the remaining point cloud, points were further filtered out that did not fall within a maximum density threshold as informed by a k-d tree. Next, to reclaim points that had been inside vegetation but had been filtered about by the above steps, smaller point neighborhoods were again generated with the use of a k-d tree and points that fell within a now smaller threshold were reclaimed as vegetation points. After vegetation points had successfully been isolated, CHMs were generated. Tree labels were finally generated by identifying local maxima “islands” across the CHMs, applying watershed segmentation from the Python library scikit-image to the CHMs with the local maxima as markers, and filtering the resulting labels by eccentricity and total label area <sup id="fnref:13"><a href="#fn:13" class="footnote" rel="footnote" role="doc-noteref">13</a></sup>. These labels are referred to as “ORIG”.</p>

<p>It should be noted that the resulting labels, while benefiting from the advantage of being able to be generated in a matter of minutes compared to the many hours and days it would take for hand-drawn annotation at the same scale, are of lower quality than their hand-drawn counterparts, as tree shapes can be somewhat unnatural (blocky instead of smooth) and at times contained non-tree pixels.</p>

<p><em>Validation and test label sets</em></p>

<p>To ensure the model predictions were validated on a more reliable label set, the final 0.5-km<sup>2</sup> tile was hand-annotated using napari, a multi-dimensional image viewer for Python <sup id="fnref:14"><a href="#fn:14" class="footnote" rel="footnote" role="doc-noteref">14</a></sup>. Due to the limited hand-labeled dataset size, these labels were used for both validation and final model testing in lieu of additional high-quality labeled datasets (Figure 3).</p>

<p>In total, the training label set consisted of 7,359 trees, and the validation/test label set contained 1,193 trees.</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/hand-vs-auto-labels.png" />
  <figcaption><b>Figure 3.</b> Examples of semi-automated training labels and hand-drawn validation/test labels. Note the blockiness of the training labels, as well as the small segment of building classified as a tree.</figcaption>
</figure>

<p><em>Label erosion</em></p>

<p>To explore the effect of training the models on labels that did not include the tree canopy edges (as opposed to discouraging the learning of the borders using weighting schemes), eroded training and validation sets were generated from the semi-automated label sets. These labels are referred to as “ERODED”. Label erosion was performed with a 1x1 kernel using scikit-image’s simple morphological erosion method (Figure 4) <sup id="fnref:13:1"><a href="#fn:13" class="footnote" rel="footnote" role="doc-noteref">13</a></sup>.</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/orig-vs-eroded-labels.png" />
  <figcaption><b>Figure 4.</b> Non-eroded (ORIG) and eroded (ERODED) label sets.</figcaption>
</figure>

<h2 id="weight-maps">Weight maps</h2>

<p>Four weighting schemes were used: i) Ronneberger weight maps (RONN) as described in Ronneberger et al., (2015) <sup id="fnref:3:2"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, computed as</p>

\[\begin{aligned}
w(\mathbf{x}) = w_{c}(\mathbf{x}) + w_0 \cdot \exp \left(−\frac{ (d_1(\mathbf{x})+d_2(\mathbf{x}))^2}{2 \sigma^2}\right)
\end{aligned}\]

<p>and characterized by the highest weights occurring at touching (or almost touching) tree-to-tree borders with a rapid decay as tree-to-tree distances increase; ii) modified Ronneberger weight maps (BOUNDS10) in which weights &gt;= 3 are assigned a value of 10 and weights &lt; 1 are assigned a value of 0; iii) border weight maps (BORD10) in which the inner edge pixels of each label are set to 10 and all others to 0; and iv) no weights (ALL1) in which all “weights” are set to 1 (Figure 5).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_ronneberger_weights</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">wc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">w0</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    <span class="s">"""
    Generate weight maps as specified in the U-Net paper
    for boolean mask.
    
    "U-Net: Convolutional Networks for Biomedical Image Segmentation"
    https://arxiv.org/pdf/1505.04597.pdf
    
    Parameters
    ----------
    y: Numpy array
        2D array of shape (image_height, image_width) representing boolean (or binary)
        mask of objects.
    wc: dict
        Dictionary of weight classes.
    w0: int
        Border weight parameter.
    sigma: int
        Border width parameter.
    Returns
    -------
    Numpy array
        Training weights. A 2D array of shape (image_height, image_width).
    """</span>
    
    <span class="c1"># Check if mask is boolean or binary mask
</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">label</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        
    <span class="n">no_labels</span> <span class="o">=</span> <span class="n">labels</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">label_ids</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">labels</span><span class="p">))[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_ids</span><span class="p">)))</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">label_ids</span><span class="p">):</span>
            <span class="n">distances</span><span class="p">[:,:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">distance_transform_edt</span><span class="p">(</span><span class="n">labels</span> <span class="o">!=</span> <span class="n">label_id</span><span class="p">)</span>

        <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">d1</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">d2</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w0</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="p">((</span><span class="n">d1</span> <span class="o">+</span> <span class="n">d2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">no_labels</span>
        
        <span class="k">if</span> <span class="n">wc</span><span class="p">:</span>
            <span class="n">class_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">wc</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">class_weights</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">class_weights</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">w</span>
</code></pre></div></div>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/weight_maps.png" />
  <figcaption><b>Figure 5.</b>  Comparison of weight map types. Labels have been adjusted to appear darker than the background solely for visualization purposes, and during training labels + background were set to 1. Note that the borders in BORD10 are all 10, though they may appear continuous on some displays.</figcaption>
</figure>

<h2 id="deep-learning-model-training">Deep learning model training</h2>

<p><em>CNN architecture</em></p>

<p>For the task of tree identification, a U-Net architecture was used with a slightly-modified structure compared to the Ronneberger, et al. structure, and batch normalization was applied before each encoding operator (downsampling) and after each decoding operator (upsampling).</p>

<p><em>Loss function and weights</em></p>

<p>Because a core focus of this exploration is the effect of different weight maps on prediction quality, and weight maps directly modify the losses calculated after each forward pass of the CNN, choosing an appropriate loss function can greatly affect the model’s performance. When choosing a loss function, it is important to consider whether or not there is a class imbalance in the dataset. In the Berlin trees dataset only ~28% of the pixels represent trees, and therefore can be considered an imbalanced dataset. For instances such as this where the negative class abundance significantly outweighs that of the positive class, traditional accuracy-oriented loss functions become less useful, and it is preferable to use loss functions that prioritize measuring the overlap, or intersection-over-union (IoU), of the predictions with the true values <sup id="fnref:15"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">15</a></sup>. In this case, the Tversky loss function was selected as the primary metric for training as it is similar to other popular loss functions for image segmentation such as Dice or Jaccard losses, but can be adjusted according to desired outcomes <sup id="fnref:16"><a href="#fn:16" class="footnote" rel="footnote" role="doc-noteref">16</a></sup>.</p>

<p>With the Tversky loss function, specificity (proportion of false positives [FP]) and sensitivity (proportion of false negatives [FN]) can be weighted by alpha (\(\alpha\)) and beta (\(\beta\)) values, respectively, with the requirement that the sum of the two values be equal to 1. Contrary to previous approaches where FNs were weighted higher than FPs as high sensitivity is typically preferred for imbalanced datasets, the \(\alpha\) and \(\beta\) weights were set to 0.6 and 0.4, respectively, in order to emphasize FPs <sup id="fnref:17"><a href="#fn:17" class="footnote" rel="footnote" role="doc-noteref">17</a></sup>. The hypothesis here is that, as the borders of tree canopies are the focus, if the model is encouraged to be slightly more conservative when predicting a pixel is part of a tree, then the model will be less likely to predict tree crown borders with high confidence, as they are most likely to be confused with the background.</p>

<p>To further de-incentivize the models to classify tree crown borders as trees, all prediction groups used in the Tversky loss calculation (TPs, FPs, and FNs), are multiplied by the weight maps, increasing the influence of the predictions at the tree crown boundaries or borders on the overall loss calculation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tversky</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.4</span><span class="p">):</span>
  <span class="s">"""Calculate the Tversky loss for imbalanced classes

  Args:
      y_true (tensor): Array of the ground truth data of size (m * n * 2) where the last axis is labels + weights
      y_pred (tensor): Array containing pixelwise predictions as logits
      alpha (float, optional): Weight of false positives. Defaults to 0.6.
      beta (float, optional): Weight of false negatives. Defaults to 0.4.

  Returns:
      float: Loss
  """</span>
  <span class="c1"># Labels
</span>  <span class="n">y_t</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_true</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

  <span class="c1"># Weights
</span>  <span class="n">y_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_true</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">ones</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">p0</span> <span class="o">=</span> <span class="n">y_pred</span>  <span class="c1"># Probability that pixels are class i
</span>  <span class="n">p1</span> <span class="o">=</span> <span class="n">ones</span> <span class="o">-</span> <span class="n">y_pred</span>  <span class="c1"># Probability that pixels are not class i
</span>  <span class="n">g0</span> <span class="o">=</span> <span class="n">y_t</span>  <span class="c1"># Ground truth
</span>  <span class="n">g1</span> <span class="o">=</span> <span class="n">ones</span> <span class="o">-</span> <span class="n">y_t</span>

  <span class="n">tp</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_weights</span> <span class="o">*</span> <span class="n">p0</span> <span class="o">*</span> <span class="n">g0</span><span class="p">)</span>
  <span class="n">fp</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_weights</span> <span class="o">*</span> <span class="n">p0</span> <span class="o">*</span> <span class="n">g1</span><span class="p">)</span>
  <span class="n">fn</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_weights</span> <span class="o">*</span> <span class="n">p1</span> <span class="o">*</span> <span class="n">g0</span><span class="p">)</span>

  <span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.00001</span>
  <span class="n">numerator</span> <span class="o">=</span> <span class="n">tp</span>
  <span class="n">denominator</span> <span class="o">=</span> <span class="n">tp</span> <span class="o">+</span> <span class="n">fp</span> <span class="o">+</span> <span class="n">fn</span> <span class="o">+</span> <span class="n">EPSILON</span>
  <span class="n">score</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

  <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</code></pre></div></div>

<p><em>Model training</em></p>

<p>The input size for the U-Net models was set to 256x256 pixels, and the ratio of training to validation patches was approximately 9 to 1. In addition to the border-weighted Tversky loss function described above, the learning rate was dynamically optimized during training using the ADADELTA optimizer <sup id="fnref:18"><a href="#fn:18" class="footnote" rel="footnote" role="doc-noteref">18</a></sup>. Each model was trained for 200 epochs with a batch size of 16.</p>

<p>In total, seven models were trained using different combinations of training label type (ORIG, ERODED) and weight map (RONN, BOUNDS10, BORD10, ALL1). It should be noted that RONN + ERODED training was not performed due to initial results clearly suggesting that the other three weight maps significantly outperformed RONN training, which is why only seven models will be presented below.</p>

<h2 id="tree-segmentation-from-deep-learning-model-predictions">Tree segmentation from deep learning model predictions</h2>

<p>For each trained model, predictions were performed on the full set of test data containing 1,193 trees over a 0.5-km<sup>2</sup> section of Berlin. The two-class output (0 or 1; “tree” or “not tree”) was then processed using several morphological algorithms in order to segment individual trees. First, a Euclidean distance transform was applied, after which local maxima were identified. Six minimum distances were tested to determine the optimal locations of local maxima for all models. Next, each local maximum “marker” was assigned a unique label value, and then watershed segmentation was performed using the inverted distances with the markers as the seed points.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_trees</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">min_dist</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="s">"""Locates individual tree labels via watershed segmentation of a binary prediction
  image.

  Args:
      y_pred (ndarray): Image containing binary predictions

  Returns:
      ndarray: Image containing segmented trees
  """</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">y_pred</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

  <span class="c1"># Calculate the distance transform
</span>  <span class="n">distance</span> <span class="o">=</span> <span class="n">ndi</span><span class="p">.</span><span class="n">distance_transform_edt</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

  <span class="c1"># Get local maxima 
</span>  <span class="n">coords</span> <span class="o">=</span> <span class="n">peak_local_max</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="n">min_distance</span><span class="o">=</span><span class="n">min_dist</span><span class="p">)</span>

  <span class="c1"># Collect the local maxima coordinates and generate unique labels for each
</span>  <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">distance</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
  <span class="n">mask</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">coords</span><span class="p">.</span><span class="n">T</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">True</span>
  <span class="n">markers</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ndi</span><span class="p">.</span><span class="n">label</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>

  <span class="c1"># Perform watershed segmentation and return resulting labels and regionprops
</span>  <span class="n">labels</span> <span class="o">=</span> <span class="n">watershed</span><span class="p">(</span><span class="o">-</span><span class="n">distance</span><span class="p">,</span> <span class="n">markers</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
  <span class="n">labels</span> <span class="o">=</span> <span class="n">label</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
  <span class="n">regions</span> <span class="o">=</span> <span class="n">regionprops</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">labels</span><span class="p">,</span> <span class="n">regions</span>
</code></pre></div></div>

<p>At this point, the resulting labels still represented the exact extent of original output of the models—only the label values had been changed. However, because some of the models were encouraged to incorrectly identify all tree pixels, two additional morphological manipulations were applied, namely, convex hull and dilation. As some of the predicted trees were not always “tree-like” in shape, each label was reshaped into its corresponding convex hull. Next, the labels were “re-grown” (dilated) to account for the “shrunken” clusters of tree pixels that were predicted without tree crown borders by models trained on eroded labels and/or with border or boundary-excluding weight maps. For most of these morphological operations scikit-image and SciPy were used <sup id="fnref:13:2"><a href="#fn:13" class="footnote" rel="footnote" role="doc-noteref">13</a></sup>, <sup id="fnref:19"><a href="#fn:19" class="footnote" rel="footnote" role="doc-noteref">19</a></sup>.</p>

<p>To determine the optimal combination of minimum distance and morphological manipulation, tree count absolute error (TCAE) and binary IoU (bIoU) were calculated for all combinations, and the optimal values were used for final ensemble predictions (Figures 6 and 7). TCAE was calculated as \(\frac{\lvert y_{true} - y_{pred} \rvert}{y_{true}} \cdot 100\).</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/min-dist_by_abs-tree-error-pct.png" />
  <figcaption><b>Figure 5.</b> TCAE as minimum distance for local maxima selection increases.</figcaption>
</figure>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/morph_vs_biou.png" />
  <figcaption><b>Figure 7.</b> Effect of label set (during training) and morphological adjustment (applied to trained model predictions) on overall bIoU. Values for “no erosion” and “eroded” represent the bIoU of unmodified predictions compared to the ORIG and ERODED label sets, respectively, while “best dist chull” and “best dist dilated” refer to bIoUs of predictions that have first had watershed segmentation applied using their optimal distances and have then had convex hull and region-growing (dilation) operations applied, respectively.</figcaption>
</figure>

<h1 id="results">Results</h1>

<h2 id="overall-performance">Overall performance</h2>

<p>To test the efficacy of the above methodology, in addition to assessing core model performance statistics, the influence of four key components was examined: i) whether models were trained on original or eroded label sets; ii) weight map type; iii) minimum distance threshold for local maxima calculation; and iv) convex hull and dilation. The best model/post-processing ensembles were then identified based on tree count absolute error (TCAE), tree area distributions, and overall bIoU (Table 1).</p>

<figure>
  <table class="dataframe" style="display: inline-table;">
    <thead>
      <tr class="best-row" style="text-align: right;">
        <th>Label Set</th>
        <th>Weights</th>
        <th>Best Min-Dist</th>
        <th>Best Morph</th>
        <th>bIoU</th>
        <th>Tree Absolute Error</th>
        <th>KS-Test <em>p</em></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>ORIG</td>
        <td>RONNN</td>
        <td>3</td>
        <td>dilated</td>
        <td>0.740</td>
        <td>18.52%</td>
        <td>0.639</td>
      </tr>
      <tr class="best-row">
        <td>ORIG</td>
        <td>BOUNDS10</td>
        <td>5</td>
        <td>chull</td>
        <td>0.799</td>
        <td class="best"><b>1.84%</b></td>
        <td>0.942</td>
      </tr>
      <tr>
        <td>ORIG</td>
        <td>BORD10</td>
        <td>5</td>
        <td>original</td>
        <td>0.798</td>
        <td>6.12%</td>
        <td>0.104</td>
      </tr>
      <tr class="best-row">
        <td>ORIG</td>
        <td>ALL1</td>
        <td>5</td>
        <td>chull</td>
        <td class="best"><b>0.801</b></td>
        <td>4.44%</td>
        <td>0.855</td>
      </tr>
      <tr>
        <td>ERODED</td>
        <td>BOUNDS10</td>
        <td>5</td>
        <td>chull</td>
        <td>0.777</td>
        <td>4.69%</td>
        <td>0.501</td>
      </tr>
      <tr class="best-row">
        <td>ERODED</td>
        <td>BORD10</td>
        <td>9</td>
        <td>dilated</td>
        <td>0.797</td>
        <td>2.85%</td>
        <td class="best"><b>0.954</b></td>
      </tr>
      <tr>
        <td>ERODED</td>
        <td>ALL1</td>
        <td>7</td>
        <td>chull</td>
        <td>0.781</td>
        <td>5.62%</td>
        <td>0.933</td>
      </tr>
    </tbody>
  </table>


  <figcaption><b>Table 1.</b> Core model + post-processing ensemble statistics. Best Min-Dist refers to the minimum distance used for determining local maxima prior to watershed segmentation, Best Morph is the morphological operations which provided the greatest bIoU (performed after watershed segmentation with Best Min-Dist, and bIoU, Tree [Count] Absolute Error, and KS-Test <em>p</em>-values were calculated from the resulting instance segmentation. In this case, higher KS-Test <em>p</em>-values are better as they suggest a closer relationship between the observed tree area distribution and the predicted distribution.</figcaption>
</figure>

<p>The highest bIoU was produced by ORIG + ALL1, which was expected as there were no penalties for predictions at tree borders. ORIG + BOUNDS10 had the lowest TCAE, and ERODED + BORD10 had the highest tree area distribution similarity. The poorest performances came from ORIG + RONNN and ORIG + BORD10. In all but one case, implementing at least the convex hull operation improved bIoU, while, of the models trained on ERODED label sets only the BORD10 weight map predictions benefited from dilation. This is understandable, as pixels classified as “border” pixels are closer to the center of mass of the tree than the boundaries, therefore allowing for greater separation between tree pixel clusters than boundary pixels.</p>

<h2 id="tree-area-histograms">Tree area histograms</h2>

<p>TCAE only provides a shallow sense of quality of the instance segmentation, however, and may be indicative of the real relationship between the predicted trees and their observed counterparts. To better understand this relationship, the distribution of predicted trees by their area was compared with the observed (true) tree area distribution and Kolmogorov-Smirnov test (KS-Test) \(p\)-value significances were computed (Figure 8).</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/tree_area_hist.png" />
  <figcaption><b>Figure 8.</b> Tree area distributions of predictions (“y_pred”) overlaid with the observed distribution (“y_true”). Purple indicates overlapping bars, and <em>p</em>-values indicate the possibility of rejecting the NULL hypothesis that the two histograms do not come from the same distribution.</figcaption>
</figure>

<p>Here it can be seen that, of the highest performers, ERODED + BORD10 produces the closest-aligned output, as all other models greatly overestimate small-area trees.</p>

<h2 id="visual-inspection">Visual inspection</h2>

<p>Further visual inspection of the ensemble predictions suggests that this difference is perhaps more significant than the higher bIoU and lower TCAE of ORIG + BOUNDS10, as the trees of ERODED + BORD10 appears more appropriately segmented (Figures 9 and 10).</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/best_preds_abserr.png" />
  <figcaption><b>Figure 9.</b> Visual plots of ensemble predictions overlaid on a sample of the original RGB image. Y_True indicates the observed test labels.</figcaption>
</figure>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/full_prediction.png" />
  <figcaption><b>Figure 10.</b> Comparison of true labels (y_true) with full tile output of the ERODED + BORD10 ensemble.</figcaption>
</figure>

<h1 id="discussion-and-conclusions">Discussion and conclusions</h1>

<p>Overall, the above exploration suggests that training a model on eroded labels with border (not boundary) weights may be able to produce better ensemble segmentation results than with the original label set and thresholded boundary weights, as has been done in previous tree detection attempts <sup id="fnref:5:2"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>\(^,\)<sup id="fnref:6:2"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. Further, convex hull transformation (at the least) followed by label dilation can produce prediction output with nearly identical coverage of tree pixels while still allowing for accurate instance segmentation of U-Net-generated semantic segmentation output. The model trained on eroded labels with border weights and modified with both convex hull and dilation transformations was ultimately able to produce instance segmentation with a resulting bIoU only 0.004 less than the unweighted model trained on uneroded labels (0.797 compared to 0.801).</p>

<p>That said, this preliminary investigation could be improved in several ways. First, K-fold cross-validation was not performed during model training, and so model resilience is not reflected here. Additionally, the training labels generated semi-automatically contain many errors and some instances of unrealistic tree segmentation, and the use of the same set of higher-quality labels, all drawn from the same geographic location, for both validation and testing can be problematic when evaluating model performance as it is unlikely to result in a well-generalized model. Furthermore, the validation/test label set was generated in patches, which led to sometimes mis-matched or truncated labels when trees spanned the borders of multiple tiles. These latter issues are matters of time and labor, however, and could be resolved with investment in higher-quality label sets across broader swaths of Berlin. With these issues in mind, however, the resulting model performance remained surprisingly accurate, and it is likely that resolving them would result in even better tree detection.</p>

<h1 id="references">References</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>T. Kattenborn, J. Leitloff, F. Schiefer, and S. Hinz, “Review on Convolutional Neural Networks (CNN) in vegetation remote sensing,” ISPRS J. Photogramm. Remote Sens., vol. 173, pp. 24–49, Mar. 2021, doi: 10.1016/j.isprsjprs.2020.12.010. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>M. Reichstein et al., “Deep learning and process understanding for data-driven Earth system science,” Nature, vol. 566, no. 7743, Art. no. 7743, Feb. 2019, doi: 10.1038/s41586-019-0912-1. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional Networks for Biomedical Image Segmentation,” in Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, Eds., in Lecture Notes in Computer Science. Cham: Springer International Publishing, 2015, pp. 234–241. doi: 10.1007/978-3-319-24574-4_28. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:4">
      <p>M. Perslev, E. B. Dam, A. Pai, and C. Igel, “One Network to Segment Them All: A General, Lightweight System for Accurate 3D Medical Image Segmentation,” in Medical Image Computing and Computer Assisted Intervention – MICCAI 2019, D. Shen, T. Liu, T. M. Peters, L. H. Staib, C. Essert, S. Zhou, P.-T. Yap, and A. Khan, Eds., in Lecture Notes in Computer Science. Cham: Springer International Publishing, 2019, pp. 30–38. doi: 10.1007/978-3-030-32245-8_4. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>M. Brandt et al., “An unexpectedly large count of trees in the West African Sahara and Sahel,” Nature, vol. 587, no. 7832, Art. no. 7832, Nov. 2020, doi: 10.1038/s41586-020-2824-5. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:5:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:6">
      <p>M. Mugabowindekwe et al., “Nation-wide mapping of tree-level aboveground carbon stocks in Rwanda,” Nat. Clim. Change, pp. 1–7, Dec. 2022, doi: 10.1038/s41558-022-01544-w. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:6:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:6:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:7">
      <p>R. Girshick, “Fast R-CNN,” presented at the Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1440–1448. Accessed: Oct. 17, 2022. [^Online]. Available: https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2015. Accessed: Oct. 17, 2022. [^Online]. Available: https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p>K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask R-CNN.” arXiv, Jan. 24, 2018. doi: 10.48550/arXiv.1703.06870. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10">
      <p>“Geoportal Berlin / Digitale farbige TrueOrthophotos 2020 (TrueDOP20RGB) - Sommerbefliegung.” Accessed: Sep. 15, 2022. [^License: dl-de/by-2-0 (http://www.govdata.de/dl-de/by-2-0)]. Available: https://fbinter.stadt-berlin.de/fb/wms/senstadt/k_luftbild2020_true_rgb <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11">
      <p>“Geoportal Berlin / Digitale Color-Infrarot TrueOrthophotos 2020 (TrueDOP20CIR) - Sommerbefliegung.” Accessed: Sep. 15, 2022. [^License: dl-de/by-2-0 (http://www.govdata.de/dl-de/by-2-0)]. Available: https://fbinter.stadt-berlin.de/fb/wms/senstadt/k_luftbild2020_true_cir <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12">
      <p>“Geoportal Berlin / Airborne Laserscanning (ALS) - Primäre 3D Laserscan-Daten.” Accessed: Sep. 15, 2022. [^License: dl-de/by-2-0 (http://www.govdata.de/dl-de/by-2-0)]. Available: https://fbinter.stadt-berlin.de/fb/feed/senstadt/a_als <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:13">
      <p>S. van der Walt et al., “scikit-image: image processing in Python,” PeerJ, vol. 2, p. e453, Jun. 2014, doi: 10.7717/peerj.453. <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:13:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:13:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:14">
      <p>N. Sofroniew et al., “napari: a multi-dimensional image viewer for Python.” Zenodo, Nov. 03, 2022. doi: 10.5281/zenodo.7276432. <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:15">
      <p>M. A. Rahman and Y. Wang, “Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation,” in Advances in Visual Computing, G. Bebis, R. Boyle, B. Parvin, D. Koracin, F. Porikli, S. Skaff, A. Entezari, J. Min, D. Iwai, A. Sadagic, C. Scheidegger, and T. Isenberg, Eds., in Lecture Notes in Computer Science. Cham: Springer International Publishing, 2016, pp. 234–244. doi: 10.1007/978-3-319-50835-1_22. <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:16">
      <p>S. S. M. Salehi, D. Erdogmus, and A. Gholipour, “Tversky loss function for image segmentation using 3D fully convolutional deep networks.” arXiv, Jun. 18, 2017. doi: 10.48550/arXiv.1706.05721. <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:17">
      <p>N. Abraham and N. M. Khan, “A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation.” arXiv, Oct. 17, 2018. doi: 10.48550/arXiv.1810.07842. <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:18">
      <p>M. D. Zeiler, “ADADELTA: An Adaptive Learning Rate Method.” arXiv, Dec. 22, 2012. doi: 10.48550/arXiv.1212.5701. <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:19">
      <p>P. Virtanen et al., “SciPy 1.0: fundamental algorithms for scientific computing in Python,” Nat. Methods, vol. 17, no. 3, Art. no. 3, Mar. 2020, doi: 10.1038/s41592-019-0686-2. <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>

        
      </section>

      <footer class="page__meta">
        
        
  


  
  
  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      
      
      <a href="/tags/#cnn" class="page__taxonomy-item" rel="tag">CNN</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#machine-learning" class="page__taxonomy-item" rel="tag">machine learning</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#orthophotos" class="page__taxonomy-item" rel="tag">orthophotos</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#tree-detection" class="page__taxonomy-item" rel="tag">tree detection</a><span class="sep">, </span>
    
      
      
      <a href="/tags/#u-net" class="page__taxonomy-item" rel="tag">U-Net</a>
    
    </span>
  </p>




        
          <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2023-05-01T00:00:00+02:00">May 1, 2023</time></p>
        
      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Understanding+weight+maps+and+label+manipulation+in+tree+detection+from+high-resolution+orthophotos+with+U-Net%20http%3A%2F%2Flocalhost%3A4000%2Fposts%2F2023%2F05%2Fweight-maps-label-manipulation-tree-detection-unet" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F2023%2F05%2Fweight-maps-label-manipulation-tree-detection-unet" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fposts%2F2023%2F05%2Fweight-maps-label-manipulation-tree-detection-unet" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/posts/2023/04/ICESat-2" class="pagination--pager" title="Geometric-based filtering of ICESat-2 ATL03 data for ground-profile retrieval in the Quebrada del Toro, Argentina
">Previous</a>
    
    
      <a href="/posts/2023/05/Conda" class="pagination--pager" title="Installing conda environments for spatial and remote-sensing analyses
">Next</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/CameraCalibration1_calibio/" rel="permalink">Camera Calibration (1): Comparison of boards and parameters with calib.io
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">Precise Camera Calibration is an important part of generating high-quality points clouds and mesh models. We outline some basic procedures to generate reliab...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/VCM03-DavidHersh/" rel="permalink">Apple segmentation in 3D point clouds
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">This internship aimed at segmenting apples from a high-resolution Terrestrial Lidar Scan (TLS) Point Cloud with a simple clustering approach and a deep learn...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/ManTuenChan_UNET_pebble_segmentation/" rel="permalink">A U-Net-SegmentAnything 2-pass approach with stereo-depth sensing for river-pebble panoptic segmentation
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">Introduction
Sediment charateristics and grain-size distribution carries important information on the drainage system, the ecosystem, and the weather conditi...</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title" itemprop="headline">
      
        <a href="/Kathmandu_Workshop_Feb2024/" rel="permalink">Climate Data Collection and Analysis, Kathmandu, Feb 26-27 2024
</a>
      
    </h2>
    
    <p class="archive__item-excerpt" itemprop="description">Workshop organized and led by Dr Taylor Smith (University of Potsdam, Germany), Dr Bodo Bookhagen (University of Potsdam, Germany) and Dr Shakil Regmi (South...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
          <li><a href="https://github.com/UP-RS-ESP" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://gitup.uni-potsdam.de/RS-ESP" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-gitlab" aria-hidden="true"></i> GitLab</a></li>
        
      
        
      
        
      
    

    <li><a href="https://www.uni-potsdam.de/de/datenschutzerklaerung.html">Datenschutzerklärung (German)</a></li>
    <li><a href="https://www.uni-potsdam.de/en/data-protection-declaration.html">Data Protection Declaration (Englisch)</a></li>
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2025 Remote Sensing and Earth Surface Processes. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







    

  





  </body>
</html>
