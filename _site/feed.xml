<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2025-09-07T10:09:17+02:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">VACON</title><subtitle>Team webpage of the Remote Sensing and Earth Surface Processes group at the University of Potsdam</subtitle><author><name>Remote Sensing and Earth Surface Processes</name><email>bodo.bookhagen@uni-potsdam.de</email></author><entry><title type="html">Camera Calibration (1): Comparison of boards and parameters with calib.io</title><link href="http://localhost:4000/CameraCalibration1_calibio/" rel="alternate" type="text/html" title="Camera Calibration (1): Comparison of boards and parameters with calib.io" /><published>2025-04-27T00:00:00+02:00</published><updated>2025-04-27T00:00:00+02:00</updated><id>http://localhost:4000/CameraCalibration1_calibio</id><content type="html" xml:base="http://localhost:4000/CameraCalibration1_calibio/"><![CDATA[<p>Precise Camera Calibration is an important part of generating high-quality points clouds and mesh models. We outline some basic procedures to generate reliable camera calibration parameters with the Calibrator software from calib.io and compare several setups, including fixed and moving camera photo taking, checkerboards and Charuco boards, and parameter optimization with Metashape Agisoft.</p>

<h1 id="introduction">Introduction</h1>

<p>There exist multiple ways to perform a camera calibration - <a href="https://opencv.org/">OpenCV</a> is likely one of the more commonly used approach in the academic sector. There are well documented approaches and codes available (e.g. <a href="https://docs.opencv.org/4.x/dc/dbb/tutorial_py_calibration.html">OpenCV Camera Calibration Tutorial</a>). There also exist commercial software packages often used in the industry. These work very well and sometimes come with their own calibration boards (e.g. <a href="https://www.mvtec.com/products/halcon">Halcon Operator Software</a> and <a href="https://www.mvtec.com/products/calibration-plates">Halcon Calibration Plates</a>). <a href="https://calib.io/">calib.io</a> focuses on producing <a href="https://calib.io/collections/products">camera calibration targets</a> for the computer vision community and the <a href="https://calib.io/products/calib">Calibrator software</a>.</p>

<p>We have previously elaborated in the github repository <a href="https://github.com/UP-RS-ESP/CameraCalibration">CameraCalibration</a> on calibration efforts with OpenCV and have made some recommendation for camera setup and photo shooting. We have updated some of these findings and also updated part of the code to visualize the distortion fields. In addition, there is a detailed description of camera calibration parameters and their robustness. The github repositry <a href="https://github.com/UP-RS-ESP/opencv-camera-model-comparison">opencv-camera-model-comparison</a> provides code and concept for estimating the ideal number of photos for camera calibration and which distortion model provides the most reliable results. This is part of Victoria Dergunova’s internship and the report is in German language.  In a future post, we will elaborate on a detailed OpenCV calibration pipeline.</p>

<p>In this short description, we use the Calibrator Software to generate camera calibration parameters.</p>

<p>All camera calibration xml files in OpenCV format are available at <a href="https://github.com/UP-RS-ESP/CameraCalibration/tree/main/calibrations">https://github.com/UP-RS-ESP/CameraCalibration/tree/main/calibrations</a> with the Sony 7RM7 data for the 35 and 55 mm lenses in <a href="https://github.com/UP-RS-ESP/CameraCalibration/tree/main/calibrations/sony_ilcm-7rm5">https://github.com/UP-RS-ESP/CameraCalibration/tree/main/calibrations/sony_ilcm-7rm5</a>.</p>

<h2 id="photo-taking-setup">Photo-Taking Setup</h2>
<p>We use a <a href="https://www.sony.com/electronics/support/e-mount-body-ilce-7-series/ilce-7rm5/specifications">Sony ILCE 7RM5</a> (<a href="https://en.wikipedia.org/wiki/Sony_%CE%B17R_V">wikipedia</a>) with a 35 mm and 50 mm (FE 50 mm F1.4GM) fixed lenses. We recorded photos in the highest resolution with 9504x6336 pixels (61 MP). Photos were taken with ISO 400, F/11, and exposure times ranging from 1/500 to 1/640 s. For a set of images, we used a tripod (photos labeled <em>fixed</em>) or a moving camera with fixed board (labeled <em>free</em>). All image stabilization and filtering options were turned off. The JPG file sizes are 50-60 MB each.</p>

<p>All calibration photos were taken by Florian Josephowitz.</p>

<h2 id="setting-up-camera-calibration-with-calibio">Setting up Camera Calibration with calib.io</h2>
<p>There exist several parameters in the calib.io Camera Calibrator software that are linked to the OpenCV implementation. During feature detection, we rely on the <em>homography symmetry</em> for subpixel refinement of checkerboards. The subpixel refinement for the Kalibr boards is carried out with a <em>polynomial fit</em>.</p>

<p>During the optimization processes, we use the following options:</p>
<ul>
  <li><em>Vanishing points</em> for the initial calibration</li>
  <li><em>Estimate Covariances</em> to have error estimates for the parameters</li>
</ul>

<p>We do <em>not</em> use:</p>
<ul>
  <li><em>Optimize Target Geometry</em> (because our targets are of high quality)</li>
  <li><em>Robust Norm</em> (because we use high quality images and assume they have low uncertainties, although this may help for Charuco targets)</li>
  <li><em>Only decreasing steps</em> (because we want to explore the full landscape of parameters)</li>
</ul>

<p>We point out that feature detection on the Charuco or Kalibr board tends to have slightly larger uncertainties (about 0.2 to 0.5 pixel higher in the relative pose error). For the Kalibr boards, we often active the <em>Robust Norm</em> because that remove points with an uncertainty larger than 1 pixel during the inversion.</p>

<p>It is a good strategy to start with a low number of camera distortion parameters. For example, starting out with free parameters for focal length (f) and the principal points (cx, cy) for the extrinsic parameters. For high-quality cameras it is reasonable to assume that the focal length is the same in the x and y direction. Initial setting for the distortion model may only consider k1 and k2 for the radial distortion. These are the second and forth order parameters for a radial fit. The sixth order (k3) parameter can be added in a second step - high-quality cameras often require a limited set of parameters. The tangential distortion caused by a tilted sensor may not be required for a high-quality camera and p1 and p2 can be added in the second iteration. After running an initial optimization with only 5 parameters: f, cx, cy, k1, and k2, you may want to add k2, p1, and p2 in a second iteration. The starting point of the second optimization process is the previous result. The starting point with only 2 distortion parameters is referred to as a 2-parameter model, while the second run with 5 distortion parameters (plus f, cx, cy) is referred to as 5-parameter model. Likely, the 5-parameter model will result in a lower overall error, but you would like to avoid overfitting (see <a href="https://github.com/UP-RS-ESP/opencv-camera-model-comparison/tree/main">opencv-camera-model-comparison</a>).</p>

<p>In the <em>Statistics tab</em> you should verify that the pooled statistics shows a low (&lt;1 pixel, better &lt; 0.5 or 0.2 pixel) Relative Pose Error (RPE). Most of the checkerboard calibrations with 100+ calibration photos resulted in a mean RPE between 0.1 and 0.2 pixel.</p>

<h2 id="calibration-boards-checkerboard-vs-charuco-or-kalibr">Calibration Boards: Checkerboard vs. Charuco (or Kalibr)</h2>
<p>The checkerboard setup is the most straight forward board to use. The detection of the black and white squares on the board is optimized with subpixel refinement approaches that allow a very precise detection of the squares. The detection is fast. The drawback is that the entire checkerboard will need to be included in the photo and all squares need to have a successful subpixel refinement - otherwise the detection process will not work and the photo will not be used. This makes it somewhat difficult to take photos with the checkerboard in the very corner of the camera field - but these are important for a successful calibration. Also, the detection process of the checkerboard corners is very sensitive to reflection and glare: Using the checkerboard in bright sunlight or with strong lights will likely lead to a failed detection process and hence no data points from these images. It may take some practice (and time) to take good photos with the checkerboard located in the far corners. The checkerboard we used for the camera calibration have 28x17 (columns x rows) with a 10 mm spacing and approximately A4 size.</p>

<p>The Charuco or Kalibr boards have individual markers for each field. This allows to take photos with only part of the board in the photo. This is beneficial for taking photos from the corner areas. However, the marker detection is much slower than for checkerboards and also less precise, because different subpixel refinement methods are chosen. The Charuco board is not that sensitive toward reflection, because if the subpixel detection fails for part of the board, the other corners are still used for the calibration process. The <a href="https://calib.io/products/kalibr-targets">Kalibr board</a> has 10 columns, 7 rows, and a spacing of 2.5 mm. These values will need to be entered manually. The subpixel refinement method is a <em>Polynomial fit</em>.</p>

<p>We find the checkerboards to be more straight forward to use for camera calibration, especially with a large number of photos (faster). But both boards produce comparable results.</p>

<p>We emphasize that the boards will need to be very smooth and even. Glueing a laser printed checkerboard on a cardboard will only result in a low-quality calibration board. This may work for a low resolution camera (2 or 8 MP), but will be imprecise for a high-resolution camera (42 or 61 MP). A board printed on aluminium or LDPE composite sheet is preferable. We printed boards on foam and this gives reasonable results for small calibration boards (A4 size), but the boards are easily bend during transport. A curved or deformed board should not be used for camera calibration.</p>

<center>
<figure>
  <a href="high_point_cloud.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/CameraCalibration1_images/Kalibr_parameters_col10_row7.png" /></a>
</figure>
</center>
<figcaption>Figure 1: Depending on the size of the board and spacing of features, the settings have to be manually adjusted. We have used these settings for our Kalibr board.</figcaption>

<h1 id="results-and-discussion">Results and Discussion</h1>

<p>We have performed several tests to elucidate the most practical setting for camera calibration. We emphasize that the camera calibration does not need to be done during every photo taking session, but it is useful to have reliable camera calibration parameters. The calibration results were processed with tools available in our github repository <a href="https://github.com/UP-RS-ESP/CameraCalibration">Camera Calibration</a>. The python code <em>calib-to-opencv.py</em> was used to convert the json output from the Calibrator software to OpenCV xml, and the code <em>compareDistortion_from_CC_xml.py</em> was used to create the figures shown below.</p>

<p>We tested the following scenarios:</p>
<ul>
  <li>Fixed vs. moving camera calibration</li>
  <li>Varying camera calibration parameters (only radial vs. radial and tangential parameters)</li>
  <li>Comparison of checkerboard vs. Charuco boards</li>
  <li>Optimization of an existing camera calibration file during Metashape Agisoft photo alignment step</li>
</ul>

<h2 id="fixed-and-moving-camera-calibration">Fixed and Moving-Camera Calibration</h2>
<p>There appears to be only very small differences between fixed and free-board calibrations. The derived distortion parameters are comparable, but in reduced light settings, you may want to use a tripod to avoid image blur.</p>

<center>
<figure>
    <a href="Sony_ILCE-7RM5_35mm_checkerboard_free_fixed_5p_25Apr2025_2panel.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/CameraCalibration1_images/Sony_ILCE-7RM5_35mm_checkerboard_free_fixed_5p_25Apr2025_2panel.png" /></a>
</figure>
</center>
<figcaption>Figure 2: Visualization of the radial and tangential camera calibration parameters for the 35 mm lens (Sony 7RM5). The blue cross marks the camera center and the orange triangle the (cx,cy) coordinate found during the optimization steps. Arrows indicate the direction of pixel offset when performing camera calibration. Colors and contour lines show magnitude of offset. There are about 20 pixels offsets in the far corners - this is low compared to the overall pixel size of the x and y axes. Calibration was carried out with more than 100 photos for each setting (fixed camera and moving camera). </figcaption>

<center>
<figure>
    <a href="Sony_ILCE-7RM5_35mm_checkerboard_free_fixed_5p_25Apr2025_2panel_diff.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/CameraCalibration1_images/Sony_ILCE-7RM5_35mm_checkerboard_free_fixed_5p_25Apr2025_2panel_diff.png" /></a>
</figure>
</center>
<figcaption>Figure 3: Offset difference between fixed and moving cameras. The absolute magnitude (left) shows the distortion difference in pixels for the two camera acquisition setups. The difference is low to moderate, given that the camera has 9504x6336 pixels. The factor difference (difference of both calibrations divided by fixed camera calibration) shows that there are some spots where the difference in camera calibration is about three times as high as distortion offset amount. This likely indicates that there were not enough calibration points taken in this area of the camera field.  </figcaption>

<center>
<figure>
    <a href="Sony_ILCE-7RM5_50mm_checkerboard_free_fixed_5p_25Apr2025_2panel_diff.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/CameraCalibration1_images/Sony_ILCE-7RM5_50mm_checkerboard_free_fixed_5p_25Apr2025_2panel_diff.png" /></a>
</figure>
</center>
<figcaption>Figure 4: Same as Figure 3, but for 50 mm lens. Again the difference between fixed and moving (free) camera results in comparable calibration results. The difference between the calibration parameters results in a low to moderate pixel offset. </figcaption>

<h2 id="radial-2-parameters-vs-radial-and-tangential-distortion-5-parameters-models">Radial (2 parameters) vs. Radial and Tangential Distortion (5 parameters) Models</h2>
<p>We explore using a limited number of camera calibration parameters to prevent overfitting and explore the lens optics.  We use a radial distortion model with only two parameters (k1 and k2 corresponding to a second and fourth order polynome describing the radial field) and no tangential distortion. We compare this to a 5-parameter distortion model that is often applied (k1, k2, k3 describe radial distortion and p1 and p2 describe tangential distortion). We refer to <a href="https://github.com/UP-RS-ESP/opencv-camera-model-comparison/tree/main">opencv-camera-model-comparison</a> for a more detailed treatment of this topic.</p>

<center>
<figure>
    <a href="Sony_ILCE-7RM5_35mm_checkerboard_free_2p5p_25Apr2025_2panel_diff.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/CameraCalibration1_images/Sony_ILCE-7RM5_35mm_checkerboard_free_2p5p_25Apr2025_2panel_diff.png" /></a>
</figure>
</center>
<figcaption>Figure 5: Difference between 2- and 5-parameter calibration models for the 35 mm lens with moving camera acquisitions. We note that the 5 parameter model with higher orders of polynomes better captures the steep curvature in the far corner. Overall, the difference is rather low, suggesting that a two parameter model may be sufficient for some cases.  </figcaption>

<center>
<figure>
    <a href="Sony_ILCE-7RM5_50mm_checkerboard_fixed_2p5p_25Apr2025_2panel_diff.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/CameraCalibration1_images/Sony_ILCE-7RM5_50mm_checkerboard_fixed_2p5p_25Apr2025_2panel_diff.png" /></a>
</figure>
</center>
<figcaption>Figure 6: Difference between 2- and 5-parameter calibration models for the 50 mm lens for a fixed setup. A longer focal length has lower distortion on the sides of the lens and the 2-parameter calibration is a reasonable model for this lens. However, in subsequent runs, we will rely on the 5-parameter model, because the overall RPE (relative pose error) is lower.  </figcaption>

<h2 id="checkerboard-vs-charuco-kalibr-board">Checkerboard vs. Charuco (Kalibr) Board</h2>
<p>A comparison between the checkerboard and Charuo board reveals that results are comparable. We note the relatively large pose error of the Charuco boards because of less precise subpixel corner detection and fewer calibration points.</p>

<center>
<figure>
    <a href="Sony_ILCE-7RM5_35mm_checkerboard_charuco_free_5p_25Apr2025_2panel_diff.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/CameraCalibration1_images/Sony_ILCE-7RM5_35mm_checkerboard_charuco_free_5p_25Apr2025_2panel_diff.png" /></a>
</figure>
</center>
<figcaption>Figure 7: Difference between the distortion model for the calibration performed with the checkerboard and Charuco board. Results for the 35 mm lens are shown. The calibration results tend to agree with generally low differences.  </figcaption>

<h2 id="refining-camera-calibration-parameters">Refining Camera Calibration parameters</h2>
<p>A reasonably reliable camera calibration can be used as a starting point for further refinement. Camera calibration relies on the detection of specific markers (e.g., squares), but SIFT features identified during structure from motion processing can also be used. Refining camera calibration within openMVS or Agisoft Metashape is really only useful if there is a very strong camera geometry (i.e., many photos taken from different angles - similar to a calibration processes). The advantage of a SIFT-based refinement is that there are usually many more features available to perform a calibration on. Here, we use the camera calibration parameters generated with the checkerboard and perform a photo alignment using SIFT features within Agisoft Metashape. We use the camera calibration as starting point and allow them to be refined during the optimization process. The setup consists of 124 photos for a 2x2 m area that results in a strong camera geometry. We furthermore have additional markers in the field (24 Agisoft markers and scales). An alternative way to look at this step is to validate the camera calibration.</p>

<p>We note that camera alignment with pre-calibrated lenses is faster and results in much better constrained tie points. There are fewer filtering steps necessary with the <em>Gradual Selection</em> tool and fewer points removed. The covariances of the tie points are lower than using the same setup without pre- calibration. The resulting calibration parameter differ, suggesting that the matrix inversion solving for camera position (6 parameters) and camera distortion (8 parameters) is not well constrained.</p>

<center>
<figure>
    <a href="Sony_ILCE-7RM5_50mm_checkerboard_fixed_MetashapeOptimized_5p_25Apr2025_2panel_diff.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/CameraCalibration1_images/Sony_ILCE-7RM5_50mm_checkerboard_fixed_MetashapeOptimized_5p_25Apr2025_2panel_diff.png" /></a>
</figure>
</center>
<figcaption>Figure 8: Difference between the distortion model generated from a fixed checkerboard setting and a Metashape Agisoft optimized camera calibration from a scene with strong camera geometry. Results for the 50 mm lens are shown. The optimization process in Agisoft Metashape only slightly changed the camera calibration parameters. The tie point cloud and the surface mesh model generated from depth map after the optimization step have high qualities. </figcaption>

<h1 id="conclusion">Conclusion</h1>

<p>A careful camera calibration results in faster and more accurate photo alignment during the structure from motion process. The point clouds and mesh surfaces generated within Metashape Agisoft with a pre-calibrated camera were more precise and contained less noise. The photo alignment step also ran faster. The optimized camera calibration parameters with a pre-calibrated camera are different than the optimized results without camera calibration. This suggests that the inversion step optimizing for camera position and distortion poses an ill defined matrix inversion problem. We suggest to always provide pre-calibration parameters.</p>

<p>Our results indicate that reliable results can be generated from various setups. Free or fixed camera setups provide similar camera calibration parameters. The photo taking with a moving camera is likely faster, although a tripod can provide extra stability in low-light condition. The type of board does not matter, but we prefer to use checkerboards for the ease of use and faster processing. We have obtained good results using the standard 5-parameter distortion model (3 radial and 2 tangential parameters), but we emphasize that a simpler, 2-parameter model performs equally well in some settings.</p>

<p>All camera calibration xml files in OpenCV format are available at <a href="https://github.com/UP-RS-ESP/CameraCalibration/tree/main/calibrations">https://github.com/UP-RS-ESP/CameraCalibration/tree/main/calibrations</a> with the Sony 7RM7 data for the 35 and 55 mm lenses in <a href="https://github.com/UP-RS-ESP/CameraCalibration/tree/main/calibrations/sony_ilcm-7rm5">https://github.com/UP-RS-ESP/CameraCalibration/tree/main/calibrations/sony_ilcm-7rm5</a>.</p>]]></content><author><name>Bodo Bookhagen</name><email>bodo.bookhagen@uni-potsdam.de</email></author><category term="Camera Calibration" /><category term="calib.io" /><category term="checkerboard" /><category term="Charuco" /><category term="Kalibr" /><category term="OpenCV" /><summary type="html"><![CDATA[Precise Camera Calibration is an important part of generating high-quality points clouds and mesh models. We outline some basic procedures to generate reliable camera calibration parameters with the Calibrator software from calib.io and compare several setups, including fixed and moving camera photo taking, checkerboards and Charuco boards, and parameter optimization with Metashape Agisoft.]]></summary></entry><entry><title type="html">Apple segmentation in 3D point clouds</title><link href="http://localhost:4000/VCM03-DavidHersh/" rel="alternate" type="text/html" title="Apple segmentation in 3D point clouds" /><published>2024-09-27T00:00:00+02:00</published><updated>2024-09-27T00:00:00+02:00</updated><id>http://localhost:4000/VCM03-DavidHersh</id><content type="html" xml:base="http://localhost:4000/VCM03-DavidHersh/"><![CDATA[<p>This internship aimed at segmenting apples from a high-resolution Terrestrial Lidar Scan (TLS) Point Cloud with a simple clustering approach and a deep learning approach.</p>

<h1 id="introduction">Introduction</h1>
<p>Detection of fruit in 3D point clouds can help aid in crop yield prediction and can be acheived through clustering and segmentation as shown by Mola et al., 2020. Li et al., 2022 used a deep learning approach to generate bounding boxes around apples and RANSAC to determine the centroid for robotic harvesting. This internship aimed at segmenting apples with simple clustering approach and a deep learning approach.</p>

<p><em>This internship was supervised by Prof. Dr. Bodo Bookhagen</em></p>

<h1 id="scanning-and-pre-processing">Scanning and Pre-processing</h1>

<p>The study site was the Streuobstwiese Golm located Northeast of the Golm campus, which has a few dozen apple and pear trees.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/study_location.jpg"><img align="right" width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/study_location.jpg" /></a>
 <figcaption> Figure 1. Location of the orchard near campus Golm.  </figcaption>
    </figure>

<p>The scans were taken of a single tree from approximately 12 perspectives. The scanner used was a Zoller+Froelich IMAGER® 5016A, which includes RGB and intensity information. Sphere control points from Zoller and Froelich were placed around the base of the tree along with paper targets to aid in the registration process.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/apple_site.jpg"><img align="right" width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/apple_site.jpg" /></a>
 <figcaption> Figure 2. Study site with registration targets.  </figcaption>
    </figure>

<p>The scans were then registered using the Z+F LaserControl® software. The software uses the sphere control points and paper targets to align the scans. The software also has a feature to remove noise from the point cloud. The final point cloud was then exported as a LAS file.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/registration.png?raw=True"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/registration.png" /></a>
 <figcaption>Figure 3. Automated removal during scanning of unreliable points based on pulse return shape. </figcaption>
    </figure>

<p>The final point cloud was clipped to remove ground points. Due to sunny conditions during scanning, some points, epecially on the top of the tree, had a purple hue (See figure 3).</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/appletree.png"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/appletree.png" /></a>
 <figcaption>Figure 4. Final point cloud after registrations and clipping. </figcaption>
    </figure>

<h1 id="segmentation">Segmentation</h1>

<p>Two general approaches to segmentation were tested. The first was using a combination of DBSCAN and RANSAC. The second was using a deep learning approach using KPConv.</p>

<h2 id="dbscan-and-ransac">DBSCAN and RANSAC</h2>

<p>Using <em>Density Based Spatial Clustering of Applications with Noise</em> (DBSCAN) and <em>Random Sampling and Consensus</em> (RANSAC) was chosen as a first approach due to the simplicity and the ability to easily use RANSAC to fit spheres to each apple. The main goal was to create clusters ideally with only a single apple. Then, using RANSAC, a sphere was fit to each cluster and the root mean squared error (RMSE) was calculated to find the best fitting sphere from the RANSAC iterations.</p>

<p>DBSCAN was implemented using the sklearn library using an $\epsilon$ value of 0.25 and a minimum of 100 points using a ball radius with sklearn. The $\epsilon$ value was tested with multiple values as this determines the maximum distance between two neighbors to be part of the same cluster (Sander et al., 1998).</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/dbscan.jpg"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/dbscan.jpg" /></a>
 <figcaption>Figure 5. DBSCAN clusters often include only one apple, but often a much larger area if a larger epsilon value was used.  </figcaption>
    </figure>

<p>Then, for each cluster a sample of 4 random points are chosen and a sphere is fit using Random Sampling and Consensus. The parameters for the RANSAC algorithm require some adjustments to match the density of the dataset, which could not be downsampled without losing significant detail. The chosen parameters were:</p>

<ul>
  <li>10,000 iterations per cluster</li>
  <li>Radius threshold of 0.03 meters (eg. if sphere from the 4 chosen points does not have a radius under 0.03 meters, then skip this iteration)</li>
  <li>A threshhold for inliers of 0.01 meters</li>
</ul>

<p>At each RANSAC iteration on a cluster, the Root Mean Squared Error is calculated as:</p>

\[\text{RMSE} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (d_i - r)^2}\]

<ul>
  <li>$n$ is the number of inlier points (points close to the sphere’s surface)</li>
  <li>$d_i$ is the Euclidean distance of the $i$-th point from the sphere’s center</li>
  <li>$r$ is the radius of the fitted sphere</li>
</ul>

<p>The final inlier points were chosen by the lowest RMSE where more that 150 points were inliers and the results saved to a PLY file as a new class. Lastly, the output is compared to a ground truth point cloud and evaluated using the following metrics:</p>

\[\text{Precision} = \frac{TP}{TP + FP}\]

\[\text{Recall} = \frac{TP}{TP+FN}\]

\[\text{F1} = 2 * \frac{Precision * Recall}{Precision + Recall}\]

<p>After trying DBSCAN and RANSAC with different combinations of values, the above parameters yielded the following scores:</p>

<table>
  <thead>
    <tr>
      <th>Metric</th>
      <th>Score</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Precision</td>
      <td>0.22</td>
    </tr>
    <tr>
      <td>Recall</td>
      <td>0.41</td>
    </tr>
    <tr>
      <td>F1</td>
      <td>0.28</td>
    </tr>
  </tbody>
</table>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/gt_vs_dbscanransac.png"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/gt_vs_dbscanransac.png" /></a>
 <figcaption>Figure 6. Ground truth (left) compared with DBSCAN+RANSAC results (right). </figcaption>
    </figure>

<p>Overall, the approach of using RANSAC and DBSCAN had a few significant drawbacks:</p>

<ul>
  <li>There were multiple parameters that required experimentation</li>
  <li>Even for this relatively small point cloud, the process was slow due to having to iterate over many clusters</li>
</ul>

<h2 id="deep-learning-approach-using-kpconv">Deep learning approach using KPConv</h2>

<p>An alternative approach using deep learning was explored. <em>Kernel Point Convolutions</em> (KPConv) was introduced by Thomas et al., 2019 and uses “Kernel Points” in a fashion analogous to Convolutional Neural Networks (CNN) and applies convolutions based on the relative position of points determined by a correlation coefficient as a way to avoid discretization into voxels or grids.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/kpconv.png?raw=True"><img width="1000" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/kpconv.png" /></a>
 <figcaption>Figure 7. Image convolution (left) and kernel convolution (right). Figure from Thomas et al., 2019. </figcaption>
    </figure>

<p>For the apple tree, KPConv was tested on half of the tree with ground truth labels of:</p>

<ol>
  <li>Branch</li>
  <li>Apple</li>
</ol>

<p>This approach utilized a fork of the <a href="https://github.com/HuguesTHOMAS/KPConv-PyTorch">PyTorch KPConv repository</a> and used a copy of the SenSatUrban dataset as a template and updating the class weights to reflect the apple tree dataset for calculating loss. The dataset was split into 5 parts, using 3 for training and 2 for validation and testing. KPConv can take additional features in additional to x,y,z coordinates, and included here was RGB. Kernel points can either be “rigid” where they are initialized in fixed positions, or “deformable” where they can adapt to the local geometry (Thomas et al., 2019). The deformable kernel points were used in this case.</p>

<p>The model training was done over 200 epochs using 12 kernel points and 5 cm kernel radius and without subsampling the original point cloud as it was sparse and KPConv does not require spatially uniform points. Training required around 3 hours on an NVIDIA Quadro M2200 GPU.</p>

<p>The results of KPConv were evaluated using Intersection over Union (IoU). The mean IoU for the full cloud was:</p>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>IoU</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Branch</td>
      <td>98.78</td>
    </tr>
    <tr>
      <td>Apple</td>
      <td>59.12</td>
    </tr>
    <tr>
      <td>Mean</td>
      <td>78.95</td>
    </tr>
  </tbody>
</table>

<p>Compared with the more manual approach of DBSCAN and RANSAC, KPConv shows a significant improvement.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/gt_vs_pred.jpg?raw=True"><img width="1000" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/DavidHersh_Apples/gt_vs_pred.jpg" /></a>
 <figcaption>Figure 8. Comparison of ground truth (left) and prediction (right). Areas of significant misclassification are circled. </figcaption>
    </figure>

<h1 id="conclusion">Conclusion</h1>

<p>The more simple approach of DBSCAN and RANSAC was difficult to apply due to the varying point cloud densities and multiple parameters that required tuning. This approach also does not scale well, but could have been sped up by initializing the 4 RANSAC points using a randomly initialized point and 3 nearest neighbors from a KD tree rather than being randomly initialized from a cluster. In comparison, the approach using KPConv yielded good results and could have been improved by adding additional input features, especially intensity. This approach generated a point-wise classification for the entire point cloud, so a final pass over the apple class in the point cloud using RANSAC similar to Li et al., 2022 could be used to generate final instance segmentations.</p>

<h1 id="references">References</h1>

<p>Gené-Mola, J., Gregorio, E., Cheein, F. A., Guevara, J., Llorens, J., Sanz-Cortiella, R., Escolà, A., &amp; Rosell-Polo, J. R. (2020). Fruit detection, yield prediction and canopy geometric characterization using LiDAR with forced air flow. Computers and Electronics in Agriculture, 168, 105121. https://doi.org/10.1016/j.compag.2019.105121</p>

<p>Sander, J., Ester, M., Kriegel, H., &amp; Xu, X. (1998). A Density-Based algorithm for discovering clusters in large spatial databases with noise. Data Mining and Knowledge Discovery, 2(2), 169–194. https://doi.org/10.1023/a:1009745219419</p>

<p>Thomas, H., Qi, C. R., Deschaud, J., Marcotegui, B., Goulette, F., &amp; Guibas, L. J. (2019, April 18). KPCONV: Flexible and Deformable Convolution for Point Clouds. arXiv.org. https://arxiv.org/abs/1904.08889</p>

<p>Li, T., Feng, Q., Qiu, Q., Xie, F., &amp; Zhao, C. (2022). Occluded Apple Fruit Detection and Localization with a Frustum-Based Point-Cloud-Processing Approach for Robotic Harvesting. Remote Sensing, 14(3), 482. https://doi.org/10.3390/rs14030482</p>]]></content><author><name>David Hersh</name></author><category term="Terrestrial laser scanning" /><category term="LiDAR" /><category term="Point Clouds" /><category term="segmentation" /><summary type="html"><![CDATA[This internship aimed at segmenting apples from a high-resolution Terrestrial Lidar Scan (TLS) Point Cloud with a simple clustering approach and a deep learning approach.]]></summary></entry><entry><title type="html">A U-Net-SegmentAnything 2-pass approach with stereo-depth sensing for river-pebble panoptic segmentation</title><link href="http://localhost:4000/ManTuenChan_UNET_pebble_segmentation/" rel="alternate" type="text/html" title="A U-Net-SegmentAnything 2-pass approach with stereo-depth sensing for river-pebble panoptic segmentation" /><published>2024-03-18T00:00:00+01:00</published><updated>2024-03-18T00:00:00+01:00</updated><id>http://localhost:4000/ManTuenChan_UNET_pebble_segmentation</id><content type="html" xml:base="http://localhost:4000/ManTuenChan_UNET_pebble_segmentation/"><![CDATA[<h1 id="introduction">Introduction</h1>
<p>Sediment charateristics and grain-size distribution carries important information on the drainage system, the ecosystem, and the weather condition (Soloy et al., 2020; Wang et al., 2019). Unfortunately, traditional methods to manually collect these information are costly, labor intensive, and time consuming. Over the years, various techniques have been developed seeking to reduce manual input through machine learning. This project is an attempt to utilize an U-Net-SAM 2-pass approach in conjunction with stereo-depth estimation to automize image based sampling.</p>

<p><em>This internship was supervised by Prof. Dr. Bodo Bookhagen.</em></p>

<h1 id="method">Method</h1>
<h2 id="approach">Approach</h2>
<p>This project approached the challenge from 2 angles. First, we introduce a depth layer estimated through stereo disparity in addition to RGB. The rationale is to provide additional morphological information to improve the segmentatino accuracy. The use of stereo vision also provides the additional benefit that accurate size measurements is readily available and segmented results can easily be applied to the point cloud through the disparity map (Mustafah et al., 2012).</p>

<p>Second, we combine instance segmentation (Meta’s Segment-Anything) and semantic segmentation (U-Net) through a 2-pass approach to achieve panoptic segmentation of grains for the extraction of individual grain samples. U-Net is a powerful tool capable of picking up features at different scale. Yet, while U-Net is capable of producing reliable results, it only performs a semantic segmentation. To separately sample individual grains, a second pass instance segmentation can be performed on top of the first pass U-Net result. The Meta Segment-Anything Model (SAM) was used in this project to perform the second pass segmentation. SAM is a pretrained model developed by Meta Platforms, Inc. (formerly Facebook). SAM was trained with large amount of data (1 billion masks and 11 million images) and evaluation of the model have shown that it provides reliable zero-shot performance (Kirillov et al., 2023). Should it performs reliably without the need of fine-tuning, significant time and computation power can be saved.</p>

<h2 id="data-collection">Data collection</h2>
<div style="display: flex; flex-direction: row; justify-content: center;">
    <figure style="flex: 1; margin-right: 0px;">
        <a href="high_imgL.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/high_imgL.jpg" /></a>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="high_imgR.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/high_imgR.jpg" /></a>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="high_depth.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/high_depth.jpg" /></a>
    </figure>
</div>
<figcaption>Figure 1: Example of the pebble setup with high-angle images: left, right, and depth images.</figcaption>

<p>The Stereolabs ZED 2i stereo camera together with their software ZED SDK 4.0 was used to collect the data used in this project. The pebble setup (Figures 1 and 2) was used as study sample to collect a total of 25 images. These 25 images were taken at 5 different angles, namely 1 (near-)nadir view and 4 top-down oblique views (backward, forward, left, and right). Each top-down oblique view were taken at 3 different angles, giving high, mid, and low-angle view images. In total that adds up to 12 oblique images. While all remaining 13 images were take in nadir view, each nadir image were taken in different lighting condition. That includes variation in lighting intensity, direction, and number light source.</p>

<div style="display: flex; flex-direction: row; justify-content: center;">
    <figure style="flex: 1; margin-right: 0px;">
        <a href="low_imgL.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/low_imgL.jpg" /></a>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="low_imgR.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/low_imgR.jpg" /></a>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="low_depth.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/low_depth.jpg" /></a>
    </figure>
</div>
<figcaption>Figure 2: Example of the pebble setup with low-angle images: left, right, and depth images.</figcaption>
<p><br /></p>

<p>ZED 2i was used to retreive the RGB image, the depth estimation and the reprojected point cloud (Figure 3). Depth sensing was performed using the neural depth mode and stored in meter unit. This mode utilize model trained by Stereolabs to fill gaps and correct for noise. The estimated depth map was aligned to the image captured by the left sensor and comes in the HD2K resolution (2208x1242). For all 25 images, each image and measurement were average of 30 exposures to provide consistency and hand labels using Napari. As shown in figure 3, depth sensing can be quite challenging at steep slopes, overhangs, and edges where a drastic change in distance can be found. There were visble signs of artifacts that is possibly the result of the model’s attempt to interpolate and smooth the transition. At a lower angle, shadowing can also be seen. Combined with the gap filling artifacts and the limitation of hardware resolution, the use of depth maps may actually hinder the segmentation instead of benefit it when it comes to lower angle shots.</p>

<div style="display: flex; flex-direction: row; justify-content: center;">
    <figure style="flex: 1; margin-right: 0px;">
        <a href="high_point_cloud.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/high_point_cloud.png" /></a>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="low_point_cloud.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/low_point_cloud.png" /></a>
    </figure>
</div>
<p><br /></p>
<figcaption>Figure 3: Example of the generated pebble-point cloud: high-left angle and low-left angle</figcaption>

<h2 id="model">Model</h2>
<p>Meta has provides 3 pretrained checkpoints for SAM. Three model have different neural network sizes, base ViT-B, large ViT-L, and huge ViT-H. ViT-H was used for the automatic mask generation with default settings.</p>

<p>The U-Net on the other hand was compiled using Adam algorithm with learning rate α = 0.1 for optimization. Binary cross-entropy loss function using binary IoU was chosen for binary segmentation and the threshold was set at 0.5. The training was performed using batch size B = 64 and epoch e = 100. In total 4 models were trained. 2 models were trained using 13 nadir images. 13 images were divided into 4500 128x128 patches, of which ~20% were retained for validation. The remaining (unseen) 12 oblique images were used as test images to evalue the model. They were not used in the training process. 2 models were trained using this dataset split, one with the depth layer and one without. For convenience, they were named split1D (with depth) and split1C (RGB only) respectively. The remaining 2 models were also differentiated by the use of depth layer but they were trained with a different dataset split. For these 2 models 9 nadir images and 4 oblique images (2 high angle, 1 mid angle, and 1 low angle) were used in the training instead of 13 nadir images. Following the same convention, they were named split2D and split2C.</p>

<p>The reason of using 2 different splits has to do with a concern regarding the dataset. While variations were made in the view angle and lighting conditions, all images were essentially capturing the same study sample. To examine if the training results can actually be generalized, split 1 and 2 restrict the view angles available to the training process as an attempt to evaluate the actual performance.</p>

<p>For all models, an 8 fold k-fold cross-validation was also performed. The training and validation dataset were shuffled before the split and all models uses the same split for each fold.</p>

<h1 id="results">Results</h1>
<h2 id="sam-segmentation">SAM segmentation</h2>
<center>
<figure>
    <a href="ZED_sam.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/ZED_sam.png" /></a>
</figure>
</center>
<figcaption> Figure 4: Segment-Anything segmentation of the pebble setup. </figcaption>

<p>The results of SAM segmentation were demonstrated below. SAM returns a binary mask for each individual segments and figure 4 shows an example of aggregated mask plotted with the original image. SAM model was trained with RGB photos, hence depth images were not used. Figure 5 plots the accuracy of the segmentation by image. Image 0 to 12 were nadir images with variation in lighting condition, 13-24 were oblique image with variation in view angle. The accuracy was assessed by pebble. Each segment was compared to the manually labeled masked in which each pebble was labeled with a different ID. Segments where pebble pixels were found was considered a pebble segment. In these segment, the pebble that takes up the most area of the segment was considered the main pebble. If the area of the main pebble inside the segment did not reach 70% of the pebbles total area, the segment will be considered as over-segmented. If the main pebble was not over-segmented and takes up at least 70% of the segment area, it is considered correctly segmented (or SAM segmented pebbles in figure 5). If non of these is true, it will be considered as under-segmented.</p>

<center>
<figure>
    <a href="sam_pixel_count.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/sam_pixel_count.png" /></a>
</figure>
</center>
<figcaption>Figure 5: Percentage of correctly segmented, over/under segmented, and not segmented grains in SAM automatic mask generation by image.</figcaption>

<h2 id="u-net-training-history-and-accuracy">U-Net training history and accuracy</h2>
<div style="display: flex; flex-direction: row; justify-content: center;">
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split1C_kfold0.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/history/pebble_unet_Split1C_kfold0.png" /></a>
        <figcaption>(a)</figcaption>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split1C_kfold0.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/history/pebble_unet_Split1C_kfold0.png" /></a>
        <figcaption>(b)</figcaption>
    </figure>
</div>
<div style="display: flex; flex-direction: row; justify-content: center;">
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split1C_kfold0.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/history/pebble_unet_Split1C_kfold0.png" /></a>
        <figcaption>(c)</figcaption>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split1C_kfold0.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/history/pebble_unet_Split1C_kfold0.png" /></a>
        <figcaption>(d)</figcaption>
    </figure>
</div>
<figcaption>Figure 6: U-Net Model training history of each split at kfold 0: (a) Split1C; (b) Split1D; (c) Split2C; (d) Split2D </figcaption>
<p><br />
Figure 6 shows the training history of all splits at the first fold. Accuracy accessed through precesion, accuracy, F1 score, and binary IoU. The metrics were only calculated for test images that the models have seen in the training process. Figure 7 shows the accuracy metrics by image at different threshold. Solid line shows the mean value over 8 folds k-fold and the shaded area highlights the 25th to 75th percentiles. For the view angle correponding to each test images please see Table 1. Figure 8 shows the the accuracy variation over the folds. Solid line represent the mean value over all test images with threshold=0.7 while the shaded area indicates the threhold = 0.5-0.9.</p>

<p>Table 1: Test images view angles</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Split</th>
      <th style="text-align: center">Nadir</th>
      <th style="text-align: center">Oblique high</th>
      <th style="text-align: center">Oblique mid</th>
      <th style="text-align: center">Oblique low</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center">split1</td>
      <td style="text-align: center">–</td>
      <td style="text-align: center">Image 0-3</td>
      <td style="text-align: center">Image 4-7</td>
      <td style="text-align: center">Image 7-11</td>
    </tr>
    <tr>
      <td style="text-align: center">split2</td>
      <td style="text-align: center">Image 8-11</td>
      <td style="text-align: center">Image 0-1</td>
      <td style="text-align: center">Image 2-4</td>
      <td style="text-align: center">Image 5-7</td>
    </tr>
  </tbody>
</table>

<div style="display: flex; flex-direction: column; justify-content: center;">
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split1C_image_accuracy.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/Accuracy_plot/pebble_unet_Split1C_image_accuracy.png" /></a>
        <figcaption>(a)</figcaption>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split1D_image_accuracy.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/Accuracy_plot/pebble_unet_Split1D_image_accuracy.png" /></a>
        <figcaption>(b)</figcaption>
    </figure>
</div>
<div style="display: flex; flex-direction: column; justify-content: center;">
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split2C_image_accuracy.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/Accuracy_plot/pebble_unet_Split2C_image_accuracy.png" /></a>
        <figcaption>(c)</figcaption>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split2D_image_accuracy.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/Accuracy_plot/pebble_unet_Split2D_image_accuracy.png" /></a>
        <figcaption>(d)</figcaption>
    </figure>
</div>
<figcaption>Figure 7: U-Net Model accuracy of each split assessed by image at 3 threshold. Shadowed area indicates the 25th to 75th percentile along kfolds: (a) Split1C; (b) Split1D; (c) Split2C; (d) Split2D</figcaption>

<div style="display: flex; flex-direction: row; justify-content: center;">
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split1C_kfold_consistency.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/Accuracy_plot/pebble_unet_Split1C_kfold_consistency.png" /></a>
        <figcaption>(a)</figcaption>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split1D_kfold_consistency.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/Accuracy_plot/pebble_unet_Split1D_kfold_consistency.png" /></a>
        <figcaption>(b)</figcaption>
    </figure>
</div>
<div style="display: flex; flex-direction: row; justify-content: center;">
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split2C_kfold_consistency.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/Accuracy_plot/pebble_unet_Split2C_kfold_consistency.png" /></a>
        <figcaption>(c)</figcaption>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_unet_Split2D_kfold_consistency.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/Accuracy_plot/pebble_unet_Split2D_kfold_consistency.png" /></a>
        <figcaption>(d)</figcaption>
    </figure>
</div>
<figcaption>Figure 8: U-Net model consistency of accuracy metrics over kfolds. Shadowed area indicates the 25th to 75th percentile along test images: (a) Split1C; (b) Split1D; (c) Split2C; (d) Split2D </figcaption>
<p><br />
Through the union of SAM mask and U-Net mask, point cloud of individual pebbles can be easily created. Figure 9 presents an example created through open3d <code class="language-plaintext highlighter-rouge">geometry.create_point_cloud_from_depth_image</code> and visualized in CloudCompare. Figure 9a visualize a RGB mesh generated from the point cloud and 9b visualize the point cloud colored by the distance between the grain and the camera with an estimate of size through calculating the distance between points. This was manually done through the CloudCompare ‘Point picking’ function and the estimate width is 13.3cm and the estimated hight is 10.1cm. Unfortunately, it is likely that this estimation is incorrect as the example presented here was not properly scaled. For the pint cloud to be correctly scaled, 4 calibration parameters are necessary. These are the x,y focal length, and principal points of the ZED 2i camera. These parameters are accessible through the camera API but unfortunately only the focal length were exported and saved before returning the camera to the University. Additionally, manually selecting the points for may raise the concern of objectivity and consistency. A better approach could be to lock for the major and minor axes as an indicator of the grain size. Nevertheless, segmentation results can be used create point clouds of samples individually and provides the oppotunity to analyse other physical charateristics such as surface smoothness and geometry.</p>

<div style="display: flex; flex-direction: row; justify-content: center;">
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_point_cloud_rgb.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/pebble_point_cloud_rgb.png" /></a>
        <figcaption>(a)</figcaption>
    </figure>
    <figure style="flex: 1; margin-right: 0px;">
        <a href="pebble_point_cloud.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/ManTuenChan_UNET/pebble_point_cloud.png" /></a>
        <figcaption>(b)</figcaption>
    </figure>
</div>
<figcaption>Figure 9: Point cloud of a pebble created from segmented depth map and RGB image with width and hight estimation: (a) RGB mesh and (b) Distance to camera in meter</figcaption>

<h1 id="conclusion-and-discussion">Conclusion and discussion</h1>
<p>This project is an attempt to utilize an U-Net-SAM 2-pass approach in conjunction with stereo-depth estimation to automize image based sampling. To assess the feasibility of this approach, the ZED 2i camera was used to capture a series of RGB and depth images from different angles and under different lighting condition. These images were manually labeled and segmented with the Meta SAM model.</p>

<p>The results of SAM segmentation were visually impressive yet the accuracy reveals that the quality of the results decreases when view angles steepen. The main issue seems to be the partial overlap that does not completely hide the pebbles. These mostly covered pebbles leave out a small edges that are still noticeable by human vision yet challenging for SAM to detect. One speculation as to why SAM is struggling with these partially covered pebbles is that it has to do with scale. Based on experience, SAM seems to favor the assumption that objects are at similar scale. It could be related to the use of point grid as input prompt, this assumption often lead to over-segmentation grains that are notably larger then the surroundings. As the view angle steepens more pebbles were partially covered, resulting in a higher percentage of non segmented pebbles. Another factor that stands out is the lighting condition. With strong artificial lighting there is a higher tendency of missing objects while the absence of artificial lighting does not really impact the performance. Nevertheless, with nadir images SAM is capable of producing reliable segmentation without the need of fine-tuning.</p>

<p>U-Net on the other hand, yields a mixed result. While the general stable and good accuracy suggest that the models were generally performing well, looking at split 1 models, the use of depth information apparently lead to larger fluctuation in performance across folds and a lower recall and F1 score. This likely suggested that depth information increases the likihood of producing flase negatives when the model was only trained with nadir images. Oppose to the expectation the depth information did not help the model to better generalize. The use of depth information in split 1 did not change how it struggles as the view angle differences increase or when the lighting was too strong. When compared to split 2, we can see that the inclusion of images take from different angles can visibly improve the performance. Here, even though hardly significant, the use of depth information slightly improved the performance and the stability across folds. Particularly at higher threshold, it also improved the models capability of handling challenging lower angle images and overexposed images. Take binary IoU as an indicator, split1D perform the worse with mean binary IoU at 0.72 across folds. Split1C shows a slight improvement at 0.78 while split2 models virtually performs equally weel at around 0.89.</p>

<p>The results suggest that the approach is a feasible method given that some variation in view angle is also included in the training dataset. The use of depth information did not benefit the segmentatino as much as expected, but it still retains its value by providing the option to perform size measurement and create point clouds. This project has a few limition. First, no variation in camera paramters were tested. The same is true for the U-Net hyperparameters. The study sample is also the most concerning limitation. All images were taken from the same study sample, which raises the question if it could also achieve comparable results in real world application. It is also unclear what causes the worse performance of split1D. Tackling these problems would be possible improvements to the proposed method.</p>

<h1 id="reference">Reference</h1>
<p>Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollár, P., &amp; Girshick, R. (2023). Segment Anything. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 3992–4003. https://doi.org/10.1109/ICCV51070.2023.00371</p>

<p>Mustafah, Y. M., Noor, R., Hasbi, H., &amp; Azma, A. W. (2012). Stereo vision images processing for real-time object distance and size measurements. 2012 International Conference on Computer and Communication Engineering (ICCCE), 659–663. https://doi.org/10.1109/ICCCE.2012.6271270</p>

<p>Soloy, A., Turki, I., Fournier, M., Costa, S., Peuziat, B., &amp; Lecoq, N. (2020). A Deep Learning-Based Method for Quantifying and Mapping the Grain Size on Pebble Beaches. Remote Sensing, 12(21), Article 21. https://doi.org/10.3390/rs12213659</p>

<p>Wang, C., Lin, X., &amp; Chen, C. (2019). Gravel Image Auto-Segmentation Based on an Improved Normalized Cuts Algorithm. Journal of Applied Mathematics and Physics, 7(3), Article 3. https://doi.org/10.4236/jamp.2019.73044</p>]]></content><author><name>Man Tuen Chan</name></author><category term="Stereo depth" /><category term="segmentation" /><category term="Segment-Anything" /><category term="U-Net" /><summary type="html"><![CDATA[Introduction Sediment charateristics and grain-size distribution carries important information on the drainage system, the ecosystem, and the weather condition (Soloy et al., 2020; Wang et al., 2019). Unfortunately, traditional methods to manually collect these information are costly, labor intensive, and time consuming. Over the years, various techniques have been developed seeking to reduce manual input through machine learning. This project is an attempt to utilize an U-Net-SAM 2-pass approach in conjunction with stereo-depth estimation to automize image based sampling. This internship was supervised by Prof. Dr. Bodo Bookhagen. Method Approach This project approached the challenge from 2 angles. First, we introduce a depth layer estimated through stereo disparity in addition to RGB. The rationale is to provide additional morphological information to improve the segmentatino accuracy. The use of stereo vision also provides the additional benefit that accurate size measurements is readily available and segmented results can easily be applied to the point cloud through the disparity map (Mustafah et al., 2012). Second, we combine instance segmentation (Meta’s Segment-Anything) and semantic segmentation (U-Net) through a 2-pass approach to achieve panoptic segmentation of grains for the extraction of individual grain samples. U-Net is a powerful tool capable of picking up features at different scale. Yet, while U-Net is capable of producing reliable results, it only performs a semantic segmentation. To separately sample individual grains, a second pass instance segmentation can be performed on top of the first pass U-Net result. The Meta Segment-Anything Model (SAM) was used in this project to perform the second pass segmentation. SAM is a pretrained model developed by Meta Platforms, Inc. (formerly Facebook). SAM was trained with large amount of data (1 billion masks and 11 million images) and evaluation of the model have shown that it provides reliable zero-shot performance (Kirillov et al., 2023). Should it performs reliably without the need of fine-tuning, significant time and computation power can be saved. Data collection Figure 1: Example of the pebble setup with high-angle images: left, right, and depth images. The Stereolabs ZED 2i stereo camera together with their software ZED SDK 4.0 was used to collect the data used in this project. The pebble setup (Figures 1 and 2) was used as study sample to collect a total of 25 images. These 25 images were taken at 5 different angles, namely 1 (near-)nadir view and 4 top-down oblique views (backward, forward, left, and right). Each top-down oblique view were taken at 3 different angles, giving high, mid, and low-angle view images. In total that adds up to 12 oblique images. While all remaining 13 images were take in nadir view, each nadir image were taken in different lighting condition. That includes variation in lighting intensity, direction, and number light source. Figure 2: Example of the pebble setup with low-angle images: left, right, and depth images. ZED 2i was used to retreive the RGB image, the depth estimation and the reprojected point cloud (Figure 3). Depth sensing was performed using the neural depth mode and stored in meter unit. This mode utilize model trained by Stereolabs to fill gaps and correct for noise. The estimated depth map was aligned to the image captured by the left sensor and comes in the HD2K resolution (2208x1242). For all 25 images, each image and measurement were average of 30 exposures to provide consistency and hand labels using Napari. As shown in figure 3, depth sensing can be quite challenging at steep slopes, overhangs, and edges where a drastic change in distance can be found. There were visble signs of artifacts that is possibly the result of the model’s attempt to interpolate and smooth the transition. At a lower angle, shadowing can also be seen. Combined with the gap filling artifacts and the limitation of hardware resolution, the use of depth maps may actually hinder the segmentation instead of benefit it when it comes to lower angle shots. Figure 3: Example of the generated pebble-point cloud: high-left angle and low-left angle Model Meta has provides 3 pretrained checkpoints for SAM. Three model have different neural network sizes, base ViT-B, large ViT-L, and huge ViT-H. ViT-H was used for the automatic mask generation with default settings. The U-Net on the other hand was compiled using Adam algorithm with learning rate α = 0.1 for optimization. Binary cross-entropy loss function using binary IoU was chosen for binary segmentation and the threshold was set at 0.5. The training was performed using batch size B = 64 and epoch e = 100. In total 4 models were trained. 2 models were trained using 13 nadir images. 13 images were divided into 4500 128x128 patches, of which ~20% were retained for validation. The remaining (unseen) 12 oblique images were used as test images to evalue the model. They were not used in the training process. 2 models were trained using this dataset split, one with the depth layer and one without. For convenience, they were named split1D (with depth) and split1C (RGB only) respectively. The remaining 2 models were also differentiated by the use of depth layer but they were trained with a different dataset split. For these 2 models 9 nadir images and 4 oblique images (2 high angle, 1 mid angle, and 1 low angle) were used in the training instead of 13 nadir images. Following the same convention, they were named split2D and split2C. The reason of using 2 different splits has to do with a concern regarding the dataset. While variations were made in the view angle and lighting conditions, all images were essentially capturing the same study sample. To examine if the training results can actually be generalized, split 1 and 2 restrict the view angles available to the training process as an attempt to evaluate the actual performance. For all models, an 8 fold k-fold cross-validation was also performed. The training and validation dataset were shuffled before the split and all models uses the same split for each fold. Results SAM segmentation Figure 4: Segment-Anything segmentation of the pebble setup. The results of SAM segmentation were demonstrated below. SAM returns a binary mask for each individual segments and figure 4 shows an example of aggregated mask plotted with the original image. SAM model was trained with RGB photos, hence depth images were not used. Figure 5 plots the accuracy of the segmentation by image. Image 0 to 12 were nadir images with variation in lighting condition, 13-24 were oblique image with variation in view angle. The accuracy was assessed by pebble. Each segment was compared to the manually labeled masked in which each pebble was labeled with a different ID. Segments where pebble pixels were found was considered a pebble segment. In these segment, the pebble that takes up the most area of the segment was considered the main pebble. If the area of the main pebble inside the segment did not reach 70% of the pebbles total area, the segment will be considered as over-segmented. If the main pebble was not over-segmented and takes up at least 70% of the segment area, it is considered correctly segmented (or SAM segmented pebbles in figure 5). If non of these is true, it will be considered as under-segmented. Figure 5: Percentage of correctly segmented, over/under segmented, and not segmented grains in SAM automatic mask generation by image. U-Net training history and accuracy (a) (b) (c) (d) Figure 6: U-Net Model training history of each split at kfold 0: (a) Split1C; (b) Split1D; (c) Split2C; (d) Split2D Figure 6 shows the training history of all splits at the first fold. Accuracy accessed through precesion, accuracy, F1 score, and binary IoU. The metrics were only calculated for test images that the models have seen in the training process. Figure 7 shows the accuracy metrics by image at different threshold. Solid line shows the mean value over 8 folds k-fold and the shaded area highlights the 25th to 75th percentiles. For the view angle correponding to each test images please see Table 1. Figure 8 shows the the accuracy variation over the folds. Solid line represent the mean value over all test images with threshold=0.7 while the shaded area indicates the threhold = 0.5-0.9. Table 1: Test images view angles Split Nadir Oblique high Oblique mid Oblique low split1 – Image 0-3 Image 4-7 Image 7-11 split2 Image 8-11 Image 0-1 Image 2-4 Image 5-7 (a) (b) (c) (d) Figure 7: U-Net Model accuracy of each split assessed by image at 3 threshold. Shadowed area indicates the 25th to 75th percentile along kfolds: (a) Split1C; (b) Split1D; (c) Split2C; (d) Split2D (a) (b) (c) (d) Figure 8: U-Net model consistency of accuracy metrics over kfolds. Shadowed area indicates the 25th to 75th percentile along test images: (a) Split1C; (b) Split1D; (c) Split2C; (d) Split2D Through the union of SAM mask and U-Net mask, point cloud of individual pebbles can be easily created. Figure 9 presents an example created through open3d geometry.create_point_cloud_from_depth_image and visualized in CloudCompare. Figure 9a visualize a RGB mesh generated from the point cloud and 9b visualize the point cloud colored by the distance between the grain and the camera with an estimate of size through calculating the distance between points. This was manually done through the CloudCompare ‘Point picking’ function and the estimate width is 13.3cm and the estimated hight is 10.1cm. Unfortunately, it is likely that this estimation is incorrect as the example presented here was not properly scaled. For the pint cloud to be correctly scaled, 4 calibration parameters are necessary. These are the x,y focal length, and principal points of the ZED 2i camera. These parameters are accessible through the camera API but unfortunately only the focal length were exported and saved before returning the camera to the University. Additionally, manually selecting the points for may raise the concern of objectivity and consistency. A better approach could be to lock for the major and minor axes as an indicator of the grain size. Nevertheless, segmentation results can be used create point clouds of samples individually and provides the oppotunity to analyse other physical charateristics such as surface smoothness and geometry. (a) (b) Figure 9: Point cloud of a pebble created from segmented depth map and RGB image with width and hight estimation: (a) RGB mesh and (b) Distance to camera in meter Conclusion and discussion This project is an attempt to utilize an U-Net-SAM 2-pass approach in conjunction with stereo-depth estimation to automize image based sampling. To assess the feasibility of this approach, the ZED 2i camera was used to capture a series of RGB and depth images from different angles and under different lighting condition. These images were manually labeled and segmented with the Meta SAM model. The results of SAM segmentation were visually impressive yet the accuracy reveals that the quality of the results decreases when view angles steepen. The main issue seems to be the partial overlap that does not completely hide the pebbles. These mostly covered pebbles leave out a small edges that are still noticeable by human vision yet challenging for SAM to detect. One speculation as to why SAM is struggling with these partially covered pebbles is that it has to do with scale. Based on experience, SAM seems to favor the assumption that objects are at similar scale. It could be related to the use of point grid as input prompt, this assumption often lead to over-segmentation grains that are notably larger then the surroundings. As the view angle steepens more pebbles were partially covered, resulting in a higher percentage of non segmented pebbles. Another factor that stands out is the lighting condition. With strong artificial lighting there is a higher tendency of missing objects while the absence of artificial lighting does not really impact the performance. Nevertheless, with nadir images SAM is capable of producing reliable segmentation without the need of fine-tuning. U-Net on the other hand, yields a mixed result. While the general stable and good accuracy suggest that the models were generally performing well, looking at split 1 models, the use of depth information apparently lead to larger fluctuation in performance across folds and a lower recall and F1 score. This likely suggested that depth information increases the likihood of producing flase negatives when the model was only trained with nadir images. Oppose to the expectation the depth information did not help the model to better generalize. The use of depth information in split 1 did not change how it struggles as the view angle differences increase or when the lighting was too strong. When compared to split 2, we can see that the inclusion of images take from different angles can visibly improve the performance. Here, even though hardly significant, the use of depth information slightly improved the performance and the stability across folds. Particularly at higher threshold, it also improved the models capability of handling challenging lower angle images and overexposed images. Take binary IoU as an indicator, split1D perform the worse with mean binary IoU at 0.72 across folds. Split1C shows a slight improvement at 0.78 while split2 models virtually performs equally weel at around 0.89. The results suggest that the approach is a feasible method given that some variation in view angle is also included in the training dataset. The use of depth information did not benefit the segmentatino as much as expected, but it still retains its value by providing the option to perform size measurement and create point clouds. This project has a few limition. First, no variation in camera paramters were tested. The same is true for the U-Net hyperparameters. The study sample is also the most concerning limitation. All images were taken from the same study sample, which raises the question if it could also achieve comparable results in real world application. It is also unclear what causes the worse performance of split1D. Tackling these problems would be possible improvements to the proposed method. Reference Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T., Whitehead, S., Berg, A. C., Lo, W.-Y., Dollár, P., &amp; Girshick, R. (2023). Segment Anything. 2023 IEEE/CVF International Conference on Computer Vision (ICCV), 3992–4003. https://doi.org/10.1109/ICCV51070.2023.00371 Mustafah, Y. M., Noor, R., Hasbi, H., &amp; Azma, A. W. (2012). Stereo vision images processing for real-time object distance and size measurements. 2012 International Conference on Computer and Communication Engineering (ICCCE), 659–663. https://doi.org/10.1109/ICCCE.2012.6271270 Soloy, A., Turki, I., Fournier, M., Costa, S., Peuziat, B., &amp; Lecoq, N. (2020). A Deep Learning-Based Method for Quantifying and Mapping the Grain Size on Pebble Beaches. Remote Sensing, 12(21), Article 21. https://doi.org/10.3390/rs12213659 Wang, C., Lin, X., &amp; Chen, C. (2019). Gravel Image Auto-Segmentation Based on an Improved Normalized Cuts Algorithm. Journal of Applied Mathematics and Physics, 7(3), Article 3. https://doi.org/10.4236/jamp.2019.73044]]></summary></entry><entry><title type="html">Climate Data Collection and Analysis, Kathmandu, Feb 26-27 2024</title><link href="http://localhost:4000/Kathmandu_Workshop_Feb2024/" rel="alternate" type="text/html" title="Climate Data Collection and Analysis, Kathmandu, Feb 26-27 2024" /><published>2024-03-15T00:00:00+01:00</published><updated>2024-03-15T00:00:00+01:00</updated><id>http://localhost:4000/Kathmandu_Workshop_Feb2024</id><content type="html" xml:base="http://localhost:4000/Kathmandu_Workshop_Feb2024/"><![CDATA[<p><em>Workshop organized and led by Dr Taylor Smith (University of Potsdam, Germany), Dr Bodo Bookhagen (University of Potsdam, Germany) and Dr Shakil Regmi (South-Eastern Finland University of Applied Sciences, Finland)</em></p>

<p><em>Workshop Goals:</em> The workshop aimed to present open-source data processing methods for working with time series and satellite data, with a particular focus on snow, water, and rainfall. The participants gained hands-on experience with open-source data processing tools, data visualization, and the design of open-source hardware.</p>

<p><strong>All workshop materials can be found on <a href="https://github.com/tasmi/Workshop_Kathmandu_Feb2024">github</a>.</strong></p>

<p><strong>YouTube recordings are available <a href="https://www.youtube.com/playlist?list=PLf4x2rqfvPEPDQ--Cozr9OMLE71Hb9cD5">here</a>.</strong></p>

<p><em>Detailed Workshop Schedule:</em></p>

<p>Day 1: Satellite Data Processing with Open-Source Tools</p>
<ul>
  <li>overview of satellite climate Data with focus on hydrologic and sediment flux data</li>
  <li>introduction to Google Earth Engine and extracting relevant data for study sites</li>
  <li>integrating watershed data and mapping climate trends</li>
  <li>creating visualizations and consistent reports</li>
</ul>

<p>Day 2: Designing, Building, and Analyzing Data from Open-Source Sensors</p>
<ul>
  <li>measuring high-fequency hydro-climatic data</li>
  <li>designing and building a low-cost river monitoring station</li>
  <li>collecting, transmitting, and prcessing high-frequency data</li>
  <li>combining sensor and satellite data for further analysis</li>
  <li>research outlook and practical applications in Nepal</li>
</ul>]]></content><author><name>Taylor Smith</name></author><category term="Open Source Sensors" /><category term="Climate Data" /><category term="Workshop" /><summary type="html"><![CDATA[Workshop organized and led by Dr Taylor Smith (University of Potsdam, Germany), Dr Bodo Bookhagen (University of Potsdam, Germany) and Dr Shakil Regmi (South-Eastern Finland University of Applied Sciences, Finland)]]></summary></entry><entry><title type="html">Structure from Motion Application for Landslide Characterization and Monitoring</title><link href="http://localhost:4000/LinaMariaPerez_UAV_landslide/" rel="alternate" type="text/html" title="Structure from Motion Application for Landslide Characterization and Monitoring" /><published>2024-03-12T00:00:00+01:00</published><updated>2024-03-12T00:00:00+01:00</updated><id>http://localhost:4000/LinaMariaPerez_UAV_landslide</id><content type="html" xml:base="http://localhost:4000/LinaMariaPerez_UAV_landslide/"><![CDATA[<p>On January 9th, 2023, a catastrophic landslide severely impacted the upper basin of Chontaduro Creek, located in the municipality of Rosas-Cauca in southwestern Colombia. This work investigates landslide velocities and spatial patterns of deformation through repeated UAV flights.</p>

<h1 id="introduction">Introduction</h1>

<p>Landslides are natural disasters that can have devastating impacts on human communities and infrastructure. Similar to other natural hazards, their severity can increase due to cascading effects and can be exacerbated by recent global climate changes. Moreover, larger landslides can result in diverse forms of damage and casualties. To address these challenges, advanced technologies such as UAV-based photogrammetry and lidar are used to rapidly and cost-efficiently assess large landslides. However, the precise georeferencing, co-registration, and alignment of UAV surveys collected from different devices with varying resolutions, flight conditions and flight heights as well as varying areal coverage present significant challenges for data analysis. To tackle these challenges, this work explores different methods to ensure accuracy in the pre-processing of this type of data.</p>

<p><em>This internship was supervised by Prof. Dr. Bodo Bookhagen.</em></p>

<h1 id="the-rosas-cauco-landslide">The Rosas-Cauco Landslide</h1>

<p>On January 9th, 2023, a catastrophic landslide severely impacted the upper basin of Chontaduro Creek, located in the municipality of Rosas-Cauca in southwestern Colombia (refer to Figure 1). This event caused extensive damage to the communities of La Soledad, Santa Clara, Párraga Viejo, and Chontaduro, resulting in the destruction of at least 150 homes and a significant portion of the Pan-American highway, which connects southern Colombia to the rest of the country. Inhabitants of the region reported their initial observations of instability on January 4th when they noticed small cracks with lengths ranging from 20 cm to 30 cm forming in the crown area near the Alfonso Córdoba school. However, it was not until the period between the afternoon of January 9th and the morning of January 10th that the landslide’s progression became starkly evident, leaving distinct markings on both the crown and flanks of the movement. Following this significant event, the landslide exhibited continuous and substantial activity until January 18th. By January 10th, the area affected by the mass movement had expanded to 64.72 hectares, and it further increased to 87.16 hectares by January 13th, ultimately reaching 89.12 hectares by January 18th.</p>

<p>The Rosas landslide, situated in Colombia, has been closely monitored by several government institutions, including the <em>Unidad Nacional de Gestión del Riesgo</em> (UNGRD), the Geological Survey of Colombia (SGC), and the National Armed Forces. These agencies have conducted numerous UAV (Unmanned Aerial Vehicle) flights to monitor the ongoing evolution of the landslide. Over the past decade, there has been a notable increase in the utilization of remote sensing technology for landslide mapping and monitoring. This surge in usage has been complemented by significant improvements in the spatial resolution of remote sensing technologies, particularly with the introduction of laser scanning (both airborne and terrestrial) and unmanned aerial vehicles (UAVs) (Niethammer et al., 2012). Earth observation methods are useful to produce detailed multi-temporal sets of images, orthophotos, and digital elevation models (DEMs). These products may provide insight into the flow kinematics such as flow rate, landslide expansion, and accumulation at the toe zone or retreating scarps. In addition, these new techniques allow volume calculations of the accumulated and removed material by the landslide and mapping of the topographic changes (Lucieer et al., 2014).</p>

<p>Furthermore, recent advances in photogrammetric image processing and computer vision have resulted in a technique known as Structure from Motion (SfM). Highly detailed 3D models can be obtained from overlapping multi-view photography with SfM algorithms. While spaceborne and airborne remote sensing techniques are commonly employed for landslide research, these methods may lack the flexibility offered by the novel remote sensing platform of UAVs. When combined with UAV technology, Structure from Motion (SfM) can provide a cost-effective and efficient means of acquiring dense and accurate 3D data of the Earth’s surface. In this study, we investigate various methods for processing the extensive UAV data acquired for the Rosas landslide, aiming to produce accurately georeferenced and aligned 3D models.</p>

<h1 id="study-area">Study area</h1>

<p>The Rosas landslide occurred in the rural district of La Soledad, located within the municipality of Rosas-Cauca, situated in the southwestern region of Colombia. This region lies within the Patía Valley intercordilleran depression, nestled between the Western Cordillera and the Central Cordillera of Colombia. The primary drainage network affected is that of the Chontaduro creek, a tributary of the Esmita River, which is part of the upper Patía River basin (refer to Figure 1).</p>

<center>
<figure>
<a href="UAV_landslide_figs_PerezRosasStudArea.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/RosasStudArea.jpg" /></a>
<figcaption>Figure 1: Shaded relief map of study area, showing major rivers, towns and a multi-temporal landslide inventory, sourced from SGC (2020)</figcaption>
</figure>
</center>

<p>The study area is situated within the western range of the Colombian Andes, a region characterized by its susceptibility to landslides. This vulnerability arises from a combination of factors including steep terrain, adverse weather conditions, and the presence of soils derived from volcanic deposits originating from the Sotará volcanic complex. A comprehensive study conducted by the Geological Survey of Colombia-SGC (SGC, 2020), examined the development and distribution of landslide in the region. Their investigation identified a total of 273 landslide events occurring within the municipality of Rosas during the period spanning from 1990 to 2019. Of these occurrences, five were deemed severe, leading to tragic casualties, substantial economic losses, and considerable damage to critical infrastructure.</p>

<p>As depicted in Figure 1, the area has a historical record of being affected by landslides of varying magnitudes. However, the Rosas landslide, which is the focal point of our study, stands out as the largest landslide identified in the surrounding region since 1990.</p>

<h2 id="geological-setting">Geological setting</h2>

<p>The geological setting of the study area is profoundly influenced by its location within the Northern Andes, where the convergence of three tectonic plates—the Nazca, Caribbean, and South American plates—gives rise to active N-NE faulting within the Andean block. This convergence, with the Nazca plate moving eastward relative to northwestern South America at a rate of 6 cm/yr, leads to the formation of the Colombia-Trench to the west and the mountain range of the Colombian Andes (Andes Pulido, 2003). Within the Colombian Andes, three distinct mountain ranges—the Western, Central, and Eastern Cordillera—converge southward into a unified range, shaped by deformation and faulting resulting from the interaction of these tectonic plates (Taboada et al., 2000). The study area, encompassing the upper Patía River basin, is situated at the southern Colombian Andes’ convergence point between the Western and Central Cordilleras. Here, the Western Cordillera rises dramatically from the Pacific Coastal Plain on its western flank, while to the east, it is separated from the Central Cordillera by the Cauca-Patia valleys spanning a distance of approximately 500 km. The Cauca-Patia valley, resembling a graben-like structure, is characterized by an asymmetric tilt and is filled with Tertiary-Quaternary continental clastics and volcanics. Bounded by the Cauca fault zone to the west and the Romeral fault zone to the east, this depression effectively separates the oceanic Western Cordillera from the ancient crystalline Central Cordillera (Figure 2).</p>

<center>
<figure>
<a href="UAV_landslide_figs_PerezGELOGY.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/GELOGY.jpg" /></a>
<figcaption>Figure 2: Geological map of the study area. Sourced from SGC (2020). </figcaption>
</figure>
</center>

<h1 id="landslide-event-characterization">Landslide event characterization</h1>

<p>The Rosas mass movement, situated within the micro-basin of Chontaduro creek, is characterized as an active rotational landslide of a complex nature, exhibiting retrogressive and widening behavior. This landslide initiation point is positioned 900 meters from the summit of Broncazo Hill and 700 meters from its base (Figure 3).</p>

<center>
<figure>
<a href="UAV_landslide_figs_Perezlandslide1.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/landslide1.png" /></a>
<figcaption>Figure 3: Panoramic view of Rosas landslide. Sourced from SGC (2023).</figcaption>
</figure>
</center>

<p>The landslide displays distinct patterns of movement and failure mechanisms across its various sections. In the uppermost region, known as the crown, where the movement commences, displacements are attributed to a rotational failure mechanism. Moving to the flanks, which constitute the lateral sides of the landslide, translational faults predominate. Within the body of the landslide, one observes intense deformation, sinking, and the formation of cracks, both transverse and parallel to the primary crown (Figure 4). In the lowermost part of the movement, termed the toe, situated furthest from the crown, translational landslides, soil block falls, and flows of earth, debris, and mud have been observed. These flows are channeled through areas with lower slopes. The dynamic interplay of these movements signifies the complex nature of the landslide, with its activity and behavior evolving in response to variations in water availability in the upper part of the basin.</p>

<center>
<figure>
<a href="UAV_landslide_figs_Perezlandslide-detail.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/landslide-detail.jpg" /></a>
<figcaption>Figure 4: Identifiable characteristics across the crown, body, and toe of the landslide: 1) A curved concave rupture surface creating a scarp measuring up to 20 meters in height. 2) Presence of tilted blocks, subsidence, and formation of cracks. 3) Occurrence of translational slides and debris flows.</figcaption>
</figure>
</center>

<h2 id="geology-and-failure-surface">Geology and failure surface</h2>

<p>The initial phase of the mass movement was characterized by a rotational slide, evidenced by a semicircular curved and concave upward rupture surface. This phase exposed a steep slope scarp reaching heights of up to 40 meters (SGC, 2023), composed of alternating layers of residual ash soils and pyroclastic flows from the Galeón Formation. Subsequently, a portion of the body adjacent to the crown tilted backward due to the concavity of the failure surface, resulting in ground subsidence. When the central region of the landslide became fully saturated with water, copious flows of earth and rock emanated from both sides of the landslide, as well as from its center.</p>

<p>In the microbasin of Chontaduro creek, a geological sequence unfolds, comprising both epiclastic and volcanic rocks attributed to the Galeón Formation (TQpg). Notably, these formations are prominently exposed in areas such as Cerro Broncazo and its environs, encompassing the Quilcacé and Esmita rivers. In these regions, the volcaniclastic deposits can attain substantial thicknesses, reaching up to 800 meters (SGC, 2020).</p>

<center>
<figure>
<a href="UAV_landslide_figs_PerezSlide4.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/Slide4.jpg" /></a>
<figcaption>Figure 5: Lithological sequence and unit details observed at the crown of the landslide.</figcaption>
</figure>
</center>

<p>The scarp of the crown provided a clear view of the sequential arrangement of materials comprising the mobilized mass. Illustrated in Figure 5 and Figure 6, from bottom to top, the lithological units are delineated as follows:</p>

<ul>
  <li>The base layer showcases rocks attributed to well-selected, sandy, hyperconcentrated lahar deposits, linked with the Galeón formation.</li>
  <li>Directly above lies a layer of clast-supported pyroclastic flows, featuring rounded volcanic rock clasts with subtle orientation indicative of flow, ensconced within a reddish sandy volcanic ash matrix.</li>
  <li>Progressing upward, there exist lenticular alluvial and colluvial deposits, resulting from the erosion and transportation of various units within the Galeón formation.</li>
  <li>Finally, at the apex of the sequence, residual soils stemming from both transport deposits and pyroclastic flows are evident.</li>
</ul>

<center>
<figure>
<a href="UAV_landslide_figs_PerezSlide3.jpg"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/Slide3.jpg" /></a>
<figcaption>Figure 6: Panoramic view of landslide crown scarp. Blue dashed line depicting the water horizon</figcaption>
</figure>
</center>

<p>As depicted in Figure 6, a highly saturated surface conducive to water flow manifests between the hyperconcentrated flow layer and the pyroclastic flow layer. The permeability of the latter, coupled with the slight permeability of the underlying layer, facilitates water flow across this contact surface. Exhibiting horizontal continuity, this surface could be associated with the sliding surface of the mass movement.Recurrence of the poorly permeable hyperconcentrated lahar layer is notable at the base of the slide, indicating a seamless continuation of the water-saturated surface from the crown to the landslide’s base.</p>

<p>Furthermore, noteworthy is the close proximity of the landslide’s crown to the ‘Alto de las Yerbas’ fault line, as referenced in the technical report on Landslide Hazard Zoning by the SGC (2020), detailed in Figure 7. Findings from the present study imply that structural and lithological factors, compounded by intense rainfall, were the primary catalysts for the occurrence of this significant landslide.</p>

<center>
<figure>
<a href="UAV_landslide_figs_PerezUAV_landslide_figs_Perezgeolandslide.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/geolandslide.png" /></a>
<figcaption>Figure 7: Geological surface units and main fault in the landslide site. Sourced from SGC (2020).</figcaption>
</figure>
</center>

<h2 id="rainfall-as-trigger-mechanism">Rainfall as trigger mechanism</h2>

<p>According to the Atlas of Colombia (IGAC, 2012), the region experiences an average annual rainfall of 2,000 mm at the summit of the Central Cordillera and 3,000 mm at the summit of the Western Cordillera. Rainfall patterns indicate a distinct summer season during June, July, August, and September, followed by a rainy season during the remaining months. The precipitation data for the study area was obtained from the Párraga weather station (Station Code: 52010050), which is managed by IDEAM (Instituto de Hidrología, Meteorología y Estudios Ambientales de Colombia). The time series analyzed encompasses 33 years of recorded data, spanning from 1990 to 2023 (Figure 8 shows the last 20 years of data). As shown in Figure 8, the study area experiences two distinct rainy seasons: the first occurs from January to May, characterized by relatively lower rainfall compared to the second rainy season, which typically occurs between October and December.</p>

<p>An important observation to highlight is that within the last 20 years of recorded data, the rainfall during the rainy season of December 2022, preceding the event, was notably exceptional. In November 2022 alone, precipitation levels reached a record high of up to 720 mm, marking it as the highest recorded rainfall within the observed period.</p>

<center>
<figure>
<a href="UAV_landslide_figs_PerezUAV_landslide_figs_PerezmonthlyPre.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/monthlyPre.png" /></a>
<figcaption>Figure 8: Visualization of total monthly rainfall at the Parraga meteorological station spanning the past 20 years. Blue bars represent months with rainfall above the mean, while green bars denote months with precipitation below the mean.</figcaption>
</figure>
</center>

<p>In the other hand, the daily time series recorded at the Párraga station exhibits a range of maximum daily precipitation values, varying between 0 mm and 95 mm. The highest recorded value, 94.8 mm, was documented on January 9th, which coincided with the onset of the landslide event (Figure 9). The cumulative rainfall data was computed over the last 60 days leading up to the triggering of the landslide, revealing an impressive total rainfall accumulation of 2309.3 millimeters recorded up to January 9, 2023 (Figure 9).</p>

<center>
<figure>
<a href="UAV_landslide_figs_PerezUAV_landslide_figs_PerezDailyRainfall.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/DailyRainfall.png" /></a>
<figcaption>Figure 9: Total daily and accumulated rainfall at the Parraga meteorological station.</figcaption>
</figure>
</center>

<h1 id="methods">Methods</h1>

<p>Aerial digital photographs of the landslide were captured on multiple occasions between January 9th and March 18th. Predominantly, these flights were conducted at an altitude of 250 meters. Over the span of 17 days, a total of 22 flights were executed by three organizations specializing in risk management and emergency disaster response: Unidad Nacional para la Gestión del Riesgo (UNGRD), Servicio Geológico Colombiano (SGC), and Policía Nacional (PNAL). Despite the involvement of various institutions, uniformity was maintained in the equipment used, ensuring consistent resolution in the photographs. However, aspects such as flight altitude, coverage area, flight plan design, and the number of captured images exhibited significant variation.</p>

<p>A further challenge encountered in the data acquisition process was the georeferencing of the flights. During the initial flights, markers were not employed, leading to difficulties in precise location mapping. To address this, on May 18th, a set of markers was installed to ensure at least one flight could be accurately georeferenced. However, these newly placed markers proved too small to be discernible in images taken from higher altitudes. Consequently, to enhance the overall georeferencing accuracy of all flights, an independent survey was executed as part of this study, utilizing natural points as reference markers in the final survey.</p>

<h2 id="point-cloud-generation">Point Cloud Generation</h2>

<p>The Structure from Motion (SfM) method uses algorithms to identify matching features in a series of overlapping digital images and calculates camera location and orientation from the differential positions of multiple matched features. From these calculations, overlapping imagery is then used to reconstruct a “sparse” or “coarse” 3D point cloud model of the photographed object or surface or scene (Brook et al., 2019). The software <em>Agisoft Metashape</em> was used to undertake the SfM processing. In Metashape, the SFM processing comprises three key steps (Agisoft, 2023):</p>

<ol>
  <li><em>Alignment</em>: Involves aerial triangulation and bundle block adjustment, where the software identifies feature points on images, matches them into tie points and determine camera positions, resulting in a tie point cloud and camera positions for depth map determination and 3D reconstruction.</li>
  <li><em>Surface Generation</em>: Creates a 3D mesh or 2.5D digital elevation model (DEM), combining photogrammetric depth maps and LiDAR data, which can be textured and exported for various applications.</li>
  <li><em>Orthomosaic Creation</em>: Generates a georeferenced orthomosaic by projecting images onto a chosen surface (DEM or mesh), useful for mapping and analysis, including vegetation indices in multispectral imagery projects.</li>
</ol>

<p>In this study we adopted the workflow implemented in Agisoft Metashape, described in the User Manual v2.0 (Agisoft, 2023). The parameters used are described n the following sections.</p>

<h2 id="image-loading-and-chunks-definition">Image loading and chunks definition</h2>

<p>Sixteen distinct chunks were produced by importing photographs from selected flights (refer to Table 1), of which five met key selection criteria: a sufficient quantity of photographs, high image quality characterized by proper exposure and sharpness, and extensive coverage of the landslide area. This careful selection was necessary as some flights were tasked with monitoring more restricted sectors of the mass movement, targeting specific areas such as the crown or the Pan-American Highway. For comprehensive area coverage and improved model resolution during that timeframe, photographs from January 18th and 19th were merged in one single chunk.</p>

<table>
  <thead>
    <tr>
      <th>Chunks</th>
      <th>Cameras</th>
      <th>Points</th>
      <th>Markers</th>
      <th>Processed</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1/14/2023</td>
      <td>1491</td>
      <td>880,613</td>
      <td>19 markers</td>
      <td>X</td>
    </tr>
    <tr>
      <td>1/17/2023</td>
      <td>567</td>
      <td>436,618</td>
      <td>4 markers</td>
      <td> </td>
    </tr>
    <tr>
      <td>1/18-19/2023</td>
      <td>878</td>
      <td>1,666,175</td>
      <td>17 markers</td>
      <td> </td>
    </tr>
    <tr>
      <td>1/19/2023</td>
      <td>369</td>
      <td>387,054</td>
      <td> </td>
      <td>X</td>
    </tr>
    <tr>
      <td>1/20/2023</td>
      <td>196</td>
      <td>194,101</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>1/26/2023</td>
      <td>332</td>
      <td>352,573</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>1/30/2023</td>
      <td>321</td>
      <td>351,487</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>2/3/2023</td>
      <td>418</td>
      <td>403.19</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>2/6/2023</td>
      <td>331</td>
      <td>336,810</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>2/11/2023</td>
      <td>576</td>
      <td>627,729</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>2/16/2023</td>
      <td>329</td>
      <td>346,862</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>2/19/2023</td>
      <td>442</td>
      <td>459,783</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>3/6/2023</td>
      <td>330</td>
      <td>324,661</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>3/21/2023</td>
      <td>331</td>
      <td>352.909</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>3/30/2023</td>
      <td>328</td>
      <td>327,605</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>5/18/2023</td>
      <td>333</td>
      <td>243,261</td>
      <td> </td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p><em>Table 1: Point clouds generated in Agisoft.</em></p>

<h2 id="photos-alignment-and-filtering">Photos alignment and filtering</h2>

<p>In each chunk, the photographs underwent an alignment process with the accuracy setting set to high, utilizing equipment source coordinates as a reference point. Camera optimization was conducted, selecting the estimation of the covariance of tie points. Subsequently, apparent outliers were removed from the sparse cloud manually to diminish potential reconstruction inaccuracies. Additional refinement was carried out by filtering based on tie point covariance and uncertainties, employing Metashape’s integrated Python scripting capabilities for a more precise adjustment (Figure 10).</p>

<center>
<figure>
<a href="UAV_landslide_figs_PerezUAV_landslide_figs_Perezcovariance.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/covariance.png" /></a>
<figcaption>Figure 10: Tie point covariances obtained after filtering.</figcaption>
</figure>
</center>

<h2 id="dense-point-cloud-generation">Dense Point Cloud generation</h2>

<p>For all chosen chunks, a dense 3D point cloud was constructed employing high-quality settings and mild depth filtering. This detailed point cloud serves as the foundation for generating Digital Surface Models (DSM) and orthomosaics. Additional settings were selected to reuse depth maps, calculate point colors and calculate point confidence (Figure 11).</p>

<center>
<figure>
<a href="UAV_landslide_figs_PerezUAV_landslide_figs_Perezdenseall.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/denseall.png" /></a>
<figcaption>Figure 11: Dense point cloud generated for 14/01/2023 survey</figcaption>
</figure>
</center>

<h2 id="natural-points-definition-and-gnss-survey">Natural points definition and GNSS survey</h2>

<p>To accurately reference the point clouds, a series of natural markers were established to guarantee high-precision (centimeter-level) localization that was readily identifiable in the field. Utilized reference points included distinct features such as sewer edges, fence corners, and light posts (see Figure 12). Out of the 27 natural markers delineated (see Figure 13), only 21 were successfully surveyed on-site. The survey faced several challenges, including the demolition of referenced structures, obscuring of markers by debris or soil, and inaccessible locations of certain points. The coordinates for each marker were recorded using a Trimble SPS855 GNSS Modular Receiver.</p>

<center>
<figure>
<a href="UAV_landslide_figs_Perezmarker12.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/marker12.png" /></a>
<figcaption>Figure 12: Example of a marker defined</figcaption>
</figure>
</center>

<center>
<figure>
<a href="UAV_landslide_figs_Perezmarkers.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/markers.png" /></a>
<figcaption>Figure 13: Markers defined. Green points represent the markers defined and recorded during field work</figcaption>
</figure>
</center>

<p>Following the download of raw data from the Trimble receiver, the post-processing software GrafNav was employed. GrafNav is an advanced GNSS post-processing suite that supports kinematic and static analysis with an advanced processing engine for GPS, GLONASS, and BeiDou signals. To refine the data, corrections were applied using the Popayan station (POPA), which is the station nearest to the collection site. The required data were acquired from the Colombian open data portal (<a href="https://www.colombiaenmapas.gov.co/">https://www.colombiaenmapas.gov.co/</a>). Both the RINEX files from the field-surveyed markers and the permanent station were inputted into GrafNav to execute the correction process. The precision-adjusted positions of the markers are shown in Table 2 and Figure 14.</p>

<center>
<figure>
<a href="UAV_landslide_figs_PerezGNSS_grafnav.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/GNSS_grafnav.png" /></a>
<figcaption>Figure 14: Points depict the resulting standard deviation for horizontal and height measurements, while bars illustrate a quality metric established by GrafNav.</figcaption>
</figure>
</center>

<p>As depicted in Figure 14, both the resulting horizontal and vertical standard deviations are below 0.656 meters, with mean values of 0.26 and 0.14 meters, respectively. Achieving accuracy at the centimeter level indicates favorable results. However, a metric of quality provided by Grafnav offers additional insight. This metric assigns values as follows:</p>

<ul>
  <li>Quality 1: Represents a fixed integer solution with excellent satellite geometry.</li>
  <li>Quality 2-3: Indicates either fixed integers with marginal geometry or converging float solutions.</li>
  <li>Quality 4-5: Suggests qualities akin to those of DGPS.</li>
  <li>Quality 6: Represents a coarse acquisition (C/A) only solution.</li>
</ul>

<p>The majority of solutions fall within the quality range of 2-3, with one solution (marker 1) exhibiting a very good quality of 1 and another (marker 10) displaying a poor quality of 6. All solutions showed satisfactory results and were subsequently utilized in the following step, which involves the precise alignment of point clouds based on markers.</p>

<table>
  <thead>
    <tr>
      <th><strong>Date</strong></th>
      <th>GPSTime</th>
      <th><strong>Easting</strong></th>
      <th><strong>Northing</strong></th>
      <th><strong>H-Ell</strong></th>
      <th><strong>H-MSL</strong></th>
      <th><strong>Q</strong></th>
      <th>SDHoriz</th>
      <th><strong>SDHeigh (m)</strong></th>
      <th><strong>Marker</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>8/09/2023</td>
      <td>32:23.0</td>
      <td>302995.460</td>
      <td>248590.161</td>
      <td>1514963</td>
      <td>1486998</td>
      <td>1</td>
      <td>0.016</td>
      <td>0.028</td>
      <td>1</td>
    </tr>
    <tr>
      <td>8/09/2023</td>
      <td>59:34.0</td>
      <td>302445.401</td>
      <td>247995.626</td>
      <td>1346507</td>
      <td>1318573</td>
      <td>3</td>
      <td>0.12</td>
      <td>0.072</td>
      <td>12</td>
    </tr>
    <tr>
      <td>8/09/2023</td>
      <td>32:47.0</td>
      <td>302540.380</td>
      <td>247938.191</td>
      <td>1342117</td>
      <td>1314170</td>
      <td>5</td>
      <td>0.559</td>
      <td>0.402</td>
      <td>13</td>
    </tr>
    <tr>
      <td>8/09/2023</td>
      <td>50:24.0</td>
      <td>302928.080</td>
      <td>248596.004</td>
      <td>1508566</td>
      <td>1480608</td>
      <td>3</td>
      <td>0.18</td>
      <td>0.112</td>
      <td>2</td>
    </tr>
    <tr>
      <td>8/09/2023</td>
      <td>01:48.0</td>
      <td>302956.219</td>
      <td>248607.942</td>
      <td>1512854</td>
      <td>1484894</td>
      <td>4</td>
      <td>0.367</td>
      <td>0.129</td>
      <td>25</td>
    </tr>
    <tr>
      <td>8/09/2023</td>
      <td>25:50.0</td>
      <td>302984.155</td>
      <td>248601.450</td>
      <td>1513498</td>
      <td>1485535</td>
      <td>5</td>
      <td>0.257</td>
      <td>0.13</td>
      <td>3</td>
    </tr>
    <tr>
      <td>8/10/2023</td>
      <td>53:34.0</td>
      <td>302155.060</td>
      <td>247741.470</td>
      <td>1338077</td>
      <td>1310161</td>
      <td>6</td>
      <td>0.291</td>
      <td>0.181</td>
      <td>10</td>
    </tr>
    <tr>
      <td>8/10/2023</td>
      <td>26:29.0</td>
      <td>302045.545</td>
      <td>247650.392</td>
      <td>1328182</td>
      <td>1300273</td>
      <td>3</td>
      <td>0.347</td>
      <td>0.215</td>
      <td>21</td>
    </tr>
    <tr>
      <td>8/10/2023</td>
      <td>29:50.0</td>
      <td>302207.493</td>
      <td>247767.666</td>
      <td>1341687</td>
      <td>1313767</td>
      <td>2</td>
      <td>0.243</td>
      <td>0.138</td>
      <td>23</td>
    </tr>
    <tr>
      <td>8/10/2023</td>
      <td>03:19.0</td>
      <td>302229.752</td>
      <td>247892.380</td>
      <td>1344103</td>
      <td>1316187</td>
      <td>3</td>
      <td>0.343</td>
      <td>0.217</td>
      <td>24</td>
    </tr>
    <tr>
      <td>8/10/2023</td>
      <td>10:53.0</td>
      <td>301981.062</td>
      <td>248881.942</td>
      <td>1547867</td>
      <td>1520018</td>
      <td>3</td>
      <td>0.263</td>
      <td>0.137</td>
      <td>26</td>
    </tr>
    <tr>
      <td>8/10/2023</td>
      <td>58:14.0</td>
      <td>301798.636</td>
      <td>248108.698</td>
      <td>1446887</td>
      <td>1419024</td>
      <td>3</td>
      <td>0.19</td>
      <td>0.117</td>
      <td>4</td>
    </tr>
    <tr>
      <td>8/10/2023</td>
      <td>09:31.0</td>
      <td>301847.911</td>
      <td>248155.130</td>
      <td>1446869</td>
      <td>1419003</td>
      <td>3</td>
      <td>0.364</td>
      <td>0.186</td>
      <td>6</td>
    </tr>
    <tr>
      <td>8/10/2023</td>
      <td>31:22.0</td>
      <td>301868.008</td>
      <td>248170.170</td>
      <td>1446827</td>
      <td>1418960</td>
      <td>3</td>
      <td>0.147</td>
      <td>0.072</td>
      <td>7</td>
    </tr>
    <tr>
      <td>8/10/2023</td>
      <td>30:20.0</td>
      <td>301875.048</td>
      <td>248297.576</td>
      <td>1452434</td>
      <td>1424571</td>
      <td>3</td>
      <td>0.235</td>
      <td>0.161</td>
      <td>8</td>
    </tr>
    <tr>
      <td>8/10/2023</td>
      <td>00:47.0</td>
      <td>301968.070</td>
      <td>248386.332</td>
      <td>1451916</td>
      <td>1424048</td>
      <td>4</td>
      <td>0.235</td>
      <td>0.09</td>
      <td>9</td>
    </tr>
    <tr>
      <td>8/11/2023</td>
      <td>58:29.0</td>
      <td>301263.276</td>
      <td>246688.966</td>
      <td>1308378</td>
      <td>1280506</td>
      <td>4</td>
      <td>0.656</td>
      <td>0.64</td>
      <td>17</td>
    </tr>
    <tr>
      <td>8/11/2023</td>
      <td>40:23.0</td>
      <td>301456.505</td>
      <td>246350.882</td>
      <td>1292738</td>
      <td>1264830</td>
      <td>2</td>
      <td>0.054</td>
      <td>0.055</td>
      <td>18</td>
    </tr>
    <tr>
      <td>8/11/2023</td>
      <td>04:19.0</td>
      <td>301430.773</td>
      <td>246394.964</td>
      <td>1296034</td>
      <td>1268131</td>
      <td>2</td>
      <td>0.077</td>
      <td>0.064</td>
      <td>19</td>
    </tr>
    <tr>
      <td>8/11/2023</td>
      <td>30:58.0</td>
      <td>301364.741</td>
      <td>246498.245</td>
      <td>1300102</td>
      <td>1272211</td>
      <td>4</td>
      <td>0.364</td>
      <td>0.259</td>
      <td>20</td>
    </tr>
    <tr>
      <td>8/11/2023</td>
      <td>34:20.0</td>
      <td>301586.893</td>
      <td>247430.776</td>
      <td>1319567</td>
      <td>1291696</td>
      <td>5</td>
      <td>0.604</td>
      <td>0.323</td>
      <td>27</td>
    </tr>
  </tbody>
</table>

<p><em>Table 2: GNSS data corrected in Grafnav.</em></p>

<h2 id="chunk-alignment">Chunk alignment</h2>

<p>For a precise alignment of point clouds, the process begins with the correction of a designated reference point cloud; in this case, the cloud dated 01/14/2023 is used. The coordinates of the markers are revised using the data acquired from the recent field campaign. Following this update, camera parameters are re-optimized, and a new dense point cloud is generated. This newly referenced point cloud then serves as the reference for aligning subsequent point clouds through the point-based method, a technique available to align chunks within <em>Agisoft</em>.</p>

<p>Despite this registration, some misalignment persist. To address this, some pre-filtering steps and global registration methos are appied to subsequently apply a meticulous registration is conducted using the Iterative Closest Point (ICP) method, elaborated in the following sections.</p>

<h2 id="pre-processing-and-global-registration">Pre-processing and Global registration</h2>

<p>The registration process was exclusively conducted within the stable zones identified within the study area. By avoiding regions experiencing landslide activity, where the terrain undergoes constant changes, we ensure a precise correlation between zones exhibiting minimal alterations over time. The stable areas from both the reference and target point clouds were manually selected and extracted using <em>Cloud Compare</em>.</p>

<p>Subsequently, pre-registration steps were performed on these stable area point clouds, including:</p>

<ol>
  <li>
    <p><strong>Initial Filtering Step</strong>: This involved the removal of statistical outliers using an Open3D function. Points that deviate significantly from the average distance to their neighbors were identified as outliers and removed. This step was iterated until all points not forming a continuous surface were eliminated.</p>
  </li>
  <li>
    <p><strong>Nearest Neighbor Distance Calculation</strong>: Next, KDtree was employed to compute the nearest neighbor distances between both point clouds (Figure 15). In some regions where there was a lack of overlapping between the point clouds, resulting distances exceeded 3 meters. To address this issue and ensure complete overlap, points farther than 3 meters from one point cloud to the other were filtered out.</p>
  </li>
</ol>

<center>
<figure>
<a href="UAV_landslide_figs_Perezinitialdistance.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/initialdistance.png" /></a>
<figcaption>Figure 15: Nearest neighbor distances between both point clouds.</figcaption>
</figure>
</center>

<p>After the initial filtering step, the alignment process proceeds with an initial coarse alignment to bring the point clouds into close proximity. This alignment method incorporates feature identification to refine the search for corresponding points. This process generally involves two key steps:</p>

<ol>
  <li>
    <p><strong>Utilizing Fast Point Feature Histograms (FPFH)</strong>: FPFH is a technique employed in 3D registration, which aims to align multiple 3D point clouds. FPFH analyzes each point in a 3D point cloud by examining its surrounding points. It computes a histogram describing the geometric features of each point based on its neighbors, encompassing attributes like angle and distance between neighboring points. FPFH facilitates the matching of points across different 3D scenes by leveraging the geometric properties of each point and its surroundings (Rusu et al., 2011).</p>
  </li>
  <li>
    <p><strong>Application of <em>registration_ransac_based_on_feature_matching</em> Function</strong>: Open3D’s <code class="language-plaintext highlighter-rouge">registration_ransac_based_on_feature_matching</code> function implements the RANSAC (Random Sample Consensus) algorithm tailored for point cloud registration via feature matching. RANSAC randomly selects a minimal subset of points from both point clouds to estimate a transformation, such as rotation and translation, between them (Fischler et al., 1981). Subsequently, the estimated transformation is evaluated by applying it to all feature correspondences obtained in the previous step, and the number of inliers—correspondences consistent with the estimated transformation—is tallied.</p>
  </li>
</ol>

<p>In essence, these refined alignment techniques enable the accurate registration of point clouds by iteratively estimating transformations based on feature correspondences, thereby enhancing the robustness and accuracy of the alignment process. Although the global alignment has improved the distribution of distances, as depicted in Figure 16, with a mean value of less than 2 meters, the Global Registration Modified Hausdorff Distance has increased to 3.75 meters. This discrepancy underscores the necessity for a more precise registration process to minimize the offset between the two point clouds.</p>

<center>
<figure>
<a href="UAV_landslide_figs_Perezglobalregistration.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/globalregistration.png" /></a>
<figcaption>Figure 16: Nearest neighbor distances between both point clouds before and after global registration.</figcaption>
</figure>
</center>

<h2 id="icp-registration">ICP registration</h2>

<p>Unlike global alignment, fine registration methods require two point clouds that already have a rough correspondence. The Iterative Closest Point (ICP) algorithm is commonly used for local refinement and has two main variants:</p>

<p><strong>Point-to-point ICP</strong></p>

<p>The Iterative Closest Point (ICP) algorithm, introduced in the early 1990s by various authors (Besl et al. (1992), Chen et al. (1992), Zhang (1994)), stands out among the myriad registration methods proposed in the literature. It has become one of the most well-known for efficiently registering two 2D or 3D point sets under Euclidean (rigid) transformation.</p>

<p>The process follows a simple concept (Yang et al., 2015):</p>

<ol>
  <li>
    <p>Begin with an initial transformation involving rotation and translation.</p>
  </li>
  <li>
    <p>Alternate between two steps:</p>

    <p>a.  Establish closest-point correspondences under the current transformation.</p>

    <p>b.  Estimate a new transformation using these correspondences.</p>
  </li>
  <li>
    <p>Repeat steps 2a and 2b until convergence is achieved.</p>
  </li>
</ol>

<p>Notably, the point-to-point ICP method is capable of directly processing the raw point sets without regard to their inherent characteristics, such as distribution, density, and noise level.</p>

<p>The <code class="language-plaintext highlighter-rouge">registration_icp</code> function of <em>Open3D</em> aligns both stable-area point clouds using the global transformation matrix from the previous step via ICP registration. subsequently, we use the <code class="language-plaintext highlighter-rouge">evaluate_registration</code> function that calculates two key metrics: <strong>Fitness</strong>, that measures overlapping area (# of inlier correspondences / # of points in target), higher values indicate better alignment. And <strong>Inlier RMSE</strong>, that measures RMSE of all inlier correspondences with lower values indicating better alignment.</p>

<p>As depicted in Figure 17, the final registration demonstrates excellent performance, with mean distances smaller than 50 centimeters. Subsequently, the final transformation matrix derived from this registration is applied to the full dataset, encompassing both stable and landslide areas.</p>

<center>
<figure>
<a href="UAV_landslide_figs_Perezfinalregistration.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/UAV_landslide_figs_Perez/finalregistration.png" /></a>
<figcaption>Figure 17: Nearest neighbor distances between both point clouds before and after global registration and ICP registration.</figcaption>
</figure>
</center>

<p>In Table 3, a comprehensive overview of the effectiveness of various registration methods in enhancing the alignment of point clouds is provided. By meticulously examining the metrics presented in the table, including distances between corresponding points before and after registration, the impact of each method on the alignment quality becomes evident.</p>

<table>
  <thead>
    <tr>
      <th> </th>
      <th style="text-align: center">Hausdorff Distance</th>
      <th style="text-align: center">Modified Hausdorff Distance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Before global alignment</td>
      <td style="text-align: center">154.51</td>
      <td style="text-align: center">3.312067224</td>
    </tr>
    <tr>
      <td>After global alignment</td>
      <td style="text-align: center">145.26</td>
      <td style="text-align: center">3.75</td>
    </tr>
    <tr>
      <td>After ICP</td>
      <td style="text-align: center">-</td>
      <td style="text-align: center">2.41</td>
    </tr>
  </tbody>
</table>

<p><em>Table 3: Hausdorff and Modified Hausdorff distance calculated throughout the registration process</em></p>

<h1 id="key-project-insights">Key project insights</h1>

<ul>
  <li>
    <p>Regarding the GNSS survey, various points for improvement were identified. Allocating more time to conduct the survey offers the opportunity to collect data under optimal conditions, ensuring minimal cloud cover for improved satellite connectivity. It’s highly recommended to avoid locations with high environmental noise, although in densely populated areas like the study site, finding such sites proved challenging. Natural landmarks, initially identified using high-resolution orthophotos, were utilized; however, a more efficient method for accurately georeferencing UAV-generated models involves setting markers during flight execution. Some markers selected during pre-field preparations had disappeared or changed by the time of the GNSS survey, posing a limitation to the method.</p>
  </li>
  <li>
    <p>The heterogeneity in data acquisition, spanning different UAV devices, flight heights, number of photos captured, varying point density within and between point clouds, and distinct spatial extents, poses challenges during post-processing and alignment.</p>
  </li>
</ul>

<h1 id="acknowledgements">Acknowledgements</h1>
<p>Funding for field work was provided by the BMBF project ESKOLA and the University of Potsdam.</p>

<h1 id="references">References</h1>

<p>Agisoft. (2023). Metashape professional 2.0 user manual. Retrieved from <a href="https://www.agisoft.com/pdf/metashape-pro_2_0_en.pdf">https://www.agisoft.com/pdf/metashape-pro_2_0_en.pdf</a></p>

<p>Besl, P. J., &amp; McKay, N. D. (1992). Method for registration of 3-d shapes. Sensor Fusion IV: Control Paradigms and Data Structures, 1611, 586–606.</p>

<p>Brook, M. S., &amp; Merkle, J. (2019). Monitoring active landslides in the auckland region utilising UAV/structure-from-motion photogrammetry. Japanese Geotechnical Society Special Publication, 6(2), 1–6.</p>

<p>Chen, Y., &amp; Medioni, G. (1992). Object modelling by registration of multiple range images. Image and Vision Computing, 10(3), 145–155.</p>

<p>Fischler, M. A., &amp; Bolles, R. C. (1981). Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6), 381–395.</p>

<p>IGAC. (2012). Atlas geográfico. In Obtenido de <a href="http://atlasgeografico.net/departamento-del-meta.html">http://atlasgeografico.net/departamento-del-meta.html</a>.</p>

<p>Lucieer, A., Jong, S. M. de, &amp; Turner, D. (2014). Mapping landslide displacements using structure from motion (SfM) and image correlation of multi-temporal UAV photography. Progress in Physical Geography, 38(1), 97–116.</p>

<p>Niethammer, U., James, M., Rothmund, S., Travelletti, J., &amp; Joswig, M. (2012). UAV-based remote sensing of the super-sauze landslide: Evaluation and results. Engineering Geology, 128, 2–11.</p>

<p>Pulido, N. (2003). Seismotectonics of the northern andes (colombia) and the development of seismic networks. Bulletin of the International Institute of Seismology and Earthquake Engineering, Special Edition, 69–76.</p>

<p>Rusu, R. B., &amp; Cousins, S. (2011). 3d is here: Point cloud library (pcl). 2011 IEEE International Conference on Robotics and Automation, 1–4.</p>

<p>SGC. (2020). ZONIFICACIÓN DE AMENAZA POR MOVIMIENTOS EN MASA EN EL MUNICIPIO DE ROSAS – CAUCA ESCALA 1:25.000.</p>

<p>SGC. (2023). Informe visita de emergencia a la microcuenca de la quebrada chontaduro y concepto técnico sobre el trazado alterno de la vía panamericana – municipio de rosas, departamento del cauca. Bogotá, enero de 2023.</p>

<p>Taboada, A., Rivera, L. A., Fuenzalida, A., Cisternas, A., Philip, H., Bijwaard, H., Olaya, J., &amp; Rivera, C. (2000). Geodynamics of the northern andes: Subductions and intracontinental deformation (colombia). Tectonics, 19(5), 787–813.</p>

<p>Yang, J., Li, H., Campbell, D., &amp; Jia, Y. (2015). Go-ICP: A globally optimal solution to 3D ICP point-set registration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(11), 2241–2254.</p>

<p>Zhang, Z. (1994). Iterative point matching for registration of free-form curves and surfaces. International Journal of Computer Vision, 13(2), 119–152.</p>]]></content><author><name>Lina Pérez</name></author><category term="UAV" /><category term="landslides" /><category term="point cloud" /><category term="point cloud registration" /><category term="ICP" /><summary type="html"><![CDATA[On January 9th, 2023, a catastrophic landslide severely impacted the upper basin of Chontaduro Creek, located in the municipality of Rosas-Cauca in southwestern Colombia. This work investigates landslide velocities and spatial patterns of deformation through repeated UAV flights. Introduction Landslides are natural disasters that can have devastating impacts on human communities and infrastructure. Similar to other natural hazards, their severity can increase due to cascading effects and can be exacerbated by recent global climate changes. Moreover, larger landslides can result in diverse forms of damage and casualties. To address these challenges, advanced technologies such as UAV-based photogrammetry and lidar are used to rapidly and cost-efficiently assess large landslides. However, the precise georeferencing, co-registration, and alignment of UAV surveys collected from different devices with varying resolutions, flight conditions and flight heights as well as varying areal coverage present significant challenges for data analysis. To tackle these challenges, this work explores different methods to ensure accuracy in the pre-processing of this type of data. This internship was supervised by Prof. Dr. Bodo Bookhagen. The Rosas-Cauco Landslide On January 9th, 2023, a catastrophic landslide severely impacted the upper basin of Chontaduro Creek, located in the municipality of Rosas-Cauca in southwestern Colombia (refer to Figure 1). This event caused extensive damage to the communities of La Soledad, Santa Clara, Párraga Viejo, and Chontaduro, resulting in the destruction of at least 150 homes and a significant portion of the Pan-American highway, which connects southern Colombia to the rest of the country. Inhabitants of the region reported their initial observations of instability on January 4th when they noticed small cracks with lengths ranging from 20 cm to 30 cm forming in the crown area near the Alfonso Córdoba school. However, it was not until the period between the afternoon of January 9th and the morning of January 10th that the landslide’s progression became starkly evident, leaving distinct markings on both the crown and flanks of the movement. Following this significant event, the landslide exhibited continuous and substantial activity until January 18th. By January 10th, the area affected by the mass movement had expanded to 64.72 hectares, and it further increased to 87.16 hectares by January 13th, ultimately reaching 89.12 hectares by January 18th. The Rosas landslide, situated in Colombia, has been closely monitored by several government institutions, including the Unidad Nacional de Gestión del Riesgo (UNGRD), the Geological Survey of Colombia (SGC), and the National Armed Forces. These agencies have conducted numerous UAV (Unmanned Aerial Vehicle) flights to monitor the ongoing evolution of the landslide. Over the past decade, there has been a notable increase in the utilization of remote sensing technology for landslide mapping and monitoring. This surge in usage has been complemented by significant improvements in the spatial resolution of remote sensing technologies, particularly with the introduction of laser scanning (both airborne and terrestrial) and unmanned aerial vehicles (UAVs) (Niethammer et al., 2012). Earth observation methods are useful to produce detailed multi-temporal sets of images, orthophotos, and digital elevation models (DEMs). These products may provide insight into the flow kinematics such as flow rate, landslide expansion, and accumulation at the toe zone or retreating scarps. In addition, these new techniques allow volume calculations of the accumulated and removed material by the landslide and mapping of the topographic changes (Lucieer et al., 2014). Furthermore, recent advances in photogrammetric image processing and computer vision have resulted in a technique known as Structure from Motion (SfM). Highly detailed 3D models can be obtained from overlapping multi-view photography with SfM algorithms. While spaceborne and airborne remote sensing techniques are commonly employed for landslide research, these methods may lack the flexibility offered by the novel remote sensing platform of UAVs. When combined with UAV technology, Structure from Motion (SfM) can provide a cost-effective and efficient means of acquiring dense and accurate 3D data of the Earth’s surface. In this study, we investigate various methods for processing the extensive UAV data acquired for the Rosas landslide, aiming to produce accurately georeferenced and aligned 3D models. Study area The Rosas landslide occurred in the rural district of La Soledad, located within the municipality of Rosas-Cauca, situated in the southwestern region of Colombia. This region lies within the Patía Valley intercordilleran depression, nestled between the Western Cordillera and the Central Cordillera of Colombia. The primary drainage network affected is that of the Chontaduro creek, a tributary of the Esmita River, which is part of the upper Patía River basin (refer to Figure 1). Figure 1: Shaded relief map of study area, showing major rivers, towns and a multi-temporal landslide inventory, sourced from SGC (2020) The study area is situated within the western range of the Colombian Andes, a region characterized by its susceptibility to landslides. This vulnerability arises from a combination of factors including steep terrain, adverse weather conditions, and the presence of soils derived from volcanic deposits originating from the Sotará volcanic complex. A comprehensive study conducted by the Geological Survey of Colombia-SGC (SGC, 2020), examined the development and distribution of landslide in the region. Their investigation identified a total of 273 landslide events occurring within the municipality of Rosas during the period spanning from 1990 to 2019. Of these occurrences, five were deemed severe, leading to tragic casualties, substantial economic losses, and considerable damage to critical infrastructure. As depicted in Figure 1, the area has a historical record of being affected by landslides of varying magnitudes. However, the Rosas landslide, which is the focal point of our study, stands out as the largest landslide identified in the surrounding region since 1990. Geological setting The geological setting of the study area is profoundly influenced by its location within the Northern Andes, where the convergence of three tectonic plates—the Nazca, Caribbean, and South American plates—gives rise to active N-NE faulting within the Andean block. This convergence, with the Nazca plate moving eastward relative to northwestern South America at a rate of 6 cm/yr, leads to the formation of the Colombia-Trench to the west and the mountain range of the Colombian Andes (Andes Pulido, 2003). Within the Colombian Andes, three distinct mountain ranges—the Western, Central, and Eastern Cordillera—converge southward into a unified range, shaped by deformation and faulting resulting from the interaction of these tectonic plates (Taboada et al., 2000). The study area, encompassing the upper Patía River basin, is situated at the southern Colombian Andes’ convergence point between the Western and Central Cordilleras. Here, the Western Cordillera rises dramatically from the Pacific Coastal Plain on its western flank, while to the east, it is separated from the Central Cordillera by the Cauca-Patia valleys spanning a distance of approximately 500 km. The Cauca-Patia valley, resembling a graben-like structure, is characterized by an asymmetric tilt and is filled with Tertiary-Quaternary continental clastics and volcanics. Bounded by the Cauca fault zone to the west and the Romeral fault zone to the east, this depression effectively separates the oceanic Western Cordillera from the ancient crystalline Central Cordillera (Figure 2). Figure 2: Geological map of the study area. Sourced from SGC (2020). Landslide event characterization The Rosas mass movement, situated within the micro-basin of Chontaduro creek, is characterized as an active rotational landslide of a complex nature, exhibiting retrogressive and widening behavior. This landslide initiation point is positioned 900 meters from the summit of Broncazo Hill and 700 meters from its base (Figure 3). Figure 3: Panoramic view of Rosas landslide. Sourced from SGC (2023). The landslide displays distinct patterns of movement and failure mechanisms across its various sections. In the uppermost region, known as the crown, where the movement commences, displacements are attributed to a rotational failure mechanism. Moving to the flanks, which constitute the lateral sides of the landslide, translational faults predominate. Within the body of the landslide, one observes intense deformation, sinking, and the formation of cracks, both transverse and parallel to the primary crown (Figure 4). In the lowermost part of the movement, termed the toe, situated furthest from the crown, translational landslides, soil block falls, and flows of earth, debris, and mud have been observed. These flows are channeled through areas with lower slopes. The dynamic interplay of these movements signifies the complex nature of the landslide, with its activity and behavior evolving in response to variations in water availability in the upper part of the basin. Figure 4: Identifiable characteristics across the crown, body, and toe of the landslide: 1) A curved concave rupture surface creating a scarp measuring up to 20 meters in height. 2) Presence of tilted blocks, subsidence, and formation of cracks. 3) Occurrence of translational slides and debris flows. Geology and failure surface The initial phase of the mass movement was characterized by a rotational slide, evidenced by a semicircular curved and concave upward rupture surface. This phase exposed a steep slope scarp reaching heights of up to 40 meters (SGC, 2023), composed of alternating layers of residual ash soils and pyroclastic flows from the Galeón Formation. Subsequently, a portion of the body adjacent to the crown tilted backward due to the concavity of the failure surface, resulting in ground subsidence. When the central region of the landslide became fully saturated with water, copious flows of earth and rock emanated from both sides of the landslide, as well as from its center. In the microbasin of Chontaduro creek, a geological sequence unfolds, comprising both epiclastic and volcanic rocks attributed to the Galeón Formation (TQpg). Notably, these formations are prominently exposed in areas such as Cerro Broncazo and its environs, encompassing the Quilcacé and Esmita rivers. In these regions, the volcaniclastic deposits can attain substantial thicknesses, reaching up to 800 meters (SGC, 2020). Figure 5: Lithological sequence and unit details observed at the crown of the landslide. The scarp of the crown provided a clear view of the sequential arrangement of materials comprising the mobilized mass. Illustrated in Figure 5 and Figure 6, from bottom to top, the lithological units are delineated as follows: The base layer showcases rocks attributed to well-selected, sandy, hyperconcentrated lahar deposits, linked with the Galeón formation. Directly above lies a layer of clast-supported pyroclastic flows, featuring rounded volcanic rock clasts with subtle orientation indicative of flow, ensconced within a reddish sandy volcanic ash matrix. Progressing upward, there exist lenticular alluvial and colluvial deposits, resulting from the erosion and transportation of various units within the Galeón formation. Finally, at the apex of the sequence, residual soils stemming from both transport deposits and pyroclastic flows are evident. Figure 6: Panoramic view of landslide crown scarp. Blue dashed line depicting the water horizon As depicted in Figure 6, a highly saturated surface conducive to water flow manifests between the hyperconcentrated flow layer and the pyroclastic flow layer. The permeability of the latter, coupled with the slight permeability of the underlying layer, facilitates water flow across this contact surface. Exhibiting horizontal continuity, this surface could be associated with the sliding surface of the mass movement.Recurrence of the poorly permeable hyperconcentrated lahar layer is notable at the base of the slide, indicating a seamless continuation of the water-saturated surface from the crown to the landslide’s base. Furthermore, noteworthy is the close proximity of the landslide’s crown to the ‘Alto de las Yerbas’ fault line, as referenced in the technical report on Landslide Hazard Zoning by the SGC (2020), detailed in Figure 7. Findings from the present study imply that structural and lithological factors, compounded by intense rainfall, were the primary catalysts for the occurrence of this significant landslide. Figure 7: Geological surface units and main fault in the landslide site. Sourced from SGC (2020). Rainfall as trigger mechanism According to the Atlas of Colombia (IGAC, 2012), the region experiences an average annual rainfall of 2,000 mm at the summit of the Central Cordillera and 3,000 mm at the summit of the Western Cordillera. Rainfall patterns indicate a distinct summer season during June, July, August, and September, followed by a rainy season during the remaining months. The precipitation data for the study area was obtained from the Párraga weather station (Station Code: 52010050), which is managed by IDEAM (Instituto de Hidrología, Meteorología y Estudios Ambientales de Colombia). The time series analyzed encompasses 33 years of recorded data, spanning from 1990 to 2023 (Figure 8 shows the last 20 years of data). As shown in Figure 8, the study area experiences two distinct rainy seasons: the first occurs from January to May, characterized by relatively lower rainfall compared to the second rainy season, which typically occurs between October and December. An important observation to highlight is that within the last 20 years of recorded data, the rainfall during the rainy season of December 2022, preceding the event, was notably exceptional. In November 2022 alone, precipitation levels reached a record high of up to 720 mm, marking it as the highest recorded rainfall within the observed period. Figure 8: Visualization of total monthly rainfall at the Parraga meteorological station spanning the past 20 years. Blue bars represent months with rainfall above the mean, while green bars denote months with precipitation below the mean. In the other hand, the daily time series recorded at the Párraga station exhibits a range of maximum daily precipitation values, varying between 0 mm and 95 mm. The highest recorded value, 94.8 mm, was documented on January 9th, which coincided with the onset of the landslide event (Figure 9). The cumulative rainfall data was computed over the last 60 days leading up to the triggering of the landslide, revealing an impressive total rainfall accumulation of 2309.3 millimeters recorded up to January 9, 2023 (Figure 9). Figure 9: Total daily and accumulated rainfall at the Parraga meteorological station. Methods Aerial digital photographs of the landslide were captured on multiple occasions between January 9th and March 18th. Predominantly, these flights were conducted at an altitude of 250 meters. Over the span of 17 days, a total of 22 flights were executed by three organizations specializing in risk management and emergency disaster response: Unidad Nacional para la Gestión del Riesgo (UNGRD), Servicio Geológico Colombiano (SGC), and Policía Nacional (PNAL). Despite the involvement of various institutions, uniformity was maintained in the equipment used, ensuring consistent resolution in the photographs. However, aspects such as flight altitude, coverage area, flight plan design, and the number of captured images exhibited significant variation. A further challenge encountered in the data acquisition process was the georeferencing of the flights. During the initial flights, markers were not employed, leading to difficulties in precise location mapping. To address this, on May 18th, a set of markers was installed to ensure at least one flight could be accurately georeferenced. However, these newly placed markers proved too small to be discernible in images taken from higher altitudes. Consequently, to enhance the overall georeferencing accuracy of all flights, an independent survey was executed as part of this study, utilizing natural points as reference markers in the final survey. Point Cloud Generation The Structure from Motion (SfM) method uses algorithms to identify matching features in a series of overlapping digital images and calculates camera location and orientation from the differential positions of multiple matched features. From these calculations, overlapping imagery is then used to reconstruct a “sparse” or “coarse” 3D point cloud model of the photographed object or surface or scene (Brook et al., 2019). The software Agisoft Metashape was used to undertake the SfM processing. In Metashape, the SFM processing comprises three key steps (Agisoft, 2023): Alignment: Involves aerial triangulation and bundle block adjustment, where the software identifies feature points on images, matches them into tie points and determine camera positions, resulting in a tie point cloud and camera positions for depth map determination and 3D reconstruction. Surface Generation: Creates a 3D mesh or 2.5D digital elevation model (DEM), combining photogrammetric depth maps and LiDAR data, which can be textured and exported for various applications. Orthomosaic Creation: Generates a georeferenced orthomosaic by projecting images onto a chosen surface (DEM or mesh), useful for mapping and analysis, including vegetation indices in multispectral imagery projects. In this study we adopted the workflow implemented in Agisoft Metashape, described in the User Manual v2.0 (Agisoft, 2023). The parameters used are described n the following sections. Image loading and chunks definition Sixteen distinct chunks were produced by importing photographs from selected flights (refer to Table 1), of which five met key selection criteria: a sufficient quantity of photographs, high image quality characterized by proper exposure and sharpness, and extensive coverage of the landslide area. This careful selection was necessary as some flights were tasked with monitoring more restricted sectors of the mass movement, targeting specific areas such as the crown or the Pan-American Highway. For comprehensive area coverage and improved model resolution during that timeframe, photographs from January 18th and 19th were merged in one single chunk. Chunks Cameras Points Markers Processed 1/14/2023 1491 880,613 19 markers X 1/17/2023 567 436,618 4 markers   1/18-19/2023 878 1,666,175 17 markers   1/19/2023 369 387,054   X 1/20/2023 196 194,101     1/26/2023 332 352,573     1/30/2023 321 351,487     2/3/2023 418 403.19     2/6/2023 331 336,810     2/11/2023 576 627,729     2/16/2023 329 346,862     2/19/2023 442 459,783     3/6/2023 330 324,661     3/21/2023 331 352.909     3/30/2023 328 327,605     5/18/2023 333 243,261     Table 1: Point clouds generated in Agisoft. Photos alignment and filtering In each chunk, the photographs underwent an alignment process with the accuracy setting set to high, utilizing equipment source coordinates as a reference point. Camera optimization was conducted, selecting the estimation of the covariance of tie points. Subsequently, apparent outliers were removed from the sparse cloud manually to diminish potential reconstruction inaccuracies. Additional refinement was carried out by filtering based on tie point covariance and uncertainties, employing Metashape’s integrated Python scripting capabilities for a more precise adjustment (Figure 10). Figure 10: Tie point covariances obtained after filtering. Dense Point Cloud generation For all chosen chunks, a dense 3D point cloud was constructed employing high-quality settings and mild depth filtering. This detailed point cloud serves as the foundation for generating Digital Surface Models (DSM) and orthomosaics. Additional settings were selected to reuse depth maps, calculate point colors and calculate point confidence (Figure 11). Figure 11: Dense point cloud generated for 14/01/2023 survey Natural points definition and GNSS survey To accurately reference the point clouds, a series of natural markers were established to guarantee high-precision (centimeter-level) localization that was readily identifiable in the field. Utilized reference points included distinct features such as sewer edges, fence corners, and light posts (see Figure 12). Out of the 27 natural markers delineated (see Figure 13), only 21 were successfully surveyed on-site. The survey faced several challenges, including the demolition of referenced structures, obscuring of markers by debris or soil, and inaccessible locations of certain points. The coordinates for each marker were recorded using a Trimble SPS855 GNSS Modular Receiver. Figure 12: Example of a marker defined Figure 13: Markers defined. Green points represent the markers defined and recorded during field work Following the download of raw data from the Trimble receiver, the post-processing software GrafNav was employed. GrafNav is an advanced GNSS post-processing suite that supports kinematic and static analysis with an advanced processing engine for GPS, GLONASS, and BeiDou signals. To refine the data, corrections were applied using the Popayan station (POPA), which is the station nearest to the collection site. The required data were acquired from the Colombian open data portal (https://www.colombiaenmapas.gov.co/). Both the RINEX files from the field-surveyed markers and the permanent station were inputted into GrafNav to execute the correction process. The precision-adjusted positions of the markers are shown in Table 2 and Figure 14. Figure 14: Points depict the resulting standard deviation for horizontal and height measurements, while bars illustrate a quality metric established by GrafNav. As depicted in Figure 14, both the resulting horizontal and vertical standard deviations are below 0.656 meters, with mean values of 0.26 and 0.14 meters, respectively. Achieving accuracy at the centimeter level indicates favorable results. However, a metric of quality provided by Grafnav offers additional insight. This metric assigns values as follows: Quality 1: Represents a fixed integer solution with excellent satellite geometry. Quality 2-3: Indicates either fixed integers with marginal geometry or converging float solutions. Quality 4-5: Suggests qualities akin to those of DGPS. Quality 6: Represents a coarse acquisition (C/A) only solution. The majority of solutions fall within the quality range of 2-3, with one solution (marker 1) exhibiting a very good quality of 1 and another (marker 10) displaying a poor quality of 6. All solutions showed satisfactory results and were subsequently utilized in the following step, which involves the precise alignment of point clouds based on markers. Date GPSTime Easting Northing H-Ell H-MSL Q SDHoriz SDHeigh (m) Marker 8/09/2023 32:23.0 302995.460 248590.161 1514963 1486998 1 0.016 0.028 1 8/09/2023 59:34.0 302445.401 247995.626 1346507 1318573 3 0.12 0.072 12 8/09/2023 32:47.0 302540.380 247938.191 1342117 1314170 5 0.559 0.402 13 8/09/2023 50:24.0 302928.080 248596.004 1508566 1480608 3 0.18 0.112 2 8/09/2023 01:48.0 302956.219 248607.942 1512854 1484894 4 0.367 0.129 25 8/09/2023 25:50.0 302984.155 248601.450 1513498 1485535 5 0.257 0.13 3 8/10/2023 53:34.0 302155.060 247741.470 1338077 1310161 6 0.291 0.181 10 8/10/2023 26:29.0 302045.545 247650.392 1328182 1300273 3 0.347 0.215 21 8/10/2023 29:50.0 302207.493 247767.666 1341687 1313767 2 0.243 0.138 23 8/10/2023 03:19.0 302229.752 247892.380 1344103 1316187 3 0.343 0.217 24 8/10/2023 10:53.0 301981.062 248881.942 1547867 1520018 3 0.263 0.137 26 8/10/2023 58:14.0 301798.636 248108.698 1446887 1419024 3 0.19 0.117 4 8/10/2023 09:31.0 301847.911 248155.130 1446869 1419003 3 0.364 0.186 6 8/10/2023 31:22.0 301868.008 248170.170 1446827 1418960 3 0.147 0.072 7 8/10/2023 30:20.0 301875.048 248297.576 1452434 1424571 3 0.235 0.161 8 8/10/2023 00:47.0 301968.070 248386.332 1451916 1424048 4 0.235 0.09 9 8/11/2023 58:29.0 301263.276 246688.966 1308378 1280506 4 0.656 0.64 17 8/11/2023 40:23.0 301456.505 246350.882 1292738 1264830 2 0.054 0.055 18 8/11/2023 04:19.0 301430.773 246394.964 1296034 1268131 2 0.077 0.064 19 8/11/2023 30:58.0 301364.741 246498.245 1300102 1272211 4 0.364 0.259 20 8/11/2023 34:20.0 301586.893 247430.776 1319567 1291696 5 0.604 0.323 27 Table 2: GNSS data corrected in Grafnav. Chunk alignment For a precise alignment of point clouds, the process begins with the correction of a designated reference point cloud; in this case, the cloud dated 01/14/2023 is used. The coordinates of the markers are revised using the data acquired from the recent field campaign. Following this update, camera parameters are re-optimized, and a new dense point cloud is generated. This newly referenced point cloud then serves as the reference for aligning subsequent point clouds through the point-based method, a technique available to align chunks within Agisoft. Despite this registration, some misalignment persist. To address this, some pre-filtering steps and global registration methos are appied to subsequently apply a meticulous registration is conducted using the Iterative Closest Point (ICP) method, elaborated in the following sections. Pre-processing and Global registration The registration process was exclusively conducted within the stable zones identified within the study area. By avoiding regions experiencing landslide activity, where the terrain undergoes constant changes, we ensure a precise correlation between zones exhibiting minimal alterations over time. The stable areas from both the reference and target point clouds were manually selected and extracted using Cloud Compare. Subsequently, pre-registration steps were performed on these stable area point clouds, including: Initial Filtering Step: This involved the removal of statistical outliers using an Open3D function. Points that deviate significantly from the average distance to their neighbors were identified as outliers and removed. This step was iterated until all points not forming a continuous surface were eliminated. Nearest Neighbor Distance Calculation: Next, KDtree was employed to compute the nearest neighbor distances between both point clouds (Figure 15). In some regions where there was a lack of overlapping between the point clouds, resulting distances exceeded 3 meters. To address this issue and ensure complete overlap, points farther than 3 meters from one point cloud to the other were filtered out. Figure 15: Nearest neighbor distances between both point clouds. After the initial filtering step, the alignment process proceeds with an initial coarse alignment to bring the point clouds into close proximity. This alignment method incorporates feature identification to refine the search for corresponding points. This process generally involves two key steps: Utilizing Fast Point Feature Histograms (FPFH): FPFH is a technique employed in 3D registration, which aims to align multiple 3D point clouds. FPFH analyzes each point in a 3D point cloud by examining its surrounding points. It computes a histogram describing the geometric features of each point based on its neighbors, encompassing attributes like angle and distance between neighboring points. FPFH facilitates the matching of points across different 3D scenes by leveraging the geometric properties of each point and its surroundings (Rusu et al., 2011). Application of registration_ransac_based_on_feature_matching Function: Open3D’s registration_ransac_based_on_feature_matching function implements the RANSAC (Random Sample Consensus) algorithm tailored for point cloud registration via feature matching. RANSAC randomly selects a minimal subset of points from both point clouds to estimate a transformation, such as rotation and translation, between them (Fischler et al., 1981). Subsequently, the estimated transformation is evaluated by applying it to all feature correspondences obtained in the previous step, and the number of inliers—correspondences consistent with the estimated transformation—is tallied. In essence, these refined alignment techniques enable the accurate registration of point clouds by iteratively estimating transformations based on feature correspondences, thereby enhancing the robustness and accuracy of the alignment process. Although the global alignment has improved the distribution of distances, as depicted in Figure 16, with a mean value of less than 2 meters, the Global Registration Modified Hausdorff Distance has increased to 3.75 meters. This discrepancy underscores the necessity for a more precise registration process to minimize the offset between the two point clouds. Figure 16: Nearest neighbor distances between both point clouds before and after global registration. ICP registration Unlike global alignment, fine registration methods require two point clouds that already have a rough correspondence. The Iterative Closest Point (ICP) algorithm is commonly used for local refinement and has two main variants: Point-to-point ICP The Iterative Closest Point (ICP) algorithm, introduced in the early 1990s by various authors (Besl et al. (1992), Chen et al. (1992), Zhang (1994)), stands out among the myriad registration methods proposed in the literature. It has become one of the most well-known for efficiently registering two 2D or 3D point sets under Euclidean (rigid) transformation. The process follows a simple concept (Yang et al., 2015): Begin with an initial transformation involving rotation and translation. Alternate between two steps: a. Establish closest-point correspondences under the current transformation. b. Estimate a new transformation using these correspondences. Repeat steps 2a and 2b until convergence is achieved. Notably, the point-to-point ICP method is capable of directly processing the raw point sets without regard to their inherent characteristics, such as distribution, density, and noise level. The registration_icp function of Open3D aligns both stable-area point clouds using the global transformation matrix from the previous step via ICP registration. subsequently, we use the evaluate_registration function that calculates two key metrics: Fitness, that measures overlapping area (# of inlier correspondences / # of points in target), higher values indicate better alignment. And Inlier RMSE, that measures RMSE of all inlier correspondences with lower values indicating better alignment. As depicted in Figure 17, the final registration demonstrates excellent performance, with mean distances smaller than 50 centimeters. Subsequently, the final transformation matrix derived from this registration is applied to the full dataset, encompassing both stable and landslide areas. Figure 17: Nearest neighbor distances between both point clouds before and after global registration and ICP registration. In Table 3, a comprehensive overview of the effectiveness of various registration methods in enhancing the alignment of point clouds is provided. By meticulously examining the metrics presented in the table, including distances between corresponding points before and after registration, the impact of each method on the alignment quality becomes evident.   Hausdorff Distance Modified Hausdorff Distance Before global alignment 154.51 3.312067224 After global alignment 145.26 3.75 After ICP - 2.41 Table 3: Hausdorff and Modified Hausdorff distance calculated throughout the registration process Key project insights Regarding the GNSS survey, various points for improvement were identified. Allocating more time to conduct the survey offers the opportunity to collect data under optimal conditions, ensuring minimal cloud cover for improved satellite connectivity. It’s highly recommended to avoid locations with high environmental noise, although in densely populated areas like the study site, finding such sites proved challenging. Natural landmarks, initially identified using high-resolution orthophotos, were utilized; however, a more efficient method for accurately georeferencing UAV-generated models involves setting markers during flight execution. Some markers selected during pre-field preparations had disappeared or changed by the time of the GNSS survey, posing a limitation to the method. The heterogeneity in data acquisition, spanning different UAV devices, flight heights, number of photos captured, varying point density within and between point clouds, and distinct spatial extents, poses challenges during post-processing and alignment. Acknowledgements Funding for field work was provided by the BMBF project ESKOLA and the University of Potsdam. References Agisoft. (2023). Metashape professional 2.0 user manual. Retrieved from https://www.agisoft.com/pdf/metashape-pro_2_0_en.pdf Besl, P. J., &amp; McKay, N. D. (1992). Method for registration of 3-d shapes. Sensor Fusion IV: Control Paradigms and Data Structures, 1611, 586–606. Brook, M. S., &amp; Merkle, J. (2019). Monitoring active landslides in the auckland region utilising UAV/structure-from-motion photogrammetry. Japanese Geotechnical Society Special Publication, 6(2), 1–6. Chen, Y., &amp; Medioni, G. (1992). Object modelling by registration of multiple range images. Image and Vision Computing, 10(3), 145–155. Fischler, M. A., &amp; Bolles, R. C. (1981). Random sample consensus: A paradigm for model fitting with applications to image analysis and automated cartography. Communications of the ACM, 24(6), 381–395. IGAC. (2012). Atlas geográfico. In Obtenido de http://atlasgeografico.net/departamento-del-meta.html. Lucieer, A., Jong, S. M. de, &amp; Turner, D. (2014). Mapping landslide displacements using structure from motion (SfM) and image correlation of multi-temporal UAV photography. Progress in Physical Geography, 38(1), 97–116. Niethammer, U., James, M., Rothmund, S., Travelletti, J., &amp; Joswig, M. (2012). UAV-based remote sensing of the super-sauze landslide: Evaluation and results. Engineering Geology, 128, 2–11. Pulido, N. (2003). Seismotectonics of the northern andes (colombia) and the development of seismic networks. Bulletin of the International Institute of Seismology and Earthquake Engineering, Special Edition, 69–76. Rusu, R. B., &amp; Cousins, S. (2011). 3d is here: Point cloud library (pcl). 2011 IEEE International Conference on Robotics and Automation, 1–4. SGC. (2020). ZONIFICACIÓN DE AMENAZA POR MOVIMIENTOS EN MASA EN EL MUNICIPIO DE ROSAS – CAUCA ESCALA 1:25.000. SGC. (2023). Informe visita de emergencia a la microcuenca de la quebrada chontaduro y concepto técnico sobre el trazado alterno de la vía panamericana – municipio de rosas, departamento del cauca. Bogotá, enero de 2023. Taboada, A., Rivera, L. A., Fuenzalida, A., Cisternas, A., Philip, H., Bijwaard, H., Olaya, J., &amp; Rivera, C. (2000). Geodynamics of the northern andes: Subductions and intracontinental deformation (colombia). Tectonics, 19(5), 787–813. Yang, J., Li, H., Campbell, D., &amp; Jia, Y. (2015). Go-ICP: A globally optimal solution to 3D ICP point-set registration. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(11), 2241–2254. Zhang, Z. (1994). Iterative point matching for registration of free-form curves and surfaces. International Journal of Computer Vision, 13(2), 119–152.]]></summary></entry><entry><title type="html">Generation of high-resolution digital topography of a rugged anticline using SPOT-6 satellite images and the NASA Ames Stereo Pipeline</title><link href="http://localhost:4000/AnanyaPandey_DEM_Generation/" rel="alternate" type="text/html" title="Generation of high-resolution digital topography of a rugged anticline using SPOT-6 satellite images and the NASA Ames Stereo Pipeline" /><published>2024-03-05T00:00:00+01:00</published><updated>2024-03-05T00:00:00+01:00</updated><id>http://localhost:4000/AnanyaPandey_DEM_Generation</id><content type="html" xml:base="http://localhost:4000/AnanyaPandey_DEM_Generation/"><![CDATA[<p>Regional-scale Earth surface process analyses require cost-effective, large-area coverage, high-resolution digital elevation models (DEMs). Stereogrammetry, employing optical data from two or more overlapping images, emerges as a viable method for generating 3D topography in areas with sparse vegetation cover.</p>

<h1 id="introduction">Introduction</h1>

<p>Digital elevation models (DEMs) are crucial for analyzing the geomorphic processes shaping landscapes. Investigating such processes in regions characterized by both active tectonics and climate sensitivity provides valuable insights into associated hazards. This work focuses on the arid intermontane Calchaquí valley of the Eastern Cordillera of the NW Argentinian Andes, a region undergoing active deformation and highly sensitive to climatic forces. The area witnesses significant discharge events during the South American Monsoon season, resulting in heavy rainfall and flooding. Its predominantly dry, sparsely vegetated land is vulnerable to extreme discharge events. Hence, topographic analyses of this area are of relevance.</p>

<p>While freely available DEMs often lack the resolution to reveal intricate topographic details, high-resolution lidar-derived DEMs are costly and practical only for small-scale evaluations. For regional-scale assessments, stereogrammetry provides a cost-effective, high-resolution option for generating digital topography (Shean et al., 2019). Stereogrammetry generates three-dimensional (3D) models of objects or landscapes with the help of two or more overlapping pairs of photographs taken from different viewpoints.</p>

<p>This study uses the SPOT 6 (Satellite pour l’Observation de la Terre) images to generate a high-resolution (3m) DEM. The DEM is created with the help of the NASA Ames Stereo Pipeline, a collection of freely available and open-source automated tools for geodesy and stereogrammetry (Beyer et al., 2018). This DEM serves for studying actively deforming structures in an intra-continental setting in NW Argentinian Andes. Additionally, multiple DEMs generated using different algorithms and parameters have been compared to analyze their accuracy.</p>

<p><em>This internship was supervised by Prof. Dr. Bodo Bookhagen.</em></p>

<h1 id="study-region">Study Region</h1>

<p>The fault-bounded intermontane Calchaquí valley lies in the southeastern part of the Eastern Cordillera of the NW Argentinian Andes. The area is part of the Andean foreland and falls in the Salta Province, near the city of Cachi. The ongoing Andean orogenic deformation has led to the formation of inverted structures in the Andean foreland of NW Argentina (Strecker et al., 2012). This has resulted in thick-skinned deformation and recent seismicity. The reactivation of lower Paleozoic and Cretaceous basement structures has formed basement-cored mountain ranges with partially connected depocenters (Carrera et al., 2006).</p>

<p>The DEM created in this work is of an anticlinal feature in the Calchaquí valley, presumably formed over a blind thrust fault, due to the reactivation of the basement structure. The DEM serves as a basis for studying how these structures form and evolve by looking into the behavior of river networks and the deformation of fluvial terraces. This can also be used for hazard studies such as debris flows and landslides.</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_1.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_1.png" width="90%" height="90%" /></a><figcaption> Figure 1: Map of the study area. (A) Map showing the location of the anticline and the surrounding regions in the continent of South America. (B) Map showing the proximity of the study area to the city of Salta in Argentina. (C) The zoomed-in view of the area of investigation - the yellow box highlights the anticline under study. </figcaption>
</figure>
</center>

<h1 id="data">Data</h1>

<ul>
  <li>Panchromatic tri-stereo SPOT 6 satellite imagery at a resolution of 1.5m of a growing anticline in the Calchaquí valley</li>
  <li>Copernicus 30m DEM</li>
</ul>

<h1 id="methods">Methods</h1>

<p>The work is divided into two sections. The first section discusses the basic steps involved in DEM generation using the Ames Stereo Pipeline (ASP). In the second, comparative analyses on DEMs created using different algorithms and parameters have been performed.</p>

<h2 id="part-1-dem-generation">Part 1: DEM Generation</h2>

<p>A high-resolution DEM can be generated using ASP by analyzing stereo images taken from different viewpoints. These images captured from slightly different angles help to create depth perception and reconstruct the spatial structure of the observed terrain. ASP provides a range of stereo-matching algorithms and predefined parameters designed for creating a DEM. Some of the tools that ASP offers for DEM generation are described below.</p>

<p><strong>Bundle Adjustment</strong></p>

<p>Error in satellite position and orientation can affect the accuracy of a DEM created using ASP. These errors can be corrected using a pre-processing step called bundle adjustment. Bundle adjustment helps 
  minimize the error between the estimated pixel locations of 3D objects and their actual positions in the captured images -  ensuring consistency among observations of a single ground feature across multiple 
  images.</p>

<p><strong>Stereo Correlation</strong></p>

<p>Correlation is the most important process within ASP. Stereo correlation matches the neighborhood of each pixel in the left image to a similar neighborhood in the right image. It works on a pair of overlapping 
  stereo images from corresponding cameras and creates a point cloud that can be converted to a DEM.</p>

<p><strong>Point Cloud Alignment</strong></p>

<p>An existing ground truth, such as a DEM, can be used to rectify imperfectly calibrated camera intrinsic parameters. This significantly reduces distortions in the resulting DEMs which closely align with the 
  ground truth. This work uses Copernicus 30m DEM for point cloud alignment. The resultant transformation matrix can also be used as a part of the bundle adjustment step for robust spatial coherence.</p>

<p><strong>Map-projection</strong></p>

<p>The stereo correlation process can fail if the two images are very different – if the cameras have very different perspectives, the terrain has very steep portions. This results in large disparity values and 
  3D terrains with noise or missing values. ASP deals with this issue with the implantation of map-projection – the left and the right images can be projected onto a low-resolution smooth terrain without holes. 
  The resultant map-projected images are then used for stereo correlation.</p>

<ul>
  <li>
    <p>Workflow Summary</p>

    <p>Step 1: The original SPOT 6 stereo images are used during the bundle adjustment <em>(bundle_adjust)</em> step for correcting camera intrinsics.</p>

    <p>Step 2: Stereo correlation <em>(parallel_stereo)</em> is performed on the images using the bundle adjustment results obtained in <em>Step 1</em>.</p>

    <p>Step 3: The resultant point cloud is converted to a DEM <em>(point2dem)</em>.</p>

    <p>Step 4: This DEM is aligned <em>(pc_align)</em> to the Copernicus 30m DEM to obtain a transformation matrix.</p>

    <p>Step 5: This transformation matrix is further utilized for a second round of bundle adjustment <em>(bundle_adjust)</em> for precise spatial coherence.</p>

    <p>Step 6: The original images are then map-projected onto the lower-resolution Copernicus DEM. This step also uses the camera adjustment obtained by previously running <em>bundle_adjust</em> in <em>Step 5</em>.</p>

    <p>Step 7: Stereo correlation is performed again using the map-projected images and the camera adjustment results obtained by running <em>bundle_adjust</em> in <em>Step 5</em>.</p>

    <p>Step 8: The point cloud is converted to a final DEM <em>(point2dem)</em>.</p>

    <center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_2.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_2.png" width="90%" height="90%" /></a><figcaption> Figure 2: Ames Stereo Pipeline - Workflow Summary </figcaption>
</figure>
</center>
  </li>
</ul>

<h2 id="part-2-dem-comparison">Part 2: DEM Comparison</h2>

<p>Various stereo-matching algorithms and other parameters offered by ASP have been used in generating DEMs. Multiple DEMs generated have been compared by two means:</p>

<p>a) Visual Inspection  - Smoothness of hillshade, presence of holes, or other artifacts.</p>

<p>b) Intersection Error  - as a relative measurement of accuracy</p>

<p><em>How can intersection error be used as a relative measurement of accuracy?</em></p>

<p>The rays emanating from matching pixels in the cameras rarely intersect perfectly on the ground because any slight error in the position of the cameras will affect the accuracy of the rays. The actual shortest distance between the rays at this point is an important error metric that measures how self-consistent the two camera models are for this point. The distance between the two emanating rays from matching points in the cameras at their closest point of intersection is the triangulation error or the intersection error (Beyer et al., 2018). Hence, the intersection error can be used as a relative measure of the accuracy of a DEM.</p>

<p>The following parameters have been explored to compare DEM quality:</p>

<ul>
  <li>
    <p>Different Stereo-matching Algorithms</p>

    <p>ASP offers several stereo-matching algorithms of which the following three have been used:</p>

    <p>(i) <em>Block Matching</em> Algorithm (BM): The block matching algorithm divides the target image into fixed-size blocks and finds for each block the corresponding block in the other image that provides the best 
     match (Fuhrt, 2008).</p>

    <p>(ii) <em>Semi-Global Matching</em> Algorithm (SGM): The SGM stereovision algorithm enables pixel-wise  matching between a pair of stereo images. The results obtained using this algorithm are highly accurate and the 
     speed to generate results is high (Hirschmüller, 2008). The only downside is its characteristic streaks in the final disparity image.</p>

    <p>(iii) <em>More-Global Matching</em> Algorithm (MGM): The MGM algorithm is similar to the SGM algorithm with a few extra operations per pixel. It gives higher-quality results by removing the streaking artifacts of 
       SGM. MGM produces qualitatively and quantitatively denser results than SGM with little computation overhead (Facciolo et al., 2015).</p>
  </li>
</ul>

<p><strong>Using Map-projected vs Original Images</strong></p>

<ul>
  <li>
    <p>Different Image Pairs</p>

    <p>SPOT imagery has three image acquisitions over the area of interest within the same orbital pass. The three images are shot with different viewing angles within the same orbit. This enables the generation of 
3D models over the area of interest. The three images are named A, B, and C. The second image, B, is acquired by a near-vertical angle. The near-nadir image acquisition enables viewing hidden items and is 
useful for urban or mountainous areas. The anticline in the present study has some rough patches of very steep terrain for which the second image, B, might be useful. However, the anticline also 
has areas for which oblique viewing angles work better, and, in that case, image pair AC might be more useful.</p>
  </li>
  <li>
    <p>Different Sizes of Correlation Kernels and Sub-Pixel Kernels</p>

    <p>The correlation kernel size determines the window size in the left and the right image for pixel comparison. Smaller kernel sizes might enhance results for intricate features, but it could also 
potentially increase the occurrence of false matches or noise. Three different combinations of correlation kernel and sub-pixel kernel sizes were explored.</p>

    <p>(i) Correlation kernel: 5x5; Sub-pixel kernel: 11x11</p>

    <p>(ii) Correlation kernel: 7x7; Sub-pixel kernel: 15x15</p>

    <p>(iii) Correlation kernel: 9x9; Sub-pixel kernel: 21x21</p>
  </li>
</ul>

<h1 id="results">Results</h1>

<h2 id="choice-of-stereo-matching-algorithms">Choice of Stereo-matching Algorithms</h2>

<p><strong>Hillshade of the DEMs Overlain by Elevation Values</strong></p>

<p>In Figure 3, the MGM DEM (C) has the least holes, and the BM DEM (A) has the most holes. On a closer inspection (zooming in on the DEMs), it can also be seen that the MGM and the SGM DEMs are smoother when 
   compared to the BM DEM. Overall, the MGM DEM produces the least number of artifacts.</p>

<center>
   <figure>
   <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_3.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_3.png" width="90%" height="90%" /></a><figcaption>  Figure 3: A - BM DEM, B - SGM DEM, C - MGM DEM </figcaption>
   </figure>
   </center>

<p><strong>Intersection Errors of the DEMs</strong></p>

<p>The intersection error plot, which is used in this study as a relative measurement of the accuracy of a DEM, also reveals lower overall intersection errors for the SGM and the MGM DEM (which are comparable), 
   and higher intersection error for the BM DEM.</p>

<center>
   <figure>
   <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_4.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_4.png" width="90%" height="90%" /></a><figcaption>  Figure 4: Plot showing comparable intersection errors for the SGM and the MGM algorithms 
   whereas higher intersection errors for the BM algorithm 
   </figcaption>
   </figure>
   </center>

<p><strong>A Zoomed-in Perspective</strong></p>

<p>The most jagged portion of the anticline is selected to look at the performance of the different stereo-matching algorithms in more difficult terrain scenarios.</p>

<p>(i) More artifacts for the BM DEM: The hillshade reveals that the BM DEM has more artifacts than the other two DEMs.</p>

<center>
   <figure>
   <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_5.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_5.png" width="90%" height="90%" /></a><figcaption>  Figure 5: Hillshade reveals more artifacts in rugged areas for the BM DEM </figcaption>
   </figure>
   </center>

<p>(ii) Higher intersection error for the BM DEM: The intersection error map of the same region reveals denser areas of higher intersection error values for the BM DEM.</p>

<center>
   <figure>
   <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_6.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_6.png" width="90%" height="90%" /></a><figcaption>  Figure 6: Denser regions of high intersection error for the BM DEM </figcaption>
   </figure>
   </center>

<p>(iii) Less good pixel region (more bad pixel region) for the BM DEM: ASP facilitates generating a “good pixel map” which indicates (in gray) pixels that were successfully matched with the correlator, and (in 
   red) those that were not matched. Larger areas of unsuccessfully matched pixels can be seen for the BM algorithm.</p>

<center>
   <figure>
   <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_7.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_7.png" width="90%" height="90%" /></a><figcaption>  Figure 7: Grey regions show well-matched pixels and red regions show badly matched pixels. 
   Larger areas of badly matched pixels can be seen for the BM DEM. </figcaption>
   </figure>
   </center>

<h2 id="map-projecting-the-original-images">Map-projecting the Original Images</h2>

<p><strong>Intersection Error of the DEMs</strong></p>

<p>The DEM created using map-projected images has a lower overall intersection error than the one created using original images.</p>

<center>
  <figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_8.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_8.png" width="90%" height="90%" /></a><figcaption> Figure 8: Intersection error plot for DEMs created using map-projected and original images 
  </figcaption>
  </figure>
  </center>

<p><strong>Hillshade and Intersection Error Map of a Zoomed-in Area of the Anticline</strong></p>

<p>The hillshade of the DEM created using map-projected images has fewer artifacts and is relatively smoother.</p>

<center>
  <figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_9.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_9.png" width="90%" height="90%" /></a><figcaption> Figure 9: Hillshade and intersection error map of a zoomed-in area of the anticline. The map- 
  projected hillshade has fewer artifacts and is relatively smoother.</figcaption>
  </figure>
  </center>

<h2 id="choice-of-image-pairs">Choice of Image Pairs</h2>

<p><strong>Intersection Error of the DEMs</strong></p>

<p>The intersection error of the DEM created using the oblique image pair, AC, is the highest. However, it appears that for this image pair, there are larger intersection errors in the very jagged portions which 
  might have contributed to the overall higher intersection errors (see below for intersection error maps). The other not-so-jagged portions (which constitute a larger area than a few jagged belts) have lower 
  intersection errors for the DEM created using the oblique image pair (AC).</p>

<center>
  <figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_10.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_10.png" width="90%" height="90%" /></a><figcaption> Figure 10: Intersection error plot for DEMs created using three different image pairs 
  </figcaption>
  </figure>
  </center>

<ul>
  <li>
    <p>Hillshade and Intersection Error Map of Rugged Areas with Steep Slopes</p>

    <p>The near-nadir image acquisition for image B enables viewing hidden items and is useful for rugged terrain. The anticline in the present study has some areas with near-vertical slopes for which the DEM 
created using an oblique image (A or C) and a near-nadir image (B) has a smoother hillshade and lower intersection errors.</p>

    <center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_11.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_11.png" width="90%" height="90%" /></a><figcaption> Figure 11: The jagged portion of the anticline where the image pair AC with oblique views 
produces the highest intersection error values. A combination of an oblique view image (A or C) with the near-nadir image (B) produces results with much lower intersection error. </figcaption>
</figure>
</center>
  </li>
</ul>

<p><strong>Hillshade and Intersection Error Map of Areas with Gentler Slopes</strong></p>

<p>The areas that do not have very steep slopes are resolved better using oblique images A and C. The hillshade is smoother.</p>

<center>
  <figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_12.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_12.png" width="90%" height="90%" /></a><figcaption> Figure 12: In the not-so-jagged portion, the image pair AC has the lowest intersection error 
  values and the least number of artifacts. </figcaption>
  </figure>
  </center>

<h2 id="choice-of-correlation-kernel-and-sub-pixel-kernel-size">Choice of Correlation Kernel and Sub-pixel Kernel Size</h2>

<p><strong>Intersection Error of the DEMs</strong></p>

<p>The subpixel kernel sets the correlation kernel size in units of pixels. The intersection error of the DEM created using a correlation kernel size of 9, and a sub-pixel kernel size of 21 is the lowest.</p>

<center>
  <figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_13.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_13.png" width="90%" height="90%" /></a><figcaption> Figure 13: Intersection error plot for DEMs created using three different kernel sizes 
  </figcaption>
  </figure>
  </center>

<p><strong>Hillshade and Intersection Error Map of a Zoomed-in Area of the Anticline</strong></p>

<p>Based on a visual inspection, the DEM created using a kernel size 21x21 is the smoothest.</p>

<center>
  <figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_14.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_14.png" width="90%" height="90%" /></a><figcaption> Figure 14: Correlation kernel size of 9 produces the smoothest DEM </figcaption>
  </figure>
  </center>

<h1 id="discussion">Discussion</h1>

<h2 id="comparison-of-intersection-errors">Comparison of Intersection Errors</h2>

<p>Based on the results, the MGM DEM produces the best quality DEM as opposed to the SGM and the BM DEM. It is worth comparing the intersection errors of these DEMs to the standard MGM DEM. A Q-Q plot, which is a
   scatterplot created by plotting two sets of quantiles against one another can be used to compare the intersection errors. It can be noted that while the SGM DEM’s intersection error values slightly deviate 
   from the MGM DEM towards higher intersection errors, the BM DEM’s values are significantly deviated.</p>

<center>
   <figure>
   <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_15.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_15.png" width="90%" height="90%" /></a><figcaption> Figure 15: Q-Q plot of intersection errors (with MGM intersection error values as "standard" 
   values). </figcaption>
   </figure>
   </center>

<h2 id="elevation-difference-between-dems">Elevation Difference between DEMs</h2>

<p>Elevation difference values between the MGM DEM and the SGM and the BM DEM respectively can express how closely the DEMs align with the standard MGM DEM. A distribution of these elevation difference values 
   shows a higher standard deviation for the BM DEM as compared to the SGM DEM.</p>

<ul>
  <li>Mean dh (MGM - SGM): -0.05 +/- 1.12</li>
  <li>Mean dh (MGM - BM): -0.10 +/- 2.15</li>
</ul>

<center>
   <figure>
   <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_16.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_16.png" width="90%" height="90%" /></a><figcaption> Figure 16: Elevation difference distribution </figcaption>
   </figure>
   </center>

<h2 id="disparity-map">Disparity Map</h2>

<p>The most important process of ASP is stereo correlation. ASP has a collection of algorithms that compute correspondences between pixels in the left image and pixels in the right image. A map showing these 
  correspondences is the disparity map. With the above observations, MGM DEM has been selected as the one producing the most accurate results. It is worth looking into the disparity maps of this algorithm. ASP 
  facilitates dividing the total disparity into vertical and horizontal components. It can be noted that the more rugged features have higher offset magnitudes as compared to the other features of the terrain. 
  This also suggests that the corresponding pixels are most probably incorrectly matched in very steep regions.</p>

<center>
  <figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_17.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_17.png" width="90%" height="90%" /></a><figcaption> Figure 17: x offset - horizontal disparity, y offset - vertical disparity. Map showing 
  disparity components for the MGM DEM </figcaption>
  </figure>
  </center>

<h2 id="map-projection">Map-projection</h2>

<p>Map-projection transforms the original images into a pair of closely aligned images - this leaves only minor perspective differences between the images, which precisely correspond to the features targeted by 
   the stereo correlation process (Beyer et al., 2018). GIF images can elaborate qualitatively on the effect of map-projection on the original images and an improvement in incorrect disparity values between 
   corresponding pixels of the stereo images.</p>

<center>
   <figure>
   <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_18.gif"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_18.gif" width="90%" height="90%" /></a><figcaption> Figure 18: Disparity between original images </figcaption>
   </figure>
   </center>

<center>
   <figure>
   <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_19.gif"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/AnanyaPandey_figs/Figure_19.gif" width="90%" height="90%" /></a><figcaption> Figure 19: Disparity after map-projection </figcaption>
   </figure>
   </center>

<h1 id="conclusion">Conclusion</h1>

<p>Stereogrammetry holds its importance in generating high-resolution surface models in areas of high relief and sparse vegetation. ASP offers a multitude of parameters for creating and refining a high-resolution DEM. Several parameters have been discussed in this study and it can be concluded that map-projecting the stereo images onto a pre-existing DEM is key to generating accurate DEMs. The disparity between corresponding pixels in original images is not a real perspective difference and can lead to large DEM errors if they are not map-projected before the stereo correlation step. It can also be seen that in difficult terrains, the MGM algorithm can create significantly better results than the other algorithms.</p>

<p>This study highlights the importance of exploring intersection errors across various DEMs to measure their relative accuracy. The DEMs that visually appear smoother and better in quality than others also consistently possess lower intersection error values.</p>

<h1 id="references">References</h1>

<p>Beyer, R. A., Alexandrov, O., and McMichael, S. (2018). The ames stereo pipeline: NASA’s open source software for deriving and processing terrain data. Earth and Space Science, 5(9):537–548.</p>

<p>Carrera, N., Muñoz, J. A., Sàbat, F., Mon, R., &amp; Roca, E. (2006). The role of inversion tectonics in the structure of the Cordillera Oriental (NW Argentinean Andes). Journal of Structural Geology, 28(11), 1921–1932. https://doi.org/10.1016/j.jsg.2006.07.006.</p>

<p>Facciolo, G., de Franchis, C., and Meinhardt, E. (2015). MGM: A Significantly More Global Matching for Stereovision. British Machine Vision Conference.</p>

<p>Furht, B. (2008). Encyclopedia of Multimedia. Springer, Boston, MA. https://doi.org/10.1007/978-0-387-78414-4_132.</p>

<p>Hirschmüller, H. (2008). Stereo processing by semiglobal matching and mutual information. IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 30, no. 2, pp. 328-341.</p>

<p>Shean, D. E., Joughin, I. R., Dutrieux, P., Smith, B. E., and Berthier, E. (2019). Ice shelf basal melt rates from a high-resolution digital elevation model (dem) record for Pine Island Glacier, Antarctica. The Cryosphere, 13(10):2633–2656.</p>

<p>Strecker, M. R., Hilley, G. E., Bookhagen, B., &amp; Sobel, E. R. (2012). Structural, geomorphic, and depositional characteristics of contiguous and broken foreland basins: Examples from the eastern flanks of the Central Andes in Bolivia and NW Argentina. Tectonics of Sedimentary Basins, edited, pp. 508–521.</p>]]></content><author><name>Ananya Pandey</name></author><category term="digital topography" /><category term="SPOT-6" /><category term="stereophotogrammetry" /><category term="NASA Ames Stereo Pipeline" /><category term="Calchaquí Valley" /><summary type="html"><![CDATA[Regional-scale Earth surface process analyses require cost-effective, large-area coverage, high-resolution digital elevation models (DEMs). Stereogrammetry, employing optical data from two or more overlapping images, emerges as a viable method for generating 3D topography in areas with sparse vegetation cover.]]></summary></entry><entry><title type="html">A line-based segmentation and subsampling approach for classifying point clouds using the random forest algorithm</title><link href="http://localhost:4000/TLS_scanline_LuisKremer/" rel="alternate" type="text/html" title="A line-based segmentation and subsampling approach for classifying point clouds using the random forest algorithm" /><published>2023-10-23T00:00:00+02:00</published><updated>2023-10-23T00:00:00+02:00</updated><id>http://localhost:4000/TLS_scanline_LuisKremer</id><content type="html" xml:base="http://localhost:4000/TLS_scanline_LuisKremer/"><![CDATA[<p>Dense point clouds generated from Terrestrial Lidar Scanner pose a challenge to classification algorithm because of their large data amounts. Here a scanline segmentation approach for phase-based lidar scanner is presented that will allow to apply common classification algorithms.</p>

<h1 id="introduction">Introduction</h1>

<p>3D data acquired from devices like laser scanners are commonly shown as point clouds (PCDs). Classifying point clouds with supervised machine learning algorithms can have two fundamental challenges: 1) the amount of data is large and lead to computationally expensive calculation that may exceed the capabilities of current GPUs; and 2) unlike gridded 2D data (e.g., images or digital elevation models), raw point clouds are irregularly shaped and unstructured, making it challenging to apply conventional 2D machine learning methods (Hu et al., 2020; Su et al., 2018; Thomas et al., 2019).</p>

<p>To address these challenges in classification, recent studies have employed pre-processing techniques to convert irregular point clouds into a uniform 3D volumetric grid using voxelization. While voxelization aids in subsampling and structuring the data, it results in the loss of geometric details and natural variances prevailing in the raw point cloud (Liu et al., 2019; Su et al., 2018). This loss of information is particularly significant in forest ecology, where detailed analyses of structural metrics (e.g., leaf shape, branch dimensions, or crown properties) from Terrestrial Laser Scanning (TLS) data is essential (Calders et al., 2020).</p>

<p>This work addresses the issue of information loss due to pre-processing steps of point clouds such as voxelization. Based on a scanline segmentation and subsampling approach, we seek to significantly reduce the amount of data while keeping most relevant information of the points within a segment. We use a Random Forest (RF) classifier to classify the subsampled data and map the results back to the raw point cloud. The outcome is a classification on the raw, full resolution data, enabling to extract features like trees or leaves without having information loss.</p>

<p><em>This internship was supervised by Prof. Dr. Bodo Bookhagen.</em></p>

<h1 id="methods">Methods</h1>

<h2 id="study-site-and-data-acquisition">Study site and data acquisition</h2>

<p>Ten TLS scans of an oak tree were acquired on the campus of the University of Potsdam in April 2023 (Figure 1). The TLS scanner used was the Z+F Imager 5016, which translates the phase shift between the outgoing and incoming laser pulses into distance estimates (Calders et al., 2020). The scanner provides x-y-z coordinates, intensity (backscatter strength) and RGB information for each point. About 6-7 million points were recorded per scan.</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/01_study_area.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/01_study_area.png" width="90%" height="90%" /></a><figcaption> Figure 1: Excerpt of the ten scans processed in the Z+F LaserControl software. The dots are the individual scan positions around the oak tree. The lines are the connections between the scans and the markers detected for alignment. The bold lines are walls. </figcaption>
</figure>
</center>

<h2 id="scanline-extraction">Scanline extraction</h2>

<p>Point clouds can be described by either using the commonly encountered Cartesian coordinates (\(x\), \(y\), \(z\)) or spherical coordinates. In spherical coordinates, a point vector is described by the radial coordinate \(r\) (distance from the scanner to the point), the vertical angle \(\theta\) (angle with respect to the z-axis) and the horizontal angle \(\phi\) (angle with respect to the x-y axis).</p>

<p>A TLS scans the environment in discrete scanlines that are closely spaced (Figure 2b). Points along a scanline have either similar vertical angles (vertical scanline) or similar horizontal angles (horizontal scanline). In this work, we focus on points with similar horizontal angles to extract the scanlines.</p>

<p>Sorting all points of one scan by their horizontal angles results in a stepped line (Figure 2a). Each step represents a slight rotation of the scanner around the vertical axis, indicating the start of a new scanline.</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/02_scanline_extraction.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/02_scanline_extraction.png" width="90%" height="90%" /></a><figcaption> Figure 2:  Shows the first 20,000 points from scan 1, sorted by horizontal angle (Φ). Figure A shows the plot of the point index against the horizontal angle. Figure B shows the same scene as a 3D plot, displaying points with their corresponding x-y-z coordinates. The red dot indicates the position of the scanner. The scene depicts the point cloud within a range of about 1 centimeter in the x-direction, 30 meters in the y-direction, and 15 meters in the z-direction. There is a wall on the left and the oak tree on the right side of the scanner. </figcaption>
</figure>
</center>

<p>Steps are identified by calculating the difference between consecutive points in the sorted point cloud. A significant difference indicates a knickpoint, which is the beginning of a new scanline (Figure 3). A threshold of 0.002 degrees was chosen. Whenever the difference between two points exceeded this threshold, the point cloud was split at this index creating a new scanline. With this method, about 5000 scanlines were extracted from a single TLS scan.</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/03_scanline_extraction.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/03_scanline_extraction.png" width="90%" height="90%" /></a><figcaption> Figure 3: Figure A shows the point cloud separated by the scanline boundaries. Each color represents an individual scanline. Figure B shows the same scene as a 3D plot. The colors in A mark the corresponding scanline in B. </figcaption>
</figure>
</center>

<h2 id="scanline-segmentation">Scanline segmentation</h2>

<p>For each scanline (Figure 4), a two-step segmentation process was implemented. First, the radial distance differences (RDD; x-axis of Figure 4a) of consecutive point pairs were calculated on the sorted and extracted scanline. For example, the differences in \(r\) across the scanline are large (Figure 4a) when there is a gap between two groups of points. Whenever the difference exceeded a predetermined threshold, the scanline was divided at that point, creating a new segment (Figure 5). A RDD threshold value of 0.4 m was selected for this work. For the specific scanline shown in Figure 5, the segmentation based on radial distance differences resulted in 21 segments.</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/04_scanline_segmentation.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/04_scanline_segmentation.png" width="90%" height="90%" /></a><figcaption> Figure 4: Shows the first scanline from scan 1 as A) the scanner distance (<i>r</i>) against the height [m], and B) as a 3D plot.  </figcaption>
</figure>
</center>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/05_scanline_segmentation.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/05_scanline_segmentation.png" width="90%" height="90%" /></a><figcaption> Figure 5: Shows the first scanline from scan 1, segmented using radial differences between points. Each color represents a segment.  </figcaption>
</figure>
</center>

<p>In the second segmentation step, the segments generated in the previous stage underwent further processing based on differences in slope. For each consecutive pair of points within a segment, the slope was calculated. For a number of \(n\) points within a segment the slope calculation can be expressed as:</p>

\[d_i = \sqrt{\left| (x_{2i} - x_{1i})^2 \right| + \left| (y_{2i} - y_{1i})^2 \right| + \left| (z_{2i} - z_{1i})^2 \right| }\]

<p>and</p>

\[m_{\text{deg}, i} = \frac{180°}{\pi} \times \arctan \left( \frac{z_{1i} - z_{2i}}{d_i} \right)\]

<p>where:</p>

<ul>
  <li>\(i\) ranges from 1 to \(n-1\)</li>
  <li>\(d_{i}\) is the 3D distance between each of the \(n-1\) consecutive point pairs: \((x_{1i}, y_{1i}, z_{1i})\) and \((x_{2i}, y_{2i}, z_{2i})\)</li>
  <li>\((m_{\text{deg}, i})\) is the slope between each of the \(n-1\) consecutive point pairs in degrees</li>
</ul>

<p>Similarly to the previous segmentation stage, for each computed slope value, the difference in slope between successive pairs was calculated. If the difference exceeded a certain slope threshold, the segment was further divided (Figure 6). For this work, a slope threshold of 25 degrees was chosen.</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/06_scanline_segmentation.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/06_scanline_segmentation.png" width="90%" height="90%" /></a><figcaption> Figure 6: Shows the first scanline from scan 1, further segmented using slope differences between points. Each color represents a segment.  </figcaption>
</figure>
</center>

<h2 id="scanline-subsampling">Scanline subsampling</h2>

<p>Finally, each segment was reduced to its centroid, representing the median position of all points within the segment. It can be considered as the center of mass of the segment. However, to preserve the natural structure of the input point cloud, the point closest to the centroid was chosen as the centroid (Figure 7).</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/07_01_centroid_subsampling.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/07_01_centroid_subsampling.png" width="90%" height="90%" /></a><figcaption> Figure 7: Centroid subsampling of the first scanline from scan 1. Each segment is condensed to the closest point to the segment median (red dots). </figcaption>
</figure>
</center>

<p>During this process, 12 attributes characterizing the segment were calculated and assigned to the centroid. The attributes are the <strong>mean</strong> and <strong>variance</strong> of:</p>

<ul>
  <li>Red</li>
  <li>Green</li>
  <li>Blue</li>
  <li>Intensity</li>
  <li>Rho (\(r\))</li>
  <li>Height (\(z\))</li>
</ul>

<p>Thus, along with the x-y-z coordinates, every centroid was represented by an array in the form of a row with 15 columns (1, 15). With a total of \(n\) segments resulting from the segmentation process, the final subsampled point cloud array had the shape (\(n\), 15) (Figure 8).</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/07_02_example_subsampling_scan01.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/07_02_example_subsampling_scan01.png" width="90%" height="90%" /></a><figcaption> Figure 8: Centroid subsampling result of the first scan. The point colors represent the mean intensity per segment. The brighter the color, the higher the intensity. The original full resolution data of 6,711,986 points was subsampled to 227,136 points. </figcaption>
</figure>
</center>

<p>In order to incorporate the RF classification results from the centroid subsampled data into the raw, full resolution point clouds, each segment and its corresponding centroid was associated with a separate array containing the segment ID. Thus, each segment was identified by its unique ID, and the corresponding centroid shared the same ID.</p>

<h1 id="application-random-forest-classification">Application: random forest classification</h1>

<p>The ten centroid subsampled point clouds were used as the starting point for a random forest classification. A random forest consists of multiple decision trees, each generated independently by sampling subsets of the training data. The final classification is determined by the majority class among the trees. This means that each tree contributes a vote for the class assignment. The class with the most votes is finally selected (Belgiu und Drăguţ, 2016). The number of trees generated for the RF classifier is user-defined - for this work a value of 100 was chosen.</p>

<p>We selected an RF classifier rather than a neural network approach because of its suitability for classification tasks with limited training data. Its architecture, based on decision trees, mitigates overfitting caused by an unbalanced selection of training data (Belgiu und Drăguţ, 2016). In addition, the RF classifier efficiently manages numerous input features and automatically selects the most appropriate ones for the final classification. That enables a transparent evaluation of the importance of different features after training, helping to determine the relevance of each feature (Niemeyer et al., 2014; Weinmann et al., 2015).</p>

<h2 id="classification-workflow">Classification workflow</h2>

<p>Before training and applying the RF classifier to the ten point clouds derived from the data acquisition, they were processed using the scanline segmentation approach and centroid subsampling described in the previous chapter. The first scan of the subsampled datasets was used for both training and testing the RF classifier. The trained model was then applied to the remaining nine scans to classify the point clouds. After classification of the centroid subsampled point clouds, the results were re-incorporated into the original full resolution data. The overall accuracy metric was used to evaluate the classification. It is important to note that we acknowledge the limitations of relying on accuracy alone as a metric to evaluate the performance of the RF classifier. However, the focus was on assessing the general applicability of the method rather than incorporating fully comprehensive performance measures of the model. Therefore, only this metric was used in the evaluation.</p>

<h2 id="training-labels">Training labels</h2>

<p>Manual labeling was applied to all points within the ten point clouds. Our focus was on ground and vegetation related features. As a result, a significant amount of points were designated as unclassified, relating to objects such as walls, bicycles, cars or street lights. The classes considered for training/testing and prediction included:</p>

<ul>
  <li>Class 0: unclassified</li>
  <li>Class 2: ground</li>
  <li>Class 3: low vegetation</li>
  <li>Class 4: leaves</li>
  <li>Class 5: tree</li>
</ul>

<p>For the training data set, the classes were unbalanced. Therefore, class weights were applied based on their respective frequencies:</p>

<table>
  <thead>
    <tr>
      <th>Class</th>
      <th>Frequency</th>
      <th>Weight</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>0</td>
      <td>24.469</td>
      <td>1.94</td>
    </tr>
    <tr>
      <td>2</td>
      <td>41.793</td>
      <td>1.14</td>
    </tr>
    <tr>
      <td>3</td>
      <td>2.369</td>
      <td>20.03</td>
    </tr>
    <tr>
      <td>4</td>
      <td>97.202</td>
      <td>0.49</td>
    </tr>
    <tr>
      <td>5</td>
      <td>71.379</td>
      <td>0.66</td>
    </tr>
  </tbody>
</table>

<h1 id="results">Results</h1>

<h2 id="overall-accuracy">Overall accuracy</h2>

<p>The overall accuracy assessment of the RF classification results was performed on the centroid subsampled point clouds and the raw, full resolution point clouds (Figure 9). For all datasets, the accuracy is above 90%, indicating a strong overall classification performance. Scans closer to the first scan (training dataset), such as scans 2, 3, 9, and 10, generally have higher accuracy scores than scans further away, such as scans 5, 6, and 7. The centroid subsampled datasets show slightly better results compared to the back-labeled full resolution datasets. However, these differences are small for most scans, becoming noticeable only for scans 5 and 7.</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/08_rf_results.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/08_rf_results.png" width="90%" height="90%" /></a><figcaption> Figure 9: Result of the random forest classification on nine scans. The figure shows the accuracy scores for the full resolution datasets and their corresponding centroid subsampled point clouds.  </figcaption>
</figure>
</center>

<h2 id="visual-impression">Visual impression</h2>

<p>The overall accuracy results show that the full resolution scans 5 and 7 perform below average. A comparison between the RF-labeled full resolution dataset of scan 5 (Figure 10a) and the ground truth dataset (Figure 10b) shows accurate classification in many areas. However, misclassifications occur mainly at the beginning of the oak trunk and to the left of the pavement, where bushes and low vegetation are present.</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/09_rf_results_visual_scene1.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/09_rf_results_pcd.png" width="90%" height="90%" /></a>
</figure>
</center>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/10_ground_truth_visual_scene1.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/10_ground_truth_pcd.png" width="90%" height="90%" /></a><figcaption> Figure 10: Visual comparison of the RF classification result (A) and the true labels (B).  </figcaption>
</figure>
</center>

<h2 id="feature-importance">Feature importance</h2>

<p>As mentioned above, a notable advantage of RF classification over other machine learning approaches is the ability to infer the importance of input features. In our RF model, it is evident that the mean height and mean intensity of the segments are key features that significantly influence the model’s classification decisions (Figure 11). Within the RGB values, the blue band information appears to have the most significant impact. In general, segment mean values play a more critical role in the RF classifier’s decision-making than segment variance values. Among the statistical measures, height and intensity are the most important, while the green and red bands are the least influential.</p>

<center>
<figure>
<a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/11_rf_feature_importance.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_scanline_LuisKremer/11_rf_feature_importance.png" width="90%" height="90%" /></a><figcaption> Figure 11: Feature importance of the RF model used to classify the points clouds. </figcaption>
</figure>
</center>

<h1 id="discussion-and-conclusion">Discussion and conclusion</h1>

<p>Overall, our proposed method for classifying TLS point clouds with a random forest classifier is able to achieve promising results. The aim was to condense the amount of data while preserving as much information as possible within the point clouds. This was achieved by a two-step scanline segmentation followed by centroid reduction of these segments. The centroids were assigned 12 attributes for classification. The advantage of this approach is the ability to seamlessly integrate the classification results of the subsampled point clouds back into the raw full resolution point clouds, facilitated by the shared unique IDs between segments and centroids.</p>

<p>Evaluation of the classification accuracy on the full resolution raw point clouds consistently shows values above 90% across all scans. Identified classification errors occur primarily at the beginning of the oak trunk and adjacent to the pavement where low vegetation is present. These errors appear as long, incorrect colored stripes. It is evident that the segment boundaries do not perfectly separate ground from low vegetation and the oak trunk. This behavior is attributed to sub-optimal parameters of the thresholds in the two-step segmentation process. However, defining a universal threshold that accurately segments objects along scanlines in TLS point clouds is challenging. Point density varies significantly depending on the distance of an object from the scanner. Therefore, closer objects require different thresholds than those further away. To address this issue, future improvements could include the implementation of adaptive thresholds. For example, considering the previous 10, 50, or 100 neighbors and comparing the point-to-point differences to the mean difference of these previous neighbors could increase the flexibility of the segmentation. This adaptive approach could mitigate the challenge of erroneous object delineation due to incorrect parameterization.</p>

<p>Further improvements to the proposed method could focus on the features used for random forest classification and their significance. It is evident that mean feature values are of greater value to the classifier than variance values. Consequently, including more mean feature values in the model could improve the classification result. To address this, the computation of features could be extended by including three meaningful attributes that were not explored in this work: slope, curvature, and segment orientation. Segment orientation can be determined by calculating the point normals within a segment. These additional attributes could improve the results by adding structural components specific to elements such as ground, leaves, branches, or trunks. With these considerations, the proposed method could possibly yield even more meaningful outcomes. This could allow in-depth analysis of classified point clouds at full resolution, providing the basis for a comprehensive understanding of vegetation morphology.</p>

<h1 id="references">References</h1>

<p>Belgiu, M., &amp; Drăguţ, L. (2016). Random forest in remote sensing: A review of applications and future directions. <em>ISPRS journal of photogrammetry and remote sensing, 114</em>, 24-31.</p>

<p>Calders, K., Adams, J., Armston, J., Bartholomeus, H., Bauwens, S., Bentley, L. P., … &amp; Verbeeck, H. (2020). Terrestrial laser scanning in forest ecology: Expanding the horizon. <em>Remote Sensing of Environment, 251</em>, 112102.</p>

<p>Hu, Q., Yang, B., Xie, L., Rosa, S., Guo, Y., Wang, Z., … &amp; Markham, A. (2020). Randla-net: Efficient semantic segmentation of large-scale point clouds. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 11108-11117).</p>

<p>Liu, Y., Fan, B., Xiang, S., &amp; Pan, C. (2019). Relation-shape convolutional neural network for point cloud analysis. In <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em> (pp. 8895-8904).</p>

<p>Niemeyer, J., Rottensteiner, F., &amp; Soergel, U. (2014). Contextual classification of lidar data and building object detection in urban areas. <em>ISPRS journal of photogrammetry and remote sensing, 87</em>, 152-165.</p>

<p>Su, H., Jampani, V., Sun, D., Maji, S., Kalogerakis, E., Yang, M. H., &amp; Kautz, J. (2018). Splatnet: Sparse lattice networks for point cloud processing. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em> (pp. 2530-2539).</p>

<p>Thomas, H., Qi, C. R., Deschaud, J. E., Marcotegui, B., Goulette, F., &amp; Guibas, L. J. (2019). Kpconv: Flexible and deformable convolution for point clouds. In <em>Proceedings of the IEEE/CVF international conference on computer vision</em> (pp. 6411-6420).</p>

<p>Weinmann, M., Jutzi, B., Hinz, S., &amp; Mallet, C. (2015). Semantic point cloud interpretation based on optimal neighborhoods, relevant features and efficient classifiers. <em>ISPRS Journal of Photogrammetry and Remote Sensing, 105</em>, 286-304.</p>]]></content><author><name>Luis Kremer</name></author><category term="TLS" /><category term="LiDAR" /><category term="line-based approach" /><category term="segmentation" /><category term="classification" /><category term="random forest" /><summary type="html"><![CDATA[Dense point clouds generated from Terrestrial Lidar Scanner pose a challenge to classification algorithm because of their large data amounts. Here a scanline segmentation approach for phase-based lidar scanner is presented that will allow to apply common classification algorithms.]]></summary></entry><entry><title type="html">Terrestrial Laser Scanning in Campus Golm: Data Acquisition and Attributes</title><link href="http://localhost:4000/TLS-DavidHersh/" rel="alternate" type="text/html" title="Terrestrial Laser Scanning in Campus Golm: Data Acquisition and Attributes" /><published>2023-06-07T00:00:00+02:00</published><updated>2023-06-07T00:00:00+02:00</updated><id>http://localhost:4000/TLS-DavidHersh</id><content type="html" xml:base="http://localhost:4000/TLS-DavidHersh/"><![CDATA[<p>Collection of full-waveform terrestrial LiDAR data poses unique challenges during data acquisition and generates data with useful extra attributes. The scanning process and resulting data are described here for a dataset collected at the University of Potsdam Golm campus in August, 2022.</p>

<p>Terrestrial Laser Scanning (TLS) has applications in natural sciences such as in forestry, where common applications include estimation of above-ground biomass (Gonzales, et al., 2017), and estimation of biophysical parameters (Calders, et al., 2018). Natural hazards such as coastal retreat, glacier movement, lava flows, and landslides can also be monitored with TLS (Jones and Hobbs, 2021). Applications also include infrastructure monitoring including detecting cracks in concrete (Turkan, et al., 2018) and monitoring bridge displacement (Erdélyi, et al., 2020).</p>

<p>The aims of this internship was to create a point cloud of the University of Potsdam Golm campus using a terrestrial laser scanner. This blog describes data acquisition and extra attributes.</p>

<p><em>This internship was supervised by Prof. Dr. Bodo Bookhagen</em></p>

<p>Dr. Benjamin Brede is thanked for providing the Riegl Lidar instrument and for converting the data from the proprietary Riegl format to laz files.</p>

<h1 id="scanning-process">Scanning Process</h1>

<p>Scans were collected from August 22nd to 25th, 2022 using a Riegl VZ-400i scanner. Scan positions were chosen according to the trade-off between time limitations and a complete coverage of the campus. Each scan required approximately 60 seconds to turn 360 degrees, after which the scanner was moved to the next scan position. The distance between scan positions was chosen to be around 30 meters. The Riegl scanner has a real-time viewer which shows the current scan position in purple and previous scans in blue, as well as varying surface brightness to indicate coverage.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig1_VZ_app.jpg?raw=True"><img align="right" width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig1_VZ_app.jpg?raw=True" /></a>
 <figcaption> Figure 1. Scan Position and scan intensity. Brighter areas indicate a higher point density.&lt;/a&gt;  </figcaption>
    </figure>

<p>During scanning, the scanner screen showed objects which are captured in the point cloud as well as unreliable points (shown in red in figure 2).</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig2_scanpos91_filter.jpg?raw=True"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig2_scanpos91_filter.jpg?raw=True" /></a>
 <figcaption>Figure 2. Automated removal during scanning of unreliable points based on pulse return shape.&lt;/a&gt;  </figcaption>
    </figure>

<p>The unreliable points are based on the pulse shape using a proprietary Riegl algorithm. An example of a theoretical difference between filtered and unfiltered returns is shown in figure 3.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig3_full_waveform_comparison.png?raw=True"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig3_full_waveform_comparison.png?raw=True" /></a>
 <figcaption>Figure 3. General shape of full waveform data (left) where each peak represents a return point. When multiple returns are close together, as may be case of vegetation, the return waveform can be filtered to remove this unreliable point (right). Modified from Ullrich &amp; Pfennigbauer, 2011. &lt;/a&gt;  </figcaption>
    </figure>

<p>In total, the 222 scans created a complete point cloud of the Golm campus. Scanning from the roofs of buildings 27, 29, and 11 added point data to the top of buildings as well.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig4_roofscanning_clip_lr.jpg?raw=True"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig4_roofscanning_clip_lr.jpg?raw=True" /></a>
  <figcaption>Figure 4. VZ-400i scanning from the roof of building 11. The scanner has an attached GNSS receiver at the top and a DSLR camera.&lt;/a&gt;  </figcaption>
    </figure>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig5_Scan_position_map_lr.jpg?raw=True"><img align="center" width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig5_Scan_position_map_lr.jpg?raw=True" /></a>
 <figcaption>Figure 5. Overview of scan positions around the Golm campus.&lt;/a&gt;  </figcaption>
    </figure>

<p>The distance between scans was maintained at around 30 meters as shown in figure 6 below. When moving from the ground to roofs, the distance between scans spiked, which causes problems for the on-board registration of the Riegl scanner, resulting in mis-aligned scans.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig6_Scan-scan_distances.png?raw=True"><img align="center" width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig6_Scan-scan_distances.png?raw=True" /></a>
 <figcaption>Figure 6. Distances between consecutive scans.&lt;/a&gt;  </figcaption>
    </figure>

<h1 id="notes-on-errors-in-data-collection">Notes on errors in data collection</h1>

<p>During planning and the scanning processing, numerous factors can influence the data quality. Some important to consider include:</p>

<ol>
  <li>Weather conditions
    <ul>
      <li>Data collection is not possible in rain or dense fog due to the interaction with water (Rasshofer and Spies, 2011)</li>
    </ul>
  </li>
  <li>Scan position
    <ul>
      <li>Avoid occlusions due to objects</li>
      <li>Keep scan-scan distances as short as possible within time constraints</li>
      <li>Continuity of scan pattern (eg. pick up where you left off)</li>
    </ul>
  </li>
</ol>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig7_scanpos106_data_occlusion.png?raw=True"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig7_scanpos106_data_occlusion.png?raw=True" /></a>
 <figcaption>Figure 7. A scan position close to a large vehicle resulting in a gap in data behind the vehicle. This can be avoided by choosing scan positions away from large objects such as cars and trees. &lt;/a&gt;  </figcaption>
    </figure>

<h1 id="data-characteristics-and-attributes">Data characteristics and attributes</h1>

<p>After collecting data, the data was pre-processed in the Riegl software and files were exported in .laz format. Each scan had around 9 million points, giving a total number of points collected for the project at around 2 billion.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig8_points_per_scan.jpg?raw=True"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig8_points_per_scan.jpg?raw=True" /></a>
 <figcaption>Figure 8. Number of points collected for each scan. Locations closely surrounded by buildings such as the courtyard between 27 and 29 have higher point numbers, while roof scans have as low as approx. 5 million points. &lt;/a&gt;  </figcaption>
    </figure>

<p>Unlike with airborne lidar, the point densities of TLS drop quickly with distance from the scanner, therefore, each scan is considered an “unstructured” point cloud where densities are varying primarily with distance from the scanner, but also by the viewing geometry (eg. a building orthogonal to the scanner will be sampled more densely than if scanned from an oblique angle).</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig9_Distance_from_scanner_example.jpg?raw=True"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig9_Distance_from_scanner_example.jpg?raw=True" /></a>
 <figcaption>Figure 9. Example of the distribution of points based on the distance from the scanner for a single scan. Most points are within a few meters of the scanner. &lt;/a&gt;  </figcaption>
    </figure>

<p>Along with XYZ coordinates, each point has the following data:</p>
<ul>
  <li>RGB data from the attached DSLR camera</li>
  <li>Reflectance</li>
  <li>Deviation</li>
  <li>Return number</li>
  <li>Number of returns</li>
</ul>

<p>The attached DSLR camera takes 8 photos during scanning. RGB information is added to each scan automatically. The RGB information may be incomplete or incorrect, especially for partially obscured objects such as the far side of a scanned tree. Figure 10 shows common incorrect and missing data.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig10_scanpos50_rgb_annotated.jpg?raw=True"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig10_scanpos50_rgb_annotated.jpg?raw=True" /></a>
 <figcaption>Figure 10. Examples of incorrect or missing RGB data common in vegetation. &lt;/a&gt;  </figcaption>
    </figure>

<p>One of the important attributes is reflectance. Reflectance is a measure of the strength of the backscatter, which is valuable for classification, object detection and other uses, but has numerous noise components, especially viewing geometry (Kashani, et al., 2015). Using the Riegl scanner, the reflectance values are not corrected for viewing geometry. Therefore, using reflectance for classification or other tasks may require correction for viewing angle first.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig11_scan52_reflectance.png?raw=True"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig11_scan52_reflectance.png?raw=True" /></a>
 <figcaption>Figure 11. Difference in reflectance value across the same material. As the scan angle becomes more oblique, more energy is directed away from the scanner, so reflectance decreases along the building height. &lt;/a&gt;  </figcaption>
    </figure>

<p>Another attribute of the data is deviation, an integer value describing how many different elements contribute to a given return pulse (Calders et al., 2017). This value is also a function of the distance from the scanner (Li et al., 2018).</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig12_Deviation_example_scanpos40_lr.jpg?raw=True"><img width="800" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig12_Deviation_example_scanpos40_lr.jpg?raw=True" /></a>
 <figcaption>Figure 12. Pulse deviation values for a single scan. High values in vegetation indicate that mutiple components are present (eg. an irregular return pulse shape). &lt;/a&gt;  </figcaption>
    </figure>

<p>A last important extra attribute is the return number and number of returns. Most points in TLS are close to the scanner, and the beam divergence for the Riegl VZ-400i is ~0.3 mrad, resulting in a very small beam footprint. However, the return characteristics may be useful (eg. the first return of multiple returns could indicate vegetation). The return characteristics are shown below.</p>

<figure>
  <a href="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig13_return_characteristics_scanpos5.png?raw=True"><img width="2000" src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_figs_Hersh/fig13_return_characteristics_scanpos5.png?raw=True" /></a>
 <figcaption>Figure 13. Example return characteristics for 7,566,659 points from scan position 5 include the return number (a), number of returns (b) and with these attributes, the return ratio (c) can be computed as return number/number of returns. &lt;/a&gt;  </figcaption>
    </figure>

<h1 id="final-remarks">Final remarks</h1>

<p>Terrestrial laser scanning in the (semi-) urban environment of the Golm campus using the Riegl VZ-400i generated a large (over 2 billion points) point cloud. The full waveform data has extra attributes which can be used for classification. Some attributes, such as RGB information, is immediately usable. Other attributes, such as reflectance, may require further processing (eg. correction for viewing geometry) before further use. Data acquisition is time-consuming and susceptible to numerous errors. Therefore, planning for scanning requires careful consideration of weather conditions, objects in the scene (such as vehicles) and ideal scan patterns.</p>

<h1 id="references">References</h1>

<p>Calders, K., Disney, M., Armston, J., Burt, A. J., Brede, B., Origo, N., Muir, J., &amp; Nightingale, J. (2017). Evaluation of the Range Accuracy and the Radiometric Calibration of Multiple Terrestrial Laser Scanning Instruments for Data Interoperability. IEEE Transactions on Geoscience and Remote Sensing, 55(5), 2716–2724. https://doi.org/10.1109/tgrs.2017.2652721</p>

<p>Calders, K., Origo, N., Burt, A. J., Disney, M., Nightingale, J., Raumonen, P., Åkerblom, M., Malhi, Y., &amp; Lewis, P. (2018). Realistic Forest Stand Reconstruction from Terrestrial LiDAR for Radiative Transfer Modelling. Remote Sensing, 10(6), 933. https://doi.org/10.3390/rs10060933</p>

<p>Erdélyi, J., Kopáčik, A., &amp; Kyrinovič, P. (2020). Spatial Data Analysis for Deformation Monitoring of Bridge Structures. Applied Sciences, 10(23), 8731. https://doi.org/10.3390/app10238731</p>

<p>Jones, L., &amp; Hobbs, P. V. (2021). The Application of Terrestrial LiDAR for Geohazard Mapping, Monitoring and Modelling in the British Geological Survey. Remote Sensing, 13(3), 395. https://doi.org/10.3390/rs13030395</p>

<p>Kashani, A., Olsen, M. H., Parrish, C. C., &amp; Wilson, N. (2015). A Review of LIDAR Radiometric Processing: From Ad Hoc Intensity Correction to Rigorous Radiometric Calibration. Sensors, 15(11), 28099–28128. https://doi.org/10.3390/s151128099</p>

<p>Li, X., Yang, B., Xie, X., Li, D., &amp; Xu, L. (2018). Influence of Waveform Characteristics on LiDAR Ranging Accuracy and Precision. Sensors, 18(4), 1156. https://doi.org/10.3390/s18041156</p>

<p>Rasshofer, R. H., Spies, M., &amp; Spies, H. (2011). Influences of weather phenomena on automotive laser radar systems. Advances in Radio Science, 9, 49–60. https://doi.org/10.5194/ars-9-49-2011</p>

<p>Turkan, Y., Hong, J., Laflamme, S., &amp; Puri, N. (2018). Adaptive wavelet neural network for terrestrial laser scanner-based crack detection. Automation in Construction, 94, 191–202. https://doi.org/10.1016/j.autcon.2018.06.017</p>

<p>Ullrich, A., &amp; Pfennigbauer, M. (2011, September). Echo digitization and waveform analysis in airborne and terrestrial laser scanning. In Photogrammetric week (Vol. 11, pp. 217-228).</p>]]></content><author><name>David Hersh</name></author><category term="Terrestrial laser scanning" /><category term="LiDAR" /><category term="Point Clouds" /><summary type="html"><![CDATA[Collection of full-waveform terrestrial LiDAR data poses unique challenges during data acquisition and generates data with useful extra attributes. The scanning process and resulting data are described here for a dataset collected at the University of Potsdam Golm campus in August, 2022.]]></summary></entry><entry><title type="html">Point cloud registration: TLS 3-D model of the Golm campus at the University of Potsdam</title><link href="http://localhost:4000/posts/2023/05/TLS-point-cloud-Golm" rel="alternate" type="text/html" title="Point cloud registration: TLS 3-D model of the Golm campus at the University of Potsdam" /><published>2023-05-15T00:00:00+02:00</published><updated>2023-05-15T00:00:00+02:00</updated><id>http://localhost:4000/posts/2023/05/TLS-Registration-Campus-Golm_Luis-Kremer</id><content type="html" xml:base="http://localhost:4000/posts/2023/05/TLS-point-cloud-Golm"><![CDATA[<p>In the last two decades, Terrestrial Laser Scanning (TLS) has gained increasing importance as a ground-based remote sensing technique for measuring three-dimensional (3-D) spaces using Light Detection and Ranging (lidar). But registering and aligning multiple scans has remained a challenge and this article explores different options for a large (n=222 scan positions) dataset.</p>

<h1 id="introduction">Introduction</h1>
<p>In the last two decades, Terrestrial Laser Scanning (TLS) has gained increasing importance as a ground-based remote sensing technique for measuring three-dimensional (3-D) spaces using Light Detection and Ranging (LiDAR). TLS has a wide range of applications, from architecture and engineering to forest inventory for determining biomass, stem volume, or biodiversity (Liang et al., 2016). The fundamental mechanism of many TLS systems is based on measuring the differences in time between the emitted and reflected laser pulses, which usually have wavelengths between 0.5 and 1.5 µm (Pfeifer &amp; Briese, 2007). By using the runtime of a laser pulse, a 3-D representation (x, y, z) of the environment can be generated. Modern scanners are capable of scanning the surroundings with laser pulse repetition rates that exceed 1 MHz, resulting in point clouds that consist of millions of points with millimeter-scale precision.</p>

<p>Unlike airborne laser scanning point clouds, terrestrial laser scanning point clouds consist of individual data acquisitions, which must be merged to obtain full coverage of the study area. However, merging multiple point clouds can be a challenging process that requires precise alignment to avoid errors and ensure features are accurately represented. To prevent misalignment of scans during data acquisition, terrestrial laser scanners are usually equipped with a GNSS antenna for the x, y, and z location of the scanner and an Inertial Measurement Unit (IMU) for measuring the orientation and acceleration of the scanner with respect to the ground (Lillesand et al., 2015). However, this concept is limited when the GNSS signal is weak (e.g., near dense vegetation), the IMU is biased across scans (e.g., due to abrupt movements or long walking distances with the scanner), or the overlap between scans is too small. When these limitations occur together, they can significantly shift the resulting point clouds beyond the limit of the scanner’s onboard alignment capabilities. In cases of misalignment after the data acquisition, post-processing steps must be performed to re-align all individual scan positions. To address the alignment issues of point clouds, the Iterative Closest Point (ICP) algorithm has been established in the last three decades (Besl &amp; McKay, 1992; Chen &amp; Medioni, 1992).</p>

<p>In the summer of 2022, we collected TLS data from the entire Golm campus at the University of Potsdam using the RIEGL VZ-400i terrestrial laser scanner. The objective was to create a precise 3-D model of the campus that can be used as a reference for future point cloud comparison work. Due to time constraints and safety reasons, we were unable to use a Differential GNSS base station for signal corrections as it always requires a line-of-sight connection with the scanner. We obtained over 200 individual scans, but a significant number were unaligned and required post-processing. Therefore, our objective for this project was to systematically align these scans to construct a comprehensive 3-D model of the Golm campus. We aim to highlight potential processing challenges and provide insights on how to avoid them. The three research questions that we address in this project are: 1) which methods can be used to systematically align multiple point clouds from terrestrial laser scans; 2) which is the most suitable variant of the ICP algorithm for aligning the scans?; and 3) how does the sampling pattern impact alignment performance?</p>

<p><em>This internship was supervised by Prof. Dr. Bodo Bookhagen.</em></p>

<p><strong>A web-based view (potree) of the combined TLS data (downsampled to 25 cm) is available <a href="https://up-rs-esp.uni-potsdam.de/golm25cm_ortho_rgb_all_black/golm25_cm_RGB_ortho_all_black.html">here</a></strong></p>

<h1 id="study-area">Study area</h1>
<p>TLS data was collected from the Golm campus at the University of Potsdam from August 22 to August 25, 2022 (Figure 1). A total of 222 scan positions were acquired across the campus, with a balance between short distances to ensure adequate overlap and limiting the number of scans given the large area covered (~0.16 km<sup>2</sup>). The average distance between consecutive scan positions was 33.67 m (Figure 2), although there were variations with a standard deviation of 26.78 m. The large distances (&gt; 75 m) seen in Figure 2 were due to the scanner being relocated to a new position.</p>

<center>
<figure>
<a href="01_ScansPositions.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/01_ScansPositions.png" width="80%" height="80%" /></a><figcaption>Figure 1: Map showing the individual scan positions, with the lines indicating consecutive scans. The lines for the courtyard in the center of the image have been omitted for clarity. </figcaption>
</figure>
</center>

<center>
<figure>
<a href="02_ScanDistances.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/02_ScanDistances.png" /></a><figcaption>Figure 2: Barplot showing the scan-to-scan distances. Each bar in the plot represents the distance between two consecutive scans. For example, a bar at position 1 indicates the distance between scan 01 and scan 02.</figcaption>
</figure>
</center>

<h1 id="validity-of-the-raw-data">Validity of the raw data</h1>
<p>To analyze the validity of the more than 200 individual scans acquired in our project, the degree of noise within scans and the consistency of the initial scan alignment across the entire study area were first examined. This information was critical in determining the next steps in data processing, such as noise removal and registration of scans, which will be discussed in more detail in the following chapters.</p>

<h2 id="noise">Noise</h2>
<p>One of the advantages of TLS is that a high density of points can be captured per scan. In our case, around 10 million points per scan were obtained for most of the scans. However, the high point density can also result in the presence of noisy points in the point cloud, which can pose a challenge in data processing and analysis. One of the main causes of noise in our scans was reflection issues caused by windows. Due to the double bounce effect of the laser reflecting off a window and an object back to the scanner, the object (e.g., trees or building facades) is projected behind the window and may be misplaced in the point cloud after merging multiple scans (Figure 3). To avoid problems with the alignment of overlapping point clouds, these noise artifacts had to be removed.</p>

<center>
<figure>
<a href="03_Noise.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/03_Noise.png" /></a><figcaption>Figure 3: Merged full resolution scans including noise. The long stripes of points demonstrate the susceptibility of TLS data to window reflections of the laser. In the lower and upper middle parts, reflections are projected onto the street. </figcaption>
</figure>
</center>

<h2 id="alignment-consistency">Alignment consistency</h2>
<p>Figure 4 shows the bird’s eye view of all merged individual point clouds immediately after the data collection and thus the starting point of our 3-D model of the Golm campus. Individual point clouds were preprocessed after the acquisition, including subsampling, tree removal, and noise reduction (see methods for more details). Upon initial inspection of the 3-D visualization, it is evident that building facades are stacked, particularly in the central area of the campus. Even without zooming in, it is apparent that some scans were not accurately positioned or aligned with the scanner’s onboard processing platform. This impression is further confirmed by looking at building 27/29 of the campus from a different perspective (Figure 5). Several scans appear to be distorted and protruding into the courtyard.</p>

<center>
<figure>
<a href="04_RawData_Misalignment.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/04_RawData_Misalignment.png" /></a><figcaption>Figure 4: Bird's eye view of all merged individual point clouds after data acquisition. Particularly large errors in the initial alignment are visible in the center of the image (building 27/29). </figcaption>
</figure>
</center>

<center>
<figure>
<a href="05_RawData_Misalignment_Building2729.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/05_RawData_Misalignment_Building2729.png" /></a><figcaption>Figure 5: Full resolution data of building 27/29, showing significant misalignment of scans </figcaption>
</figure>
</center>

<p>As seen above, there were several unaligned scans after the data collection, and manually checking for them was tedious. To automate this process, we calculated the cloud-to-cloud distances between each scan and all the other scans. We achieved this by merging all scans and iteratively removing one scan position for analysis. The cloud-to-cloud distance was then calculated and the scan was merged back with the entire point cloud. This process was repeated for each scan position. Figure 6 displays the results of the cloud-to-cloud distance analysis. The upper plot illustrates the minimum distance for each point in each of the 222 scans to the closest neighboring point in another scan. The lower plot presents the median cloud-to-cloud distance for each scan, taking all points into account.</p>

<p>The upper plot in Figure 6 should ideally show bright pixels along the x-axis, meaning that each point has a close neighbor point in another scan. This is the first indication of good alignment between scans, although it does not guarantee perfect alignment of the entire 3-D model as scans can be systematically shifted. The ground plot should have median cloud-to-cloud distances close to zero. However, large gaps with dark values in the upper plot and bars sticking out in the lower plot (e.g., scan ID 110-118) can be observed, and are indicative of misaligned scans. Setting an arbitrary threshold of 10 cm for cloud-to-cloud misalignment would identify 38 out of 222 scans as misaligned (red dots in Figure 6), while a threshold of 5 cm would already identify 99 unaligned scans.</p>

<center>
<figure>
<a href="06_RawData_cloudtocloud_distance.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/06_RawData_cloudtocloud_distance.png" /></a><figcaption>Figure 6: The upper plot displays the minimum distance between every point in the 222 scans and its closest neighbor in any other scan. The distance values of the points are binned and represented by dark color (few points) and bright color (many points at that distance). The lower plot shows the median cloud-to-cloud distance for each scan, which is calculated by taking the minimum distances of all points in a scan and calculating the median value. </figcaption>
</figure>
</center>

<p>Visual inspection and cloud-to-cloud distance analysis indicate that the source data are unsuitable for direct use. To create a 3-D model of the Golm campus, individual alignment of the scans was required. The next chapter addresses this topic in detail, followed by the methods used for alignment.</p>

<h1 id="point-cloud-registration">Point cloud registration</h1>
<p>As each scan in a TLS campaign has its local coordinate system, accurate registration methods are necessary to create a homogeneous data set in the same coordinate system with seamless overlapping areas of point clouds (Grant et al., 2013). During the registration, algorithms seek a suitable transformation that minimizes the distance between two overlapping point clouds. The output of the registration algorithm is a transformation matrix that represents operations such as translation, rotation, and scaling of the point cloud coordinates.</p>

<p>Significant advances have been made in the development of registration algorithms, and a general workflow consisting of coarse global registration of scans, followed by local refinement, has been established (Zhou et al., 2016). As coarse and fine registration form the fundamental building blocks of this project, an explanation of both concepts will be provided in the following section.</p>

<h2 id="global-registration">Global registration</h2>
<p>Prior to fine registration, global registration is performed, which does not require pre-alignment of point clouds. Several algorithms are available for global registration, of which RANSAC (Random Sample Consensus) is a widely used method (Zhou et al., 2016). The RANSAC algorithm is an iterative process that randomly selects a user-defined number of points in one point cloud and searches for their correspondence in the other point cloud. The transformation is estimated and evaluated based on criteria such as the distance correspondence between the two point clouds (Dung et al., 2013). This process is repeated until convergence. The search for correspondence in the point cloud that needs to be aligned (source point cloud) can require a significant number of iterations. To accelerate this process, Fast Point Feature Histograms (FPFH) are calculated (Rusu et al., 2009). FPFH describe the local geometry of a point and its neighboring points. This descriptor enables the RANSAC algorithm to search for nearest neighbors based on the FPFH, thus speeding up the correspondence search. This global registration method is implemented in the Python package <code class="language-plaintext highlighter-rouge">Open3D</code>.</p>

<p>Besides the automatic global registration mentioned above, global registration can also be performed manually using software such as <code class="language-plaintext highlighter-rouge">CloudCompare</code>. User can select a set of matching points (at least three) in both the target (reference) and source point clouds. Based on this correspondence, the transformation of the source point cloud is calculated.</p>

<h2 id="icp-registration">ICP registration</h2>

<p>Unlike global alignment, fine registration methods require two point clouds that already have a rough correspondence. The Iterative Closest Point (ICP) algorithm is commonly used for local refinement and has two main variants:</p>

<p><strong>Point-to-point ICP</strong></p>

<p>The Iterative Closest Point (ICP) registration method using the point-to-point variant was introduced by Besl and McKay (1992). This iterative algorithm consists of two stages in each iteration:</p>

<ul>
  <li>First, for each point in the source point cloud, it searches for the closest point in the target point cloud.</li>
  <li>Second, it attempts to find a transformation that minimizes the distance between the corresponding points.</li>
</ul>

<p>These two processing stages are repeated until convergence (Salvi et al., 2007).</p>

<p><strong>Point-to-plane ICP</strong></p>

<p>The point-to-plane variant of the ICP algorithm takes into account that scanned objects are not individual points, but surfaces. Unlike point-to-point, which tries to minimize the distance between individual points, the point-to-plane method assumes that points in the source point cloud are generally on a surface of the target point cloud. This accounts for the fact that the source and target point clouds are not necessarily identical.</p>

<p>The general processing procedure of the point-to-plane method is similar to that of the point-to-point approach. However, instead of searching for the closest points between the source and target point clouds, the method calculates the intersection of the point normal in the source point cloud with the tangent plane at its corresponding point in the target point cloud. Then, the algorithm attempts to find a transformation that minimizes the distance between points and the tangent planes at its correspondence points (Low, 2004). This process is repeated until convergence.</p>

<p>Pulli (1999) emphasize the advantages of the point-to-plane method compared to the point-to-point method. Some of the arguments and findings are:</p>

<ul>
  <li>The point-to-plane method converges an order of magnitude faster than the point-to-point method.</li>
  <li>The point-to-point method typically requires at least ten times more point matching and alignment iterations than the point-to-plane method.</li>
  <li>The point-to-plane method is not significantly slowed down by false point pairs since each point can slide along the tangent plane of its corresponding point.</li>
</ul>

<h1 id="methods">Methods</h1>
<p>Due to alignment issues during acquisition, as indicated in the section “alignment consistency” it was not possible to directly merge the raw point clouds from each scan to obtain a 3-D model of the Golm campus. To address the research questions, different alignment approaches and algorithms were applied. Analysis steps, necessary data pre-processing, and evaluation metrics of the results are described below.</p>

<h2 id="analysis">Analysis</h2>
<p>Since fine registration requires pre-aligned point clouds, the scans with a median cloud-to-cloud distance of more than 10 cm were removed (a total of 38 scans; see Figure 6). The RANSAC global registration approach and manual point pair selection were then applied to roughly align the 38 excluded ones to the remaining scans. Two separate alignment approaches were subsequently performed using both variants of the ICP algorithm.</p>

<ol>
  <li>
    <p>For the first approach, a <code class="language-plaintext highlighter-rouge">loop closure method</code> was implemented by starting with the first scan and aligning the second scan to the first scan. The aligned scans were merged, and the iterative process was continued by moving on to the next scan until all 222 scans were aligned and merged. To ensure a systematic approach, scans were aligned in a serpentine pattern, starting in the eastern part of the campus and continuing until the first loop was closed in the western part. Subsequently, all other scans between this serpentine pattern were aligned to the framework of the first aligned scans, resulting in the closure of the loop several times. This approach was repeated for both point-to-point and point-to-plane variants of ICP to compare their performance.</p>
  </li>
  <li>
    <p>In the second approach, an airborne laser scanning (ALS) dataset of the campus was used as the target point cloud, and each TLS scan was aligned as the source point cloud to it. This approach was repeated for both variants of the ICP algorithm (point-to-point and point-to-plane) to compare whether a different target point cloud affected the result.</p>
  </li>
</ol>

<p>The analyses were performed using Python 3.9, utilizing two primary packages: <code class="language-plaintext highlighter-rouge">laspy</code> for reading and writing .laz files, and <code class="language-plaintext highlighter-rouge">open3d</code> for implementing the ICP algorithms. The software <code class="language-plaintext highlighter-rouge">CloudCompare</code> was used for manual global registration.</p>

<h2 id="data-preparation">Data preparation</h2>

<p>Each TLS scan consists of about 10 million data points. Processing such a large amount of data has a significant impact on the computation time. Therefore, each scan was subject to the following pre-processing steps prior to analysis:</p>

<p><strong>1. Subsampling to 10 centimeters</strong></p>

<p>The subsampling of the point clouds was performed using a voxel-grid filter. This approach assigns a 3-D grid of 10 cm spacing voxels to the point cloud, and samples only the closest point to the center of each voxel. This reduces the amount of data and evens out varying point density.</p>

<p><strong>2. Outlier filter</strong></p>

<p>A statistical filter was applied to identify outliers in the point cloud. For each point, the mean distance to its <code class="language-plaintext highlighter-rouge">n</code> nearest neighbors was computed, and the overall standard deviation was determined. Points were classified as outliers if the mean distance to their <code class="language-plaintext highlighter-rouge">n</code> nearest neighbors exceeded the distance threshold of <code class="language-plaintext highlighter-rouge">m</code> multiplied by the overall standard deviation. For this analysis <code class="language-plaintext highlighter-rouge">n</code> was set to 12, and <code class="language-plaintext highlighter-rouge">m</code> to 3.</p>

<p>Additionally, significant noise artifacts (Figure 3) that remained after the statistical outlier filter were manually removed.</p>

<p><strong>3. Remove non-planar features</strong></p>

<p>The Golm campus contains both planar objects such as building facades and streets, and non-planar objects like trees. Trees can have slightly varying shapes across scans due to the motion of the leaves, which potentially affects the accuracy of the ICP alignment. To mitigate this issue, non-planar objects were filtered out for the analysis by exploiting the calculation of geometric features. Geometric features, such as <code class="language-plaintext highlighter-rouge">planarity</code>, describe the shape of a point based on its local neighborhood. For example, a point in a tree crown would have a low <code class="language-plaintext highlighter-rouge">planarity</code> value, while a point on a street would have a high value.</p>

<p>The computation of the geometric feature <code class="language-plaintext highlighter-rouge">planarity</code> can be summarized as described by Dittrich et al. (2017):</p>

<ol>
  <li>The 3D covariance matrix based on the point’s neighborhood is calculated using a fixed radius (e.g., 1 m)</li>
  <li>The eigenvectors (\(e_{1}\), \(e_{2}\), \(e_{3}\)) and eigenvalues (\(\lambda_{1}\), \(\lambda_{2}\), \(\lambda_{3}\)) of the 3D covariance matrix are calculated</li>
  <li><code class="language-plaintext highlighter-rouge">Planarity</code> is determined considering the three eigenvalues (\(\lambda_{1}\), \(\lambda_{2}\), \(\lambda_{3}\)): on a planar surface eigenvalues \(\lambda_{1}\) and \(\lambda_{2}\) are larger than \(\lambda_{3}\). Therefore, <code class="language-plaintext highlighter-rouge">planarity</code> (\(P_{\lambda}\)) is calculated based on the following ratio:
\(P_{\lambda} = \frac{\lambda_{2} - \lambda_{3}}{\lambda_{1}}\)</li>
</ol>

<p>For the analysis, a <code class="language-plaintext highlighter-rouge">planarity</code> threshold of 0.7 was selected. The value was arbitrarily chosen but led to reasonable filtering results as shown in Figure 7. As an example, Figure 7a displays the original point cloud data, while Figure 7b shows the results of the data after the data preparation stages.</p>

<center>
<figure>
<a href="07_TLS_processing_before_after.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/07_TLS_processing_before_after.png" /></a><figcaption>Figure 7: Example of data preparation. Figure A displays the original point cloud data with all features included, while Figure B shows the subsampled and non-planarity filtered point cloud data. </figcaption>
</figure>
</center>

<h2 id="evaluation-metrics">Evaluation metrics</h2>
<p>Three metrics were used in this project to evaluate the quality of the alignment for both ICP variants and alignment methods.</p>

<p><strong>Surface roughness</strong></p>

<p>Surface roughness was utilized for visual inspection rather than quantitative measurement. Surface roughness is calculated as the distance between a point and the best-fitting plane determined by its nearest neighbors. If the scans overlap perfectly on a smooth surface, such as a road, the surface roughness is minimal. Conversely, the surface roughness values increase as the alignment deteriorates.</p>

<p><strong>Median cloud-to-cloud distance</strong></p>

<p>The median cloud-to-cloud distance was determined by merging all scans and then removing one scan position at a time for analysis. For each point in the removed scan, the distance to its nearest neighbor in the remaining scans was calculated. The median distance was calculated using all the distances. The removed scan was then merged back, and the same procedure was repeated for the next scan position. The median cloud-to-cloud distance is less affected by outliers in the point clouds.</p>

<p><strong>dRMS</strong></p>

<p>A slightly modified version of the dRMS used by Li et al. (2020) was utilized to give more weight to the misaligned and outlier points. The dRMS provides a measure of the average distance between corresponding points in two aligned point clouds. The smaller the dRMS, the higher the accuracy of the alignment. For a source point cloud A and a target (reference) point cloud B, the dRMS can be expressed as:</p>

\[dRMS(A,B)=\sqrt{\frac{1}{n}\sum_{i=1}^{n}\left ( \left \| a_{i}-b_{j} \right \| \right )^2}\]

<p>where:</p>

<ul>
  <li>\(a_{i}\) is the point in the source point cloud corresponding to \(b_{j}\) in the target point cloud \(n\) the number of points in \(A\)</li>
</ul>

<h1 id="results">Results</h1>
<h2 id="global-alignment">Global alignment</h2>
<p>The automated approach using the RANSAC algorithm supported by FPFH was initially attempted for alignment but was later discarded due to significant runtime issues and the inability to achieve convergence. Especially in the part of Figure 5, the algorithm was not able to come up with a satisfactory solution for the fine registration. Therefore, the manual approach was applied, which involved selecting corresponding point pairs in the target and source point clouds, resulting in successful alignment, as shown in Figure 8. However, some parts of the campus were still misaligned, indicated by sudden changes in surface roughness (Figure 8).</p>

<center>
<figure>
<a href="08_Building2729_surfaceroughness.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/08_Building2729_surfaceroughness.png" /></a><figcaption>Figure 8: Building 27/29 after global alignment, showing well-represented building walls. However, surface roughness exceeding 20 cm (yellow areas) reveals misalignment of scans in the z-direction.  </figcaption>
</figure>
</center>

<h2 id="fine-registration">Fine registration</h2>

<p><strong>Loop closure approach</strong></p>

<p>Overall, the point-to-point ICP variant using the loop closure approach achieved a good alignment result. Figure 9a indicates that only three scans remained above the 10 centimeter threshold for the median cloud-to-cloud distance threshold. Additionally, after visual inspection using surface roughness, it was observed that two of these scans were not misaligned but rather had poor overlap, resulting in an overall increase in cloud-to-cloud distance. Therefore, almost all scans were correctly aligned using this method, with a majority having below 5 cm cloud-to-cloud distance.</p>

<p>On the other hand, the point-to-plane ICP variant’s alignment performance was overall poor, with 87 scans having a cloud-to-cloud distance of more than 10 centimeters (Figure 9b). This method even deteriorated the initial alignment of the raw data (Figure 6).</p>

<center>
<figure>
<a href="09_LoopClosure_Barplot.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/09_LoopClosure_Barplot.png" /></a><figcaption>Figure 9: Comparison of results of the loop closure approach: (A) Point-to-point variant leads to an overall good alignment result. The bars sticking out are aligned scans, however, with poor overlap and thus increased value; (B) In contrast, the point-to-plane variant showed significant misalignment for many scans, with over 80 scans having a median cloud-to-cloud distance greater than 10 cm. </figcaption>
</figure>
</center>

<p><strong>ALS reference approach</strong></p>

<p>In the case of using the ALS data set as the reference point cloud, the point-to-point ICP variant had a slightly worse alignment performance compared to the loop closure approach, as depicted in Figure 10a. Despite this, the point-to-point approach still resulted in an improvement over the alignment of the raw data, with 18 scans having a median cloud-to-cloud distance greater than 10 centimeters. The scans displayed good alignment agreement along the z-direction but showed deficient alignment in the x- and y-directions, particularly along building walls.</p>

<p>The point-to-plane ICP variant performed better compared to the loop closure approach, with 28 scans exhibiting a median cloud-to-cloud distance greater than 10 centimeters (Figure 10b). However, the point-to-plane variant still underperformed in comparison to the point-to-point variant.</p>

<center>
<figure>
<a href="10_ALSReference_Barplot.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/10_ALSReference_Barplot.png" /></a><figcaption>Figure 10: Comparison of results of the ALS reference approach: (A) Point-to-point variant shows misalignment only for individual scans, particularly scan 180-200 where building walls are poorly represented; (B) Point-to-plane variant shows misalignment for more scans, with five scans having a median cloud-to-cloud distance greater than 1 m. </figcaption>
</figure>
</center>

<p><strong>Overall performance and final 3-D model</strong></p>

<p>The overall performance of the two ICP variants and the two alignment approaches are shown in the table below. The evaluation metrics, including the dRMS and median cloud-to-cloud distance, suggest that the point-to-point ICP variant using the loop closure approach performed best in this analysis. Additionally, the point-to-point ALS reference approach also demonstrated reasonable results. Conversely, the point-to-plane variant performed worse in both metrics, indicating that it negatively impacted the alignment of the raw data.</p>

<table>
  <thead>
    <tr>
      <th>Method</th>
      <th>dRMS</th>
      <th>Median cloud-to-cloud distance</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Raw data</td>
      <td>0.633</td>
      <td>0.157</td>
    </tr>
    <tr>
      <td>Loop closure: Point-to-Point</td>
      <td><strong>0.433</strong></td>
      <td><strong>0.038</strong></td>
    </tr>
    <tr>
      <td>Loop closure: Point-to-Plane</td>
      <td>0.767</td>
      <td>0.207</td>
    </tr>
    <tr>
      <td>ALS reference: Point-to-Point</td>
      <td>0.434</td>
      <td>0.059</td>
    </tr>
    <tr>
      <td>ALS reference: Point-to-Plane</td>
      <td>0.859</td>
      <td>0.460</td>
    </tr>
  </tbody>
</table>

<p>The final 3-D model of the Golm campus was created using the registered data from the best alignment result achieved through the point-to-point loop closure approach (Figure 11).</p>

<center>
<figure>
<a href="11_final_viz_golm.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/11_final_viz_golm.png" /></a><figcaption>Figure 11: Final 3-D model of the Golm campus at the University of Potsdam </figcaption>
</figure>
</center>

<h1 id="discussion-and-conclusion">Discussion and conclusion</h1>
<h2 id="loop-closure-vs-als-reference-approach">Loop closure vs. ALS reference approach</h2>

<p>The loop closure approach has been shown to achieve accurate results in the x- and y-directions but reveals issues in the z-direction. This can be attributed to slight alignment errors that propagate through the point cloud. These errors become evident when closing the loops and are even larger when the overlap between the scans is not given.</p>

<p>On the other hand, the ALS reference approach performs well in the z-direction due to the high overlap of ALS and TLS point clouds. However, this method shows lower accuracy in the x- and y-directions, which is likely due to the fact that the ALS dataset does not sufficiently represent the facades of the buildings.</p>

<p>For future work, a combination of the loop closure and ALS reference approaches could be beneficial to improve the overall alignment. The loop closure approach could be used first to align the point clouds in the x- and y-directions, followed by the ALS approach to refine the alignment only in the z-direction.</p>

<h2 id="point-to-point-vs-point-to-plane-icp">Point-to-point vs. point-to-plane ICP</h2>
<p>Given the conditions of this project, the point-to-point ICP algorithm outperformed the point-to-plane ICP algorithm and resulted in an overall improvement of the registered data compared to the raw data. Considering the significant number of scans that were misaligned after the data acquisition, the point-to-point algorithm was able to refine the registration with an accuracy of centimeters to millimeters. The removal of non-planar and slightly moving features, such as leaves or branches, proved to be beneficial for the algorithm. This is because the point-to-point variant of the ICP algorithm aims to find identical points in two different point clouds, making it more effective to use only man-made features that are consistent across the scans.</p>

<p>While the point-to-point variant yielded superior results, the point-to-plane variant underperformed and even deteriorated the raw data alignment. This result was unexpected given that the point-to-plane variant is commonly regarded as more realistic due to the challenge of identifying identical points in two distinct point clouds. The inaccurate outcomes of the point-to-plane algorithm can be attributed to two main factors:</p>

<ol>
  <li>
    <p>Restricting the point clouds to planar surfaces was effective for the point-to-point variant but may not have been optimal for the point-to-plane variant. The campus environment, consisting of buildings with large walls lacking distinctive features, posed a challenge for the point-to-plane variant. As a result, the algorithm had difficulty in finding matching features, resulting in the slipping of two large walls.</p>
  </li>
  <li>
    <p>The algorithm randomly oriented the point normals of the point clouds. Figure 12 shows an example of a point cloud with three walls and their corresponding point normals. Ideally, all point normals should face the direction of the scanner. However, since the scanner’s location was not specified, the point normals were randomly oriented in both possible directions. Consequently, the algorithm projected the point normals onto the planes of non-corresponding points, causing significant shifts in the x- and y-directions, as observed in our analysis.</p>
  </li>
</ol>

<center>
<figure>
<a href="12_Normal_orientation.png"><img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/TLS_Golm_images/12_Normal_orientation.png" /></a><figcaption>Figure 12: Figure A is showing a point cloud with three walls and their computed normals in Figure B </figcaption>
</figure>
</center>

<h2 id="impact-of-the-tls-sampling-pattern-on-alignment-performance">Impact of the TLS sampling pattern on alignment performance</h2>
<p>The RIEGL VZ-400i and other similar scanners are equipped with onboard alignment capabilities, which can eliminate the need for post-processing alignment if a suitable sampling pattern is employed. However, as mentioned in the introduction, the scanner’s alignment capabilities are limited when either the GNSS or IMU system is not performing as expected, often due to factors such as weak signal strength, large distances between scans, or the absence of re-initialization at the last scan position after the scanner is turned off. The combined effect of these factors can lead to the alignment issues encountered after the data acquisition in this project.</p>

<p>While this project demonstrated that misalignment issues in a large data set with over 200 scans can be resolved through global alignment and refined alignment using ICP, these processing steps can be avoided or simplified with consistent TLS sampling. Two crucial aspects to consider are short distances (10-20 m) between scans and a consistent sampling pattern in which each scan has a follow-up scan and no location is scanned more than once. Short distance ensures adequate overlap of point clouds from different scans, enabling matching of the same objects during alignment. A consistent scan pattern is essential for proper alignment of subsequent scans, as jumping back and forth between scans can disrupt onboard alignment and potentially lead to misalignment. Keeping these considerations in mind when conducting TLS scanning can help streamline the alignment process and ensure obtaining an accurate 3-D model.</p>

<h1 id="important-takeaways-of-the-tls-project">Important takeaways of the TLS project</h1>
<ul>
  <li>A consistent TLS sampling pattern with each scan having at least two close-by neighboring scans should be followed to ensure reliable on-board registration for the RIEGL VZ-400i scanner</li>
  <li>Short distances (10-20 m) between scans to provide enough overlap can facilitate point cloud registration tasks</li>
  <li>The combination of global and fine registration through the ICP algorithm is able of addressing large misalignments even between scans with little overlap</li>
  <li>The performance of the point-to-point ICP variant benefits from removing non-planar features as handling slightly moving objects across the scans can be challenging</li>
  <li>The implementation of the point-to-plane ICP variant requires careful consideration (e.g., properly oriented point normals) but has the potential to further improve the results of this project</li>
  <li>The combination of the loop-closure approach using TLS scans for x and y alignment and ALS data for z alignment shows promise for the large area covered in this project</li>
</ul>

<h1 id="references">References</h1>

<p>Besl, P. J., &amp; McKay, N. D. (1992). Method for registration of 3-d shapes. Sensor fusion IV: control paradigms and data structures, 1611, 586–606.</p>

<p>Dittrich, A., Weinmann, M., &amp; Hinz, S. (2017). Analytical and numerical investigations on the accuracy and robustness of geometric features extracted from 3d point cloud data. ISPRS journal of photogrammetry and remote sensing, 126, 195–208.</p>

<p>Dung, L. -R., Huang, C.-M., Wu, Y. -Y., et al. (2013). Implementation of ransac algorithm for feature-based image registration. J. Comput. Commun, 1(6), 46–50.</p>

<p>Grant, D., Bethel, J., &amp; Crawford, M. (2013). Comparative study of two automatic registration algorithms. ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences, 2, 91–95.</p>

<p>Li, P., Wang, R., Wang, Y., &amp; Tao, W. (2020). Evaluation of the icp algorithm in 3d point cloud registration. IEEE Access, 8, 68030–68048.</p>

<p>Liang, X., Kankare, V., Hyyppä, J., Wang, Y., Kukko, A., Haggrén, H., Yu, X., Kaartinen,
H., Jaakkola, A., Guan, F., et al. (2016). Terrestrial laser scanning in forest
inventories. ISPRS Journal of Photogrammetry and Remote Sensing, 115, 63–77.</p>

<p>Lillesand, T., Kiefer, R. W., &amp; Chipman, J. (2015). Remote sensing and image interpretation. John Wiley &amp; Sons.</p>

<p>Low, K.-L. (2004). Linear least-squares optimization for point-to-plane icp surface registration. Chapel Hill, University of North Carolina, 4(10), 1–3.</p>

<p>Pfeifer, N., &amp; Briese, C. (2007). Geometrical aspects of airborne laser scanning and ter-
restrial laser scanning. International Archives of Photogrammetry, Remote Sensing
and Spatial Information Sciences, 36(3/W52), 311–319.</p>

<p>Pulli, K. (1999). Multiview registration for large data sets. Second international conference on 3-d digital imaging and modeling (cat. no. pr00062), 160–168.</p>

<p>Rusu, R. B., Blodow, N., &amp; Beetz, M. (2009). Fast point feature histograms (fpfh) for 3d registration. 2009 IEEE international conference on robotics and automation, 3212–3217.</p>

<p>Salvi, J., Matabosch, C., Fofi, D., &amp; Forest, J. (2007). A review of recent range image registration methods with accuracy evaluation. Image and Vision computing, 25(5), 578–596.</p>

<p>Zhou, Q. -Y., Park, J., &amp; Koltun, V. (2016). Fast global registration. Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, 766–782.</p>]]></content><author><name>Luis Kremer</name></author><category term="TLS" /><category term="LiDAR" /><category term="3-D campus model" /><category term="point cloud registration" /><category term="ICP" /><summary type="html"><![CDATA[In the last two decades, Terrestrial Laser Scanning (TLS) has gained increasing importance as a ground-based remote sensing technique for measuring three-dimensional (3-D) spaces using Light Detection and Ranging (lidar). But registering and aligning multiple scans has remained a challenge and this article explores different options for a large (n=222 scan positions) dataset.]]></summary></entry><entry><title type="html">Understanding weight maps and label manipulation in tree detection from high-resolution orthophotos with U-Net</title><link href="http://localhost:4000/posts/2023/05/weight-maps-label-manipulation-tree-detection-unet" rel="alternate" type="text/html" title="Understanding weight maps and label manipulation in tree detection from high-resolution orthophotos with U-Net" /><published>2023-05-01T00:00:00+02:00</published><updated>2023-05-01T00:00:00+02:00</updated><id>http://localhost:4000/posts/2023/05/Tree-Segmentation-UNet_Daniel-Lusk</id><content type="html" xml:base="http://localhost:4000/posts/2023/05/weight-maps-label-manipulation-tree-detection-unet"><![CDATA[<p>Fine individiual tree crown delineation can be achieved from RGB + NIR orthophotos using CNN-based semantic segmentation with weighted losses, but how do different weight maps perform, and how does manipulation of training label sets affect this?</p>

<h1 id="introduction">Introduction</h1>

<p>Convolutional neural networks (CNN) have been used in vegetation remote sensing for years, and are an especially popular choice for image classification tasks <sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup>\(^,\)<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>. Fully convolutional neural networks (FCNN) such as U-Net, in particular, are currently considered best-in-class when it comes to image segmentation as their output is of the same resolution as their inputs, providing pixel-by-pixel classification, and have seen growing popularity in remote sensing image segmentation <sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>\(^,\)<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup>\(^,\)<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>\(^,\)<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. Not only are FCNNs like U-Net effective at semantic segmentation—the process of classifying an image pixel-by-pixel, but not identifying distinct objects—others, such as Mask R-CNN, are capable of performing instance segmentation—the classification of not only pixels but also of objects (Figure 1) <sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup>\(^,\)<sup id="fnref:8"><a href="#fn:8" class="footnote" rel="footnote" role="doc-noteref">8</a></sup>\(^,\)<sup id="fnref:9"><a href="#fn:9" class="footnote" rel="footnote" role="doc-noteref">9</a></sup>. However, while instance segmentation architectures can be powerful for tasks like tree detection, their architectures are often deep and multi-faceted and can require the optimization of a multitude of different hyperparameters. U-Net, on the other hand, is lightweight and simple to implement, with few hyperparameters, and is therefore an appealing option for researchers wishing to perform image segmentation while minimizing model tuning.</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/semantic-vs-instance.png" />
  <figcaption><b>Figure 1.</b> A comparison of semantic segmentation (e.g. the output of architectures like U-Net) and instance segmentation.</figcaption>
</figure>

<p>The problem remains, however, that U-Net can only perform semantic segmentation, i.e. it cannot provide object detection, and so additional methods are needed to process the semantically classified output into individual objects. Weight maps, for example, can be used to focus special attention on the boundaries between objects in order to train the model to be more conservative with class predictions in those regions, as has been done to achieve cell segmentation in medical images<sup id="fnref:3:1"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>. The semantic output can then be segmented algorithmically to identify individual objects. This same approach has been applied successfully by Brandt, <em>et al</em>. (2020) and Mugabowindekwe <em>et al</em>. (2022) to remote sensing imagery for tree detection <sup id="fnref:5:1"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>\(^,\)<sup id="fnref:6:1"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. To achieve this, the authors utilized weight maps corresponding to the boundaries of tree crowns during training to reduce the connectivity of neighboring trees in the semantic output and then performed several morphological operations on the resulting semantic output, such as circle fitting and region growing to recover the “missing” pixels. This ensemble process (training with weight maps + morphological post-processing) can prove difficult with imagery of dense forests in which tree crowns border each other on all sides, however, and deeper understanding of the influence of training schemes and post-processing methods may be useful in improving tree detection from optical remote sensing imagery.</p>

<p>In this exploration, aerial orthophotos of the city of Berlin, Germany are used in the training of seven U-Net-based models. We seek to answer three questions: i) which weight map types should be used; ii) what is the effect of training a model on eroded (shrunken) labels compared to unmodified labels; and iii) which post-processing operations are most effective in isolating individual trees from semantic output.</p>

<h1 id="methods">Methods</h1>
<h2 id="study-site-and-data-acquisition">Study site and data acquisition</h2>
<p>True aerial orthophotos of Berlin were acquired by the Berlin Office of Cartography and Geodesy in daylight hours in the summer of 2020 with a spatial resolution of 0.2 m and a positional accuracy of +/- 0.4 m <sup id="fnref:10"><a href="#fn:10" class="footnote" rel="footnote" role="doc-noteref">10</a></sup>\(^,\)<sup id="fnref:11"><a href="#fn:11" class="footnote" rel="footnote" role="doc-noteref">11</a></sup>. The data used for this study consists of red, green, blue (RGB), and near-infrared (NIR) bands, and were sourced from the TrueDOP20RGB and TrueDOP20CIR datasets available for download on Geoportal Berlin (FIS-Broker). Overall, four 1-km<sup>2</sup> tiles and one 0.5-km<sup>2</sup> tile were obtained from Geoportal Berlin for a total extent of 4.5-km<sup>2</sup> (Figure 2).</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/overview.png" />
  <figcaption><b>Figure 2.</b> Overview of study area (Berlin, Germany). Training tiles are outlined in blue, and the test/validation tile is outlined in green.</figcaption>
</figure>

<p>To generate canopy height maps (CHM) for semi-automated label generation prior to the training of the U-Net models, LiDAR point clouds acquired using airborne laser scanning (ALS) were also collected from Geoportal Berlin, with the same respective extents as the four 1-km<sup>2</sup> orthophoto tiles <sup id="fnref:12"><a href="#fn:12" class="footnote" rel="footnote" role="doc-noteref">12</a></sup>. The point clouds were pre-classified with the following classes: soil (class 2), low (class 3), medium (class 4), and high vegetation (class 5), outliers (class low points class 7), and default (class 0).</p>

<h2 id="data-preprocessing">Data preprocessing</h2>

<p><em>Normalization and NDVI</em></p>

<p>All band values for RGB and NIR were scaled to 0-1, and NDVI was calculated for all images using the formula \(\frac{NIR - Red}{NIR + Red}\). NDVI values were then also normalized to 0-1 to ensure consistency of data ranges.</p>

<h2 id="label-generation">Label generation</h2>

<p><em>Training label set</em></p>

<p>Tree labels for model training were generated for the four 1-km<sup>2</sup> tiles using in a “semi-automated” fashion. First, vegetation points were reclassified by filtering out all last-return points, as these points are most likely to be hard surfaces such as ground or buildings. From the remaining point cloud, points were further filtered out that did not fall within a maximum density threshold as informed by a k-d tree. Next, to reclaim points that had been inside vegetation but had been filtered about by the above steps, smaller point neighborhoods were again generated with the use of a k-d tree and points that fell within a now smaller threshold were reclaimed as vegetation points. After vegetation points had successfully been isolated, CHMs were generated. Tree labels were finally generated by identifying local maxima “islands” across the CHMs, applying watershed segmentation from the Python library scikit-image to the CHMs with the local maxima as markers, and filtering the resulting labels by eccentricity and total label area <sup id="fnref:13"><a href="#fn:13" class="footnote" rel="footnote" role="doc-noteref">13</a></sup>. These labels are referred to as “ORIG”.</p>

<p>It should be noted that the resulting labels, while benefiting from the advantage of being able to be generated in a matter of minutes compared to the many hours and days it would take for hand-drawn annotation at the same scale, are of lower quality than their hand-drawn counterparts, as tree shapes can be somewhat unnatural (blocky instead of smooth) and at times contained non-tree pixels.</p>

<p><em>Validation and test label sets</em></p>

<p>To ensure the model predictions were validated on a more reliable label set, the final 0.5-km<sup>2</sup> tile was hand-annotated using napari, a multi-dimensional image viewer for Python <sup id="fnref:14"><a href="#fn:14" class="footnote" rel="footnote" role="doc-noteref">14</a></sup>. Due to the limited hand-labeled dataset size, these labels were used for both validation and final model testing in lieu of additional high-quality labeled datasets (Figure 3).</p>

<p>In total, the training label set consisted of 7,359 trees, and the validation/test label set contained 1,193 trees.</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/hand-vs-auto-labels.png" />
  <figcaption><b>Figure 3.</b> Examples of semi-automated training labels and hand-drawn validation/test labels. Note the blockiness of the training labels, as well as the small segment of building classified as a tree.</figcaption>
</figure>

<p><em>Label erosion</em></p>

<p>To explore the effect of training the models on labels that did not include the tree canopy edges (as opposed to discouraging the learning of the borders using weighting schemes), eroded training and validation sets were generated from the semi-automated label sets. These labels are referred to as “ERODED”. Label erosion was performed with a 1x1 kernel using scikit-image’s simple morphological erosion method (Figure 4) <sup id="fnref:13:1"><a href="#fn:13" class="footnote" rel="footnote" role="doc-noteref">13</a></sup>.</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/orig-vs-eroded-labels.png" />
  <figcaption><b>Figure 4.</b> Non-eroded (ORIG) and eroded (ERODED) label sets.</figcaption>
</figure>

<h2 id="weight-maps">Weight maps</h2>

<p>Four weighting schemes were used: i) Ronneberger weight maps (RONN) as described in Ronneberger et al., (2015) <sup id="fnref:3:2"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup>, computed as</p>

\[\begin{aligned}
w(\mathbf{x}) = w_{c}(\mathbf{x}) + w_0 \cdot \exp \left(−\frac{ (d_1(\mathbf{x})+d_2(\mathbf{x}))^2}{2 \sigma^2}\right)
\end{aligned}\]

<p>and characterized by the highest weights occurring at touching (or almost touching) tree-to-tree borders with a rapid decay as tree-to-tree distances increase; ii) modified Ronneberger weight maps (BOUNDS10) in which weights &gt;= 3 are assigned a value of 10 and weights &lt; 1 are assigned a value of 0; iii) border weight maps (BORD10) in which the inner edge pixels of each label are set to 10 and all others to 0; and iv) no weights (ALL1) in which all “weights” are set to 1 (Figure 5).</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">calculate_ronneberger_weights</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">wc</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">w0</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    <span class="s">"""
    Generate weight maps as specified in the U-Net paper
    for boolean mask.
    
    "U-Net: Convolutional Networks for Biomedical Image Segmentation"
    https://arxiv.org/pdf/1505.04597.pdf
    
    Parameters
    ----------
    y: Numpy array
        2D array of shape (image_height, image_width) representing boolean (or binary)
        mask of objects.
    wc: dict
        Dictionary of weight classes.
    w0: int
        Border weight parameter.
    sigma: int
        Border width parameter.
    Returns
    -------
    Numpy array
        Training weights. A 2D array of shape (image_height, image_width).
    """</span>
    
    <span class="c1"># Check if mask is boolean or binary mask
</span>    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">label</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
        
    <span class="n">no_labels</span> <span class="o">=</span> <span class="n">labels</span> <span class="o">==</span> <span class="mi">0</span>
    <span class="n">label_ids</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">unique</span><span class="p">(</span><span class="n">labels</span><span class="p">))[</span><span class="mi">1</span><span class="p">:]</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_ids</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">labels</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="nb">len</span><span class="p">(</span><span class="n">label_ids</span><span class="p">)))</span>

        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">label_id</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">label_ids</span><span class="p">):</span>
            <span class="n">distances</span><span class="p">[:,:,</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">distance_transform_edt</span><span class="p">(</span><span class="n">labels</span> <span class="o">!=</span> <span class="n">label_id</span><span class="p">)</span>

        <span class="n">distances</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sort</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
        <span class="n">d1</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[:,:,</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">d2</span> <span class="o">=</span> <span class="n">distances</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">w0</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">/</span><span class="mi">2</span><span class="o">*</span><span class="p">((</span><span class="n">d1</span> <span class="o">+</span> <span class="n">d2</span><span class="p">)</span> <span class="o">/</span> <span class="n">sigma</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">no_labels</span>
        
        <span class="k">if</span> <span class="n">wc</span><span class="p">:</span>
            <span class="n">class_weights</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">wc</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
                <span class="n">class_weights</span><span class="p">[</span><span class="n">labels</span> <span class="o">==</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
            <span class="n">w</span> <span class="o">=</span> <span class="n">w</span> <span class="o">+</span> <span class="n">class_weights</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">w</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">w</span>
</code></pre></div></div>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/weight_maps.png" />
  <figcaption><b>Figure 5.</b>  Comparison of weight map types. Labels have been adjusted to appear darker than the background solely for visualization purposes, and during training labels + background were set to 1. Note that the borders in BORD10 are all 10, though they may appear continuous on some displays.</figcaption>
</figure>

<h2 id="deep-learning-model-training">Deep learning model training</h2>

<p><em>CNN architecture</em></p>

<p>For the task of tree identification, a U-Net architecture was used with a slightly-modified structure compared to the Ronneberger, et al. structure, and batch normalization was applied before each encoding operator (downsampling) and after each decoding operator (upsampling).</p>

<p><em>Loss function and weights</em></p>

<p>Because a core focus of this exploration is the effect of different weight maps on prediction quality, and weight maps directly modify the losses calculated after each forward pass of the CNN, choosing an appropriate loss function can greatly affect the model’s performance. When choosing a loss function, it is important to consider whether or not there is a class imbalance in the dataset. In the Berlin trees dataset only ~28% of the pixels represent trees, and therefore can be considered an imbalanced dataset. For instances such as this where the negative class abundance significantly outweighs that of the positive class, traditional accuracy-oriented loss functions become less useful, and it is preferable to use loss functions that prioritize measuring the overlap, or intersection-over-union (IoU), of the predictions with the true values <sup id="fnref:15"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">15</a></sup>. In this case, the Tversky loss function was selected as the primary metric for training as it is similar to other popular loss functions for image segmentation such as Dice or Jaccard losses, but can be adjusted according to desired outcomes <sup id="fnref:16"><a href="#fn:16" class="footnote" rel="footnote" role="doc-noteref">16</a></sup>.</p>

<p>With the Tversky loss function, specificity (proportion of false positives [FP]) and sensitivity (proportion of false negatives [FN]) can be weighted by alpha (\(\alpha\)) and beta (\(\beta\)) values, respectively, with the requirement that the sum of the two values be equal to 1. Contrary to previous approaches where FNs were weighted higher than FPs as high sensitivity is typically preferred for imbalanced datasets, the \(\alpha\) and \(\beta\) weights were set to 0.6 and 0.4, respectively, in order to emphasize FPs <sup id="fnref:17"><a href="#fn:17" class="footnote" rel="footnote" role="doc-noteref">17</a></sup>. The hypothesis here is that, as the borders of tree canopies are the focus, if the model is encouraged to be slightly more conservative when predicting a pixel is part of a tree, then the model will be less likely to predict tree crown borders with high confidence, as they are most likely to be confused with the background.</p>

<p>To further de-incentivize the models to classify tree crown borders as trees, all prediction groups used in the Tversky loss calculation (TPs, FPs, and FNs), are multiplied by the weight maps, increasing the influence of the predictions at the tree crown boundaries or borders on the overall loss calculation.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">tversky</span><span class="p">(</span><span class="n">y_true</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.6</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mf">0.4</span><span class="p">):</span>
  <span class="s">"""Calculate the Tversky loss for imbalanced classes

  Args:
      y_true (tensor): Array of the ground truth data of size (m * n * 2) where the last axis is labels + weights
      y_pred (tensor): Array containing pixelwise predictions as logits
      alpha (float, optional): Weight of false positives. Defaults to 0.6.
      beta (float, optional): Weight of false negatives. Defaults to 0.4.

  Returns:
      float: Loss
  """</span>
  <span class="c1"># Labels
</span>  <span class="n">y_t</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_true</span><span class="p">[...,</span> <span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

  <span class="c1"># Weights
</span>  <span class="n">y_weights</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">y_true</span><span class="p">[...,</span> <span class="mi">1</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">ones</span> <span class="o">=</span> <span class="mi">1</span>
  <span class="n">p0</span> <span class="o">=</span> <span class="n">y_pred</span>  <span class="c1"># Probability that pixels are class i
</span>  <span class="n">p1</span> <span class="o">=</span> <span class="n">ones</span> <span class="o">-</span> <span class="n">y_pred</span>  <span class="c1"># Probability that pixels are not class i
</span>  <span class="n">g0</span> <span class="o">=</span> <span class="n">y_t</span>  <span class="c1"># Ground truth
</span>  <span class="n">g1</span> <span class="o">=</span> <span class="n">ones</span> <span class="o">-</span> <span class="n">y_t</span>

  <span class="n">tp</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_weights</span> <span class="o">*</span> <span class="n">p0</span> <span class="o">*</span> <span class="n">g0</span><span class="p">)</span>
  <span class="n">fp</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_weights</span> <span class="o">*</span> <span class="n">p0</span> <span class="o">*</span> <span class="n">g1</span><span class="p">)</span>
  <span class="n">fn</span> <span class="o">=</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">y_weights</span> <span class="o">*</span> <span class="n">p1</span> <span class="o">*</span> <span class="n">g0</span><span class="p">)</span>

  <span class="n">EPSILON</span> <span class="o">=</span> <span class="mf">0.00001</span>
  <span class="n">numerator</span> <span class="o">=</span> <span class="n">tp</span>
  <span class="n">denominator</span> <span class="o">=</span> <span class="n">tp</span> <span class="o">+</span> <span class="n">fp</span> <span class="o">+</span> <span class="n">fn</span> <span class="o">+</span> <span class="n">EPSILON</span>
  <span class="n">score</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

  <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">tf</span><span class="p">.</span><span class="n">reduce_mean</span><span class="p">(</span><span class="n">score</span><span class="p">)</span>
</code></pre></div></div>

<p><em>Model training</em></p>

<p>The input size for the U-Net models was set to 256x256 pixels, and the ratio of training to validation patches was approximately 9 to 1. In addition to the border-weighted Tversky loss function described above, the learning rate was dynamically optimized during training using the ADADELTA optimizer <sup id="fnref:18"><a href="#fn:18" class="footnote" rel="footnote" role="doc-noteref">18</a></sup>. Each model was trained for 200 epochs with a batch size of 16.</p>

<p>In total, seven models were trained using different combinations of training label type (ORIG, ERODED) and weight map (RONN, BOUNDS10, BORD10, ALL1). It should be noted that RONN + ERODED training was not performed due to initial results clearly suggesting that the other three weight maps significantly outperformed RONN training, which is why only seven models will be presented below.</p>

<h2 id="tree-segmentation-from-deep-learning-model-predictions">Tree segmentation from deep learning model predictions</h2>

<p>For each trained model, predictions were performed on the full set of test data containing 1,193 trees over a 0.5-km<sup>2</sup> section of Berlin. The two-class output (0 or 1; “tree” or “not tree”) was then processed using several morphological algorithms in order to segment individual trees. First, a Euclidean distance transform was applied, after which local maxima were identified. Six minimum distances were tested to determine the optimal locations of local maxima for all models. Next, each local maximum “marker” was assigned a unique label value, and then watershed segmentation was performed using the inverted distances with the markers as the seed points.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_trees</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span> <span class="n">min_dist</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
  <span class="s">"""Locates individual tree labels via watershed segmentation of a binary prediction
  image.

  Args:
      y_pred (ndarray): Image containing binary predictions

  Returns:
      ndarray: Image containing segmented trees
  """</span>
  <span class="n">y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">y_pred</span><span class="p">).</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>

  <span class="c1"># Calculate the distance transform
</span>  <span class="n">distance</span> <span class="o">=</span> <span class="n">ndi</span><span class="p">.</span><span class="n">distance_transform_edt</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>

  <span class="c1"># Get local maxima 
</span>  <span class="n">coords</span> <span class="o">=</span> <span class="n">peak_local_max</span><span class="p">(</span><span class="n">distance</span><span class="p">,</span> <span class="n">min_distance</span><span class="o">=</span><span class="n">min_dist</span><span class="p">)</span>

  <span class="c1"># Collect the local maxima coordinates and generate unique labels for each
</span>  <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">distance</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="nb">bool</span><span class="p">)</span>
  <span class="n">mask</span><span class="p">[</span><span class="nb">tuple</span><span class="p">(</span><span class="n">coords</span><span class="p">.</span><span class="n">T</span><span class="p">)]</span> <span class="o">=</span> <span class="bp">True</span>
  <span class="n">markers</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">ndi</span><span class="p">.</span><span class="n">label</span><span class="p">(</span><span class="n">mask</span><span class="p">)</span>

  <span class="c1"># Perform watershed segmentation and return resulting labels and regionprops
</span>  <span class="n">labels</span> <span class="o">=</span> <span class="n">watershed</span><span class="p">(</span><span class="o">-</span><span class="n">distance</span><span class="p">,</span> <span class="n">markers</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="n">y_pred</span><span class="p">)</span>
  <span class="n">labels</span> <span class="o">=</span> <span class="n">label</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
  <span class="n">regions</span> <span class="o">=</span> <span class="n">regionprops</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">labels</span><span class="p">,</span> <span class="n">regions</span>
</code></pre></div></div>

<p>At this point, the resulting labels still represented the exact extent of original output of the models—only the label values had been changed. However, because some of the models were encouraged to incorrectly identify all tree pixels, two additional morphological manipulations were applied, namely, convex hull and dilation. As some of the predicted trees were not always “tree-like” in shape, each label was reshaped into its corresponding convex hull. Next, the labels were “re-grown” (dilated) to account for the “shrunken” clusters of tree pixels that were predicted without tree crown borders by models trained on eroded labels and/or with border or boundary-excluding weight maps. For most of these morphological operations scikit-image and SciPy were used <sup id="fnref:13:2"><a href="#fn:13" class="footnote" rel="footnote" role="doc-noteref">13</a></sup>, <sup id="fnref:19"><a href="#fn:19" class="footnote" rel="footnote" role="doc-noteref">19</a></sup>.</p>

<p>To determine the optimal combination of minimum distance and morphological manipulation, tree count absolute error (TCAE) and binary IoU (bIoU) were calculated for all combinations, and the optimal values were used for final ensemble predictions (Figures 6 and 7). TCAE was calculated as \(\frac{\lvert y_{true} - y_{pred} \rvert}{y_{true}} \cdot 100\).</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/min-dist_by_abs-tree-error-pct.png" />
  <figcaption><b>Figure 5.</b> TCAE as minimum distance for local maxima selection increases.</figcaption>
</figure>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/morph_vs_biou.png" />
  <figcaption><b>Figure 7.</b> Effect of label set (during training) and morphological adjustment (applied to trained model predictions) on overall bIoU. Values for “no erosion” and “eroded” represent the bIoU of unmodified predictions compared to the ORIG and ERODED label sets, respectively, while “best dist chull” and “best dist dilated” refer to bIoUs of predictions that have first had watershed segmentation applied using their optimal distances and have then had convex hull and region-growing (dilation) operations applied, respectively.</figcaption>
</figure>

<h1 id="results">Results</h1>

<h2 id="overall-performance">Overall performance</h2>

<p>To test the efficacy of the above methodology, in addition to assessing core model performance statistics, the influence of four key components was examined: i) whether models were trained on original or eroded label sets; ii) weight map type; iii) minimum distance threshold for local maxima calculation; and iv) convex hull and dilation. The best model/post-processing ensembles were then identified based on tree count absolute error (TCAE), tree area distributions, and overall bIoU (Table 1).</p>

<figure>
  <table class="dataframe" style="display: inline-table;">
    <thead>
      <tr class="best-row" style="text-align: right;">
        <th>Label Set</th>
        <th>Weights</th>
        <th>Best Min-Dist</th>
        <th>Best Morph</th>
        <th>bIoU</th>
        <th>Tree Absolute Error</th>
        <th>KS-Test <em>p</em></th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td>ORIG</td>
        <td>RONNN</td>
        <td>3</td>
        <td>dilated</td>
        <td>0.740</td>
        <td>18.52%</td>
        <td>0.639</td>
      </tr>
      <tr class="best-row">
        <td>ORIG</td>
        <td>BOUNDS10</td>
        <td>5</td>
        <td>chull</td>
        <td>0.799</td>
        <td class="best"><b>1.84%</b></td>
        <td>0.942</td>
      </tr>
      <tr>
        <td>ORIG</td>
        <td>BORD10</td>
        <td>5</td>
        <td>original</td>
        <td>0.798</td>
        <td>6.12%</td>
        <td>0.104</td>
      </tr>
      <tr class="best-row">
        <td>ORIG</td>
        <td>ALL1</td>
        <td>5</td>
        <td>chull</td>
        <td class="best"><b>0.801</b></td>
        <td>4.44%</td>
        <td>0.855</td>
      </tr>
      <tr>
        <td>ERODED</td>
        <td>BOUNDS10</td>
        <td>5</td>
        <td>chull</td>
        <td>0.777</td>
        <td>4.69%</td>
        <td>0.501</td>
      </tr>
      <tr class="best-row">
        <td>ERODED</td>
        <td>BORD10</td>
        <td>9</td>
        <td>dilated</td>
        <td>0.797</td>
        <td>2.85%</td>
        <td class="best"><b>0.954</b></td>
      </tr>
      <tr>
        <td>ERODED</td>
        <td>ALL1</td>
        <td>7</td>
        <td>chull</td>
        <td>0.781</td>
        <td>5.62%</td>
        <td>0.933</td>
      </tr>
    </tbody>
  </table>


  <figcaption><b>Table 1.</b> Core model + post-processing ensemble statistics. Best Min-Dist refers to the minimum distance used for determining local maxima prior to watershed segmentation, Best Morph is the morphological operations which provided the greatest bIoU (performed after watershed segmentation with Best Min-Dist, and bIoU, Tree [Count] Absolute Error, and KS-Test <em>p</em>-values were calculated from the resulting instance segmentation. In this case, higher KS-Test <em>p</em>-values are better as they suggest a closer relationship between the observed tree area distribution and the predicted distribution.</figcaption>
</figure>

<p>The highest bIoU was produced by ORIG + ALL1, which was expected as there were no penalties for predictions at tree borders. ORIG + BOUNDS10 had the lowest TCAE, and ERODED + BORD10 had the highest tree area distribution similarity. The poorest performances came from ORIG + RONNN and ORIG + BORD10. In all but one case, implementing at least the convex hull operation improved bIoU, while, of the models trained on ERODED label sets only the BORD10 weight map predictions benefited from dilation. This is understandable, as pixels classified as “border” pixels are closer to the center of mass of the tree than the boundaries, therefore allowing for greater separation between tree pixel clusters than boundary pixels.</p>

<h2 id="tree-area-histograms">Tree area histograms</h2>

<p>TCAE only provides a shallow sense of quality of the instance segmentation, however, and may be indicative of the real relationship between the predicted trees and their observed counterparts. To better understand this relationship, the distribution of predicted trees by their area was compared with the observed (true) tree area distribution and Kolmogorov-Smirnov test (KS-Test) \(p\)-value significances were computed (Figure 8).</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/tree_area_hist.png" />
  <figcaption><b>Figure 8.</b> Tree area distributions of predictions (“y_pred”) overlaid with the observed distribution (“y_true”). Purple indicates overlapping bars, and <em>p</em>-values indicate the possibility of rejecting the NULL hypothesis that the two histograms do not come from the same distribution.</figcaption>
</figure>

<p>Here it can be seen that, of the highest performers, ERODED + BORD10 produces the closest-aligned output, as all other models greatly overestimate small-area trees.</p>

<h2 id="visual-inspection">Visual inspection</h2>

<p>Further visual inspection of the ensemble predictions suggests that this difference is perhaps more significant than the higher bIoU and lower TCAE of ORIG + BOUNDS10, as the trees of ERODED + BORD10 appears more appropriately segmented (Figures 9 and 10).</p>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/best_preds_abserr.png" />
  <figcaption><b>Figure 9.</b> Visual plots of ensemble predictions overlaid on a sample of the original RGB image. Y_True indicates the observed test labels.</figcaption>
</figure>

<figure>
  <img src="https://github.com/UP-RS-ESP/up-rs-esp.github.io/raw/master/_posts/Tree_Segmentation_images/full_prediction.png" />
  <figcaption><b>Figure 10.</b> Comparison of true labels (y_true) with full tile output of the ERODED + BORD10 ensemble.</figcaption>
</figure>

<h1 id="discussion-and-conclusions">Discussion and conclusions</h1>

<p>Overall, the above exploration suggests that training a model on eroded labels with border (not boundary) weights may be able to produce better ensemble segmentation results than with the original label set and thresholded boundary weights, as has been done in previous tree detection attempts <sup id="fnref:5:2"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup>\(^,\)<sup id="fnref:6:2"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup>. Further, convex hull transformation (at the least) followed by label dilation can produce prediction output with nearly identical coverage of tree pixels while still allowing for accurate instance segmentation of U-Net-generated semantic segmentation output. The model trained on eroded labels with border weights and modified with both convex hull and dilation transformations was ultimately able to produce instance segmentation with a resulting bIoU only 0.004 less than the unweighted model trained on uneroded labels (0.797 compared to 0.801).</p>

<p>That said, this preliminary investigation could be improved in several ways. First, K-fold cross-validation was not performed during model training, and so model resilience is not reflected here. Additionally, the training labels generated semi-automatically contain many errors and some instances of unrealistic tree segmentation, and the use of the same set of higher-quality labels, all drawn from the same geographic location, for both validation and testing can be problematic when evaluating model performance as it is unlikely to result in a well-generalized model. Furthermore, the validation/test label set was generated in patches, which led to sometimes mis-matched or truncated labels when trees spanned the borders of multiple tiles. These latter issues are matters of time and labor, however, and could be resolved with investment in higher-quality label sets across broader swaths of Berlin. With these issues in mind, however, the resulting model performance remained surprisingly accurate, and it is likely that resolving them would result in even better tree detection.</p>

<h1 id="references">References</h1>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p>T. Kattenborn, J. Leitloff, F. Schiefer, and S. Hinz, “Review on Convolutional Neural Networks (CNN) in vegetation remote sensing,” ISPRS J. Photogramm. Remote Sens., vol. 173, pp. 24–49, Mar. 2021, doi: 10.1016/j.isprsjprs.2020.12.010. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2">
      <p>M. Reichstein et al., “Deep learning and process understanding for data-driven Earth system science,” Nature, vol. 566, no. 7743, Art. no. 7743, Feb. 2019, doi: 10.1038/s41586-019-0912-1. <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3">
      <p>O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional Networks for Biomedical Image Segmentation,” in Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015, N. Navab, J. Hornegger, W. M. Wells, and A. F. Frangi, Eds., in Lecture Notes in Computer Science. Cham: Springer International Publishing, 2015, pp. 234–241. doi: 10.1007/978-3-319-24574-4_28. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:3:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:3:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:4">
      <p>M. Perslev, E. B. Dam, A. Pai, and C. Igel, “One Network to Segment Them All: A General, Lightweight System for Accurate 3D Medical Image Segmentation,” in Medical Image Computing and Computer Assisted Intervention – MICCAI 2019, D. Shen, T. Liu, T. M. Peters, L. H. Staib, C. Essert, S. Zhou, P.-T. Yap, and A. Khan, Eds., in Lecture Notes in Computer Science. Cham: Springer International Publishing, 2019, pp. 30–38. doi: 10.1007/978-3-030-32245-8_4. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5">
      <p>M. Brandt et al., “An unexpectedly large count of trees in the West African Sahara and Sahel,” Nature, vol. 587, no. 7832, Art. no. 7832, Nov. 2020, doi: 10.1038/s41586-020-2824-5. <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:5:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:5:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:6">
      <p>M. Mugabowindekwe et al., “Nation-wide mapping of tree-level aboveground carbon stocks in Rwanda,” Nat. Clim. Change, pp. 1–7, Dec. 2022, doi: 10.1038/s41558-022-01544-w. <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:6:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:6:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:7">
      <p>R. Girshick, “Fast R-CNN,” presented at the Proceedings of the IEEE International Conference on Computer Vision, 2015, pp. 1440–1448. Accessed: Oct. 17, 2022. [^Online]. Available: https://openaccess.thecvf.com/content_iccv_2015/html/Girshick_Fast_R-CNN_ICCV_2015_paper.html <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:8">
      <p>S. Ren, K. He, R. Girshick, and J. Sun, “Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks,” in Advances in Neural Information Processing Systems, Curran Associates, Inc., 2015. Accessed: Oct. 17, 2022. [^Online]. Available: https://proceedings.neurips.cc/paper/2015/hash/14bfa6bb14875e45bba028a21ed38046-Abstract.html <a href="#fnref:8" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:9">
      <p>K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask R-CNN.” arXiv, Jan. 24, 2018. doi: 10.48550/arXiv.1703.06870. <a href="#fnref:9" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:10">
      <p>“Geoportal Berlin / Digitale farbige TrueOrthophotos 2020 (TrueDOP20RGB) - Sommerbefliegung.” Accessed: Sep. 15, 2022. [^License: dl-de/by-2-0 (http://www.govdata.de/dl-de/by-2-0)]. Available: https://fbinter.stadt-berlin.de/fb/wms/senstadt/k_luftbild2020_true_rgb <a href="#fnref:10" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:11">
      <p>“Geoportal Berlin / Digitale Color-Infrarot TrueOrthophotos 2020 (TrueDOP20CIR) - Sommerbefliegung.” Accessed: Sep. 15, 2022. [^License: dl-de/by-2-0 (http://www.govdata.de/dl-de/by-2-0)]. Available: https://fbinter.stadt-berlin.de/fb/wms/senstadt/k_luftbild2020_true_cir <a href="#fnref:11" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:12">
      <p>“Geoportal Berlin / Airborne Laserscanning (ALS) - Primäre 3D Laserscan-Daten.” Accessed: Sep. 15, 2022. [^License: dl-de/by-2-0 (http://www.govdata.de/dl-de/by-2-0)]. Available: https://fbinter.stadt-berlin.de/fb/feed/senstadt/a_als <a href="#fnref:12" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:13">
      <p>S. van der Walt et al., “scikit-image: image processing in Python,” PeerJ, vol. 2, p. e453, Jun. 2014, doi: 10.7717/peerj.453. <a href="#fnref:13" class="reversefootnote" role="doc-backlink">&#8617;</a> <a href="#fnref:13:1" class="reversefootnote" role="doc-backlink">&#8617;<sup>2</sup></a> <a href="#fnref:13:2" class="reversefootnote" role="doc-backlink">&#8617;<sup>3</sup></a></p>
    </li>
    <li id="fn:14">
      <p>N. Sofroniew et al., “napari: a multi-dimensional image viewer for Python.” Zenodo, Nov. 03, 2022. doi: 10.5281/zenodo.7276432. <a href="#fnref:14" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:15">
      <p>M. A. Rahman and Y. Wang, “Optimizing Intersection-Over-Union in Deep Neural Networks for Image Segmentation,” in Advances in Visual Computing, G. Bebis, R. Boyle, B. Parvin, D. Koracin, F. Porikli, S. Skaff, A. Entezari, J. Min, D. Iwai, A. Sadagic, C. Scheidegger, and T. Isenberg, Eds., in Lecture Notes in Computer Science. Cham: Springer International Publishing, 2016, pp. 234–244. doi: 10.1007/978-3-319-50835-1_22. <a href="#fnref:15" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:16">
      <p>S. S. M. Salehi, D. Erdogmus, and A. Gholipour, “Tversky loss function for image segmentation using 3D fully convolutional deep networks.” arXiv, Jun. 18, 2017. doi: 10.48550/arXiv.1706.05721. <a href="#fnref:16" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:17">
      <p>N. Abraham and N. M. Khan, “A Novel Focal Tversky loss function with improved Attention U-Net for lesion segmentation.” arXiv, Oct. 17, 2018. doi: 10.48550/arXiv.1810.07842. <a href="#fnref:17" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:18">
      <p>M. D. Zeiler, “ADADELTA: An Adaptive Learning Rate Method.” arXiv, Dec. 22, 2012. doi: 10.48550/arXiv.1212.5701. <a href="#fnref:18" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:19">
      <p>P. Virtanen et al., “SciPy 1.0: fundamental algorithms for scientific computing in Python,” Nat. Methods, vol. 17, no. 3, Art. no. 3, Mar. 2020, doi: 10.1038/s41592-019-0686-2. <a href="#fnref:19" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Daniel Lusk</name></author><category term="machine learning" /><category term="CNN" /><category term="U-Net" /><category term="tree detection" /><category term="orthophotos" /><summary type="html"><![CDATA[Fine individiual tree crown delineation can be achieved from RGB + NIR orthophotos using CNN-based semantic segmentation with weighted losses, but how do different weight maps perform, and how does manipulation of training label sets affect this?]]></summary></entry></feed>